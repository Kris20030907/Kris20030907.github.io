<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>KTpro</title>
  
  <subtitle>KT&#39;s 小站</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2024-12-25T11:48:27.987Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>KTpro</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="http://example.com/2024/12/25/rocketmq-yin-ru-rocksdb-shi-xian-qian-wan-dui-lie/"/>
    <id>http://example.com/2024/12/25/rocketmq-yin-ru-rocksdb-shi-xian-qian-wan-dui-lie/</id>
    <published>2024-12-25T11:34:00.410Z</published>
    <updated>2024-12-25T11:48:27.987Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>全网仅有!!，这个方案全网都没有什么参考资料。最近写了一篇 RocketMQ 引入 RocksDB 的设计方案，在写这个的过程中我对 RocketMQ 的底层存储和架构更加清晰了，原本只熟悉 kafka，而 RocketMQ 只停留在使用以及存储的应用层面，现在趁这个机会，好好的学习总结，周会上分享给大家！</p></blockquote><p>后续有可能会录个讲解视频放到这里！</p><p>那么，现在就开始吧！</p><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在分布式消息中间件领域，Apache RocketMQ 是一款备受欢迎的高性能、低延迟的消息队列系统，广泛应用于企业级消息通信、数据流处理和事件驱动架构中。而 RocksDB 则是一个高效的嵌入式键值数据库，其基于 LSM 树的存储设计，在随机写入和大规模数据管理方面表现出色。</p><p>近年来，随着企业消息队列的规模和复杂度迅速增长，传统的 RocketMQ 存储架构逐渐暴露出一些瓶颈。为应对这些挑战，社区引入了 RocksDB，作为一种新的存储方案，用于优化核心组件的性能和扩展能力。本文将重点讨论 RocketMQ 存储架构的现状及其局限性，并探讨 RocksDB 的引入是如何帮助解决这些问题的。</p><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h1><h2 id="RocketMQ-存储结构"><a href="#RocketMQ-存储结构" class="headerlink" title="RocketMQ 存储结构"></a>RocketMQ 存储结构</h2><p>首先来回顾一下 ConsumeQueue 以及消息读写的过程。关于 RocketMQ 其他存储结构的介绍可以参考我的另一篇文档：<a href="https://xiaomi.f.mioffice.cn/wiki/R3XGw87KwiGNbgkrakSkmvch4uf">RocketMQ 存储结构</a></p><h3 id="ConsumeQueue"><a href="#ConsumeQueue" class="headerlink" title="ConsumeQueue"></a><strong>ConsumeQueue</strong></h3><p>ConsumeQueue 引入的目的主要是<strong>提高消息消费的性能</strong>，由于 RocketMQ 是基于主题 topic 的订阅模式，消息消费是针对主题进行的。如果直接从 Commitlog 文件中根据 topic 检索消息是非常低效的，因此引入了 ConsumeQueue 作为“索引”，通过 ConsumeQueue，消费者可以直接定位到消息在 CommitLog 中的具体位置，从而快速完成消息消费。ConsumeQueue 的主要功能是存储以下三类索引信息：</p><ul><li><p><strong>消息的物理偏移量（offset）：</strong> 指向 CommitLog 中对应消息的位置。</p></li><li><p><strong>消息大小（size）：</strong> 标识消息在 CommitLog 中的长度。</p></li><li><p><strong>消息</strong> <strong>Tag</strong> <strong>的 HashCode：</strong> 便于快速匹配过滤消息。</p></li></ul><blockquote><p>ConsumeQueue 文件的默认大小为 6 MB，可存储约 30 万条消息的索引信息。</p></blockquote><p>简单画个图：<br><img src="/images/1-3.png"></p><h3 id="消息读写过程"><a href="#消息读写过程" class="headerlink" title="消息读写过程"></a>消息读写过程</h3><p>消息读写的过程如下：</p><ol><li><p><strong>消息写入：</strong></p><ol><li><p>所有消息首先写入 CommitLog。</p></li><li><p>数据写入 CommitLog 后，<code>ReputMessageService</code>（即 dispatch 线程）<strong>异步从 CommitLog 中读取消息数据，构建 ConsumeQueue 和 Index 文件，用于支持后续的消息消费和查询</strong>。</p></li><li><p>Dispatch 线程提取消息的物理偏移量、消息长度及 Tag HashCode，将这些信息写入 ConsumeQueue，形成索引。</p></li></ol></li><li><p><strong>消息读取：</strong></p><ol><li><p>消费者通过逻辑偏移量从 ConsumeQueue 获取对应消息在 CommitLog 中的物理位置。</p></li><li><p>根据物理位置从 CommitLog 中读取原始消息，完成消费。</p></li></ol></li></ol><p>引用官方的一张图：<img src="/images/2-4.png"></p><h2 id="存储瓶颈"><a href="#存储瓶颈" class="headerlink" title="存储瓶颈"></a>存储瓶颈</h2><p>在一些生产环境中，随着消息队列规模的扩展，RocketMQ 的存储架构逐渐暴露出以下问题：</p><ol><li><p><strong>ConsumeQueue 文件管理成本高****：</strong></p><ol><li><p>每个 Topic 的每个队列都对应一个 ConsumeQueue 文件。ConsumeQueue 索引是基于 mmap 实现的，在百万级 Topic 场景下，会产生数百万个小文件，这些小文件的随机写入破坏了 RocketMQ 一直依赖的 CommitLog 顺序写性能，随机读写瓶颈显现（文件内部顺序写，但是多文件是随机写）、性能急剧下降，极大地增加了文件系统的管理开销。</p></li><li><p>启动或重启时需要遍历所有文件，耗时长且容易引发 I&#x2F;O 瓶颈。</p></li></ol></li><li><p>LMQ 同样存在类似问题</p><ol><li><p>LMQ 的轻量级设计能够支持亿级设备连接与百万级消息并发，满足物联网场景的需求。</p></li><li><p>虽然 LMQ 可以让一条消息对应多个索引，减少数据重复存储，但是同样会遇到队列文件过多带来的性能下降问题。</p></li></ol></li></ol><h1 id="如何解决"><a href="#如何解决" class="headerlink" title="如何解决"></a>如何解决</h1><p>为了优化百万级 Topic 场景下的存储性能和管理效率，RocketMQ 需要引入一种高效的存储引擎，来替代现有的 ConsumeQueue 文件管理机制。RocksDB 的设计完美契合这一需求：<br><img src="/images/3-3.png"></p><ol><li><p><strong>提升随机写性能：</strong></p><ol><li><p>RocksDB 基于 LSM 树（Log-Structured Merge Tree），通过 WAL（Write-Ahead Logging）和 MemoryTable 的机制，将随机写转化为 SST 文件的顺序写，显著提升写入性能。</p></li><li><p>RocksDB 的 Compaction 操作会将数据文件合并为 SST 文件并进行分层存储，进一步优化读写效率。</p></li></ol></li><li><p><strong>减少小文件管理开销：</strong></p><ol><li><p>RocksDB 将所有小文件条目聚合存储（变成内存 + SST 存储），大大减少了文件系统句柄的占用，避免 OOM 问题。</p></li><li><p>通过列族（Column Family）组织数据，可以灵活扩展和优化存储结构。</p></li></ol></li></ol><blockquote><p>这里的列族实际上可以看作是一个表，里面存放了很多条数据记录。</p></blockquote><ol start="3"><li><p><strong>优化启动性能</strong></p><ol><li><p>RocksDB 可以实现<strong>按需加载</strong>，在启动时候只需要加载必要的元数据信息，比如 Max 和 Min Offsets，不会把全部的 CQ 索引文件都加载到内存。</p></li><li><p>这种方式大幅减少了启动过程中的 I&#x2F;O 操作和内存消耗，提升了系统的启动效率。</p></li></ol></li><li><p><strong>去除 RocksDB</strong> <strong>WAL</strong></p><ol><li>CommitLog 存储了完整的 CQ 数据，并且真正的持久化成功了，因此 CQ 数据可以根据 CommitLog 进行重建。去除 WAL 可以进一步提升 RocksDB 写入性能。</li></ol></li><li><p><strong>扩展性强</strong></p><ol><li><p>RocksDB 作为嵌入式数据库，提供了列族（Column Family）特性，允许针对不同业务需求灵活调整存储设计。每个列族相当于一个独立的逻辑表，拥有独立的键空间和配置（如压缩策略、缓存策略等）。</p></li><li><p>列族可以用于隔离存储不同数据类型，例如 Topic 的消息索引、ConsumerOffset、DelayOffsets 等，实现更细粒度的管理和优化。</p></li></ol></li></ol><p>举一些例子：</p><ol><li><p><strong>高负载热点队列优化</strong>：</p><ol><li><p>针对高负载或热点队列，分配独立的列族，并通过调整列族的写入缓存大小、压缩策略等手段进行单独优化。</p></li><li><p>对于低负载或非热点队列，可以共享列族，节省存储和内存资源。</p></li><li><p>这种设计确保热点队列不会因共享存储资源而影响性能，同时减少了整体的存储开销。</p></li></ol></li><li><p><strong>延迟队列优化</strong>：</p><ol><li><p>RocketMQ 的延迟队列目前通过多个定时文件实现，存在大量小文件，管理复杂，且不同延迟级别的数据混合存储，难以灵活调整策略。</p></li><li><p>引入 RocksDB 后，可以按照延迟级别划分列族，将每个延迟级别的数据独立存储，并使用消息的延迟时间戳作为键。</p></li><li><p>这种设计既支持高效的按键查询，又方便延迟任务的范围查询，显著提升了性能和管理效率。</p></li></ol></li></ol><h1 id="RocksDB-架构和核心特点"><a href="#RocksDB-架构和核心特点" class="headerlink" title="RocksDB 架构和核心特点"></a>RocksDB 架构和核心特点</h1><h2 id="RocksDB-基础"><a href="#RocksDB-基础" class="headerlink" title="RocksDB 基础"></a>RocksDB 基础</h2><p>RocksDB 是一种可持久化的、嵌入式的 kv 存储。</p><ul><li><p>C++编写</p></li><li><p>基于 LevelDB，底层基于 LSM Tree</p></li></ul><p>其实看到嵌入式的，可以很容易想到 SqLite 数据库，特别是在 RocksDB 的上下文中，“内嵌”意味着：</p><ul><li><p>集成进应用，与应用共享内存，避免跨进程通信开销。</p></li><li><p>只支持本地存储，无法通过网络访问，也不提供分布式能力。</p></li><li><p>灵活高效，尤其适合高性能要求的小规模元数据存取。</p></li></ul><p>支持的操作：</p><ul><li><p>delete 删除键值对（逻辑删除，把记录类型改为 DEL，并把 value 置空）</p></li><li><p>put 增加或更新数据</p></li><li><p>merge 合并当前键值到已有的键值对</p></li><li><p>get （key）获取数据</p></li><li><p>支持迭代器遍历，比如根据某个以 prefix 为前缀的所有 key 之间的向前或向后迭代遍历</p></li></ul><p>接下来我们聚集到数据读写上介绍 RocksDB。</p><h2 id="写路径"><a href="#写路径" class="headerlink" title="写路径"></a>写路径</h2><p>RocksDB 的数据写入流程如下：</p><ol><li><p><strong>WAL (Write-Ahead Log)</strong> :</p><ol><li>写操作会首先写入 WAL 日志，保证数据持久化（宕机恢复）。</li></ol></li><li><p><strong>MemTable</strong>:</p><ol><li><p>数据同时写入内存中的 <strong>MemTable</strong>（默认基于 SkipList 实现）。</p></li><li><p>MemTable 支持并发写入，当写满时，转换为 <strong>Immutable MemTable 不可变的内存块</strong>。</p></li></ol></li><li><p><strong>刷盘 （Flush）</strong>：</p><ol><li>Immutable MemTable 被后台线程刷盘，生成 <strong>SST 文件</strong>（Sorted String Table）存储到磁盘的 <strong>Level 0</strong> 层。</li></ol></li><li><p><strong>Compaction</strong>:</p><ol><li><p>当 Level 0 的 SST 文件数量超过阈值时，触发 <strong>Compaction</strong>。</p></li><li><p>Compaction 会将上层的 SST 文件与下层 SST 文件合并且保证有序，丢弃无效数据（如被覆盖或删除的 Key）。</p></li></ol></li></ol><h2 id="读路径"><a href="#读路径" class="headerlink" title="读路径"></a>读路径</h2><p>数据查询时主要是按照层级查找：</p><ol><li><p><strong>MemTable 和 Immutable MemTable</strong>：优先在内存中查找。</p></li><li><p><strong>Level 0</strong>：遍历 L0 层所有 SST 文件。</p></li><li><p><strong>Level 1 及以下</strong>：通过索引可以快速定位目标 SST 文件，再根据 SST 数据块查找 Key。</p></li><li><p><strong>布隆过滤器</strong>（可选）：快速排除不存在的数据，提升查询性能。</p></li></ol><p>问题：为什么 L0 层要全部遍历？</p><ol><li><p><strong>L0 层的 SST 文件内部是有序的</strong>，但 <strong>L0 层的多个 SST 文件之间不一定有全局有序性</strong>，可能存在键的重叠，因为 <strong>Flush</strong> 操作将 <strong>Immutable MemTable</strong> 数据直接写入 L0 层生成新的 SST 文件，而不同 Flush 操作写入的 SST 文件之间没有进行合并，可能存在数据重叠（比如第一次 flush 操作 1， 5， 6。第二次 flush 操作 2， 8，<strong>块内有序，整体无序</strong>）。</p></li><li><p>由于 Compaction 的作用，<strong>L1 及以下的层</strong>，SST 文件不仅内部有序，<strong>不同 SST 文件之间也不会有键重叠</strong>，它们在逻辑上是全局有序的。</p></li></ol><h2 id="细节介绍"><a href="#细节介绍" class="headerlink" title="细节介绍"></a>细节介绍</h2><p>关于 RocksDB 更细节的介绍，这里不多介绍了。可以参考我的另一篇文档：<a href="https://xiaomi.f.mioffice.cn/wiki/YHeswbH2Biw4LUkmviNkMOWU4gc">RocksDB 优势</a></p><h2 id="有趣的-merge"><a href="#有趣的-merge" class="headerlink" title="有趣的 merge"></a>有趣的 merge</h2><p>传统的合并操作的场景：对某个 key 的 value 上新增一部分数据（追加），我们的操作是 read -&gt; append -&gt; write。这种操作有一定缺陷：</p><ol><li><p>可以看到这个操作实际上不是原子性的，即多线程同时更新会出现数据丢失。</p></li><li><p>写放大：如果写的数据比较大，或者数据量越来越大，之后的增量就都需要完整的把数据读出来再写入，浪费资源。</p></li></ol><p>RocksDB 对这个做了优化：<strong>对于增量的数据先写入 MemoryTable 和</strong> <strong>WAL****，等待 Compaction 和 Flush 操作到磁盘。如果这个期间（还没持久化到磁盘）有读请求，就在读的时候先检查有没有 hang 的 merge 操作，如果有那就先合并再返回合并后的数据给客户端。</strong></p><p>这个问题也比较明显，每次读都需要合并数据，直到增量数据被持久化，不过这里也可以通过限制 MemTable 的容量等操作来让数据尽快的持久化。</p><h1 id="RocksDB-在-RocketMQ-中的设计与实现"><a href="#RocksDB-在-RocketMQ-中的设计与实现" class="headerlink" title="RocksDB 在 RocketMQ 中的设计与实现"></a>RocksDB 在 RocketMQ 中的设计与实现</h1><h2 id="可行性分析"><a href="#可行性分析" class="headerlink" title="可行性分析"></a>可行性分析</h2><p>在前面的存储瓶颈与 RocksDB 介绍的基础上，为了实现在 RocketMQ 中对 CQ 相关数据的存储和读写，我们需要将原始的 CQ 存储单元（含 20 字节： phyOffsets，msgSize，tagCode），以及 min（或 max） offsets 通过 RocksDB 进行读写。因此，需要设计相关的列组（数据表），并保证可以根据 key 唯一确定到某条消息。</p><h2 id="设计方案"><a href="#设计方案" class="headerlink" title="设计方案"></a>设计方案</h2><p>我们分为以下三个层面进行：</p><ol><li><p>架构层面</p></li><li><p>数据存储层面</p></li><li><p>代码层面</p></li></ol><h3 id="架构层面"><a href="#架构层面" class="headerlink" title="架构层面"></a>架构层面</h3><p>这里主要描述如何构建 RocksDB CQ 单元和 Offset 的存储，以及消费关系，后续会在详细设计中介绍细节。</p><blockquote><p>Tip: 此处仅描述 RocksDB 数据流。</p></blockquote><p><img src="/images/4-3.png"></p><h3 id="数据存储格式"><a href="#数据存储格式" class="headerlink" title="数据存储格式"></a>数据存储格式</h3><p>在数据存储层面，我们需要考虑怎么唯一标识一条消息的索引。RocksDB 的 key 和 value 都是无结构的字节数组（byte[]），所以对于 key 和 value 的设置上是非常灵活的，这也意味着我们在新增、删除和更新数据的时候需要注意字节序列化和反序列化的处理。这里，我们将 CQ 相关的数据分为两个表：</p><ol><li><strong>RocksDBConsumeQueueTable</strong>：存储物理 offset、tagCode、msgSize 等 CQ 相关字段，并额外增加时间戳扩展字段。<br><img src="/images/5-3.png"></li></ol><p>可以注意到：</p><ul><li><p>由于 TopicName 的长度不固定，因此新增 Topic Bytes Array Size 字段用于确定 TopicName 的长度，这样在之后划分时更加方便。</p></li><li><p>Key 中新增了 Ctrl_1，即 ASCII 中的第一个字符，用于后续的删除操作。</p></li><li><p>Value 中新增了 Msg Store Time，主要是<strong>用于扩展</strong>的，即当查询某条消息的存储时间时，不需要按照之前那样拿到 phyOffsets 之后去 CommitLog 拿，这里直接存到 RocksDB 里可以避免一种问题： <strong>冷数据消费或从某个时间点开始消费消息的场景</strong>，不会替换掉 pagecache 中的数据，能减少一定的抖动。（所以后续如果有什么类似的需求，我们也可以通过新增 KV 的字段来进行扩展）</p></li></ul><ol start="2"><li><strong>RocksDBQueueOffsetTable</strong>：存储线程对应的 Min 或 Max 逻辑 offset 和物理 offset：</li></ol><p>我们得到最大偏移量和最小偏移量，传统的做法是进行全量扫描，拿到全部的 CQ 文件然后计算最小和最大偏移量。通过放到 RocksDB 中存储能更好的进行创建和维护，进一步提升性能。</p><p><img src="/images/6-2.png"></p><p>需要注意的一点是增加了 max&#x2F;min 的标识，用于区分存储的是最大或最小偏移量。</p><h3 id="代码层面"><a href="#代码层面" class="headerlink" title="代码层面"></a>代码层面</h3><ol><li><p>引入 ConsumeQueueStoreInterface 接口：定义 ConsumeQueue 的存储操作接口，便于不同存储实现的切换。</p></li><li><p>实现 RocksDBConsumeQueueStore：基于 RocksDB 实现了 ConsumeQueueStore 接口，负责消息消费队列的存储操作，比如获取最大&#x2F;最小偏移量（逻辑或物理）。</p></li><li><p>引入抽象 AbstractConsumeQueueStore 类：实现 ConsumeQueueStoreInterface 接口，并提供 ConsumeQueue 的抽象实现，封装了通用逻辑，便于不同存储方式的具体实现。</p></li><li><p>实现 RocksDBConsumeQueue 类：实现 ConsumeQueueInterface，作为 RocksDBConsumeQueueStore 的操作单元，并调用其相关方法进行实际的数据读写操作。</p></li><li><p>修改 MessageStore 相关类：增加对 RocksDB 存储方式的支持。</p></li><li><p>配置调整：broker 初始化时根据配置决定存储方式，支持双写。</p></li></ol><p>类图关系如下：<br><img src="/images/7-2.png"></p><h2 id="如何维护-CQ-min-offsets"><a href="#如何维护-CQ-min-offsets" class="headerlink" title="如何维护 CQ min offsets"></a>如何维护 CQ min offsets</h2><p>如果 CommitLog 中的消息由于超过消息保留时间被删除，或者因为磁盘瓶颈被删除，那物理偏移量对应的消息可能就不存在了，我们需要维护 CQ 的最小偏移量来保证消费者不会读取到过期的消息</p><ol><li><p><strong>原版</strong>：采用的方式是增加一个定时任务去判断 CQ 的最小逻辑偏移量对应的物理偏移量是否失效了，如果失效就需要对 CQ 文件进行清理。</p></li><li><p>RocksDB 版本：不采用定时任务，当调用 <code>getMinOffset</code> 的方式时判断是否过期，如果过期再删除并重新加载。</p></li></ol><h2 id="如何删除过期的-CQ-数据"><a href="#如何删除过期的-CQ-数据" class="headerlink" title="如何删除过期的 CQ 数据"></a>如何删除过期的 CQ 数据</h2><ol><li><p>原版：通过定时任务。</p></li><li><p>RocksDB 版本：实现 CompactionFilter，自定义过滤的方法，在 Compaction 过程中判断 CQ 数据是否过期（物理偏移量小于物理最小偏移量），如果过期则进行删除。</p></li></ol><h2 id="Topic-删除怎么处理表数据"><a href="#Topic-删除怎么处理表数据" class="headerlink" title="Topic 删除怎么处理表数据"></a>Topic 删除怎么处理表数据</h2><p>RocksDB：Topic 删除后，需要删除 RocksDB 中对应的表数据，包括 CQ 表和 offset 表。RocksDB 是支持范围删除的，因此这里主要是通过拼接 Ctrl 标识来实现一个字典序差值的范围进行 CQ 文件的删除，具体的实现可以参考“详细实现”章节。</p><h2 id="RocksDB-和-LMQ-支持同时开启"><a href="#RocksDB-和-LMQ-支持同时开启" class="headerlink" title="RocksDB 和 LMQ 支持同时开启"></a>RocksDB 和 LMQ 支持同时开启</h2><p>同样实现了基于 RocksDB 存储 LMQ 数据而并非使用文件系统，与一般 CQ 数据相比，主要区别在于 dispatch 以及维护逻辑偏移量的过程。后续会详细介绍。</p><h2 id="兼容性考虑"><a href="#兼容性考虑" class="headerlink" title="兼容性考虑"></a>兼容性考虑</h2><p>无论 rocksdb 开关是开启还是关闭，原始的 RocketMQ 客户端和消息收发都不会受到影响。</p><p>统一实现了存储接口，原 MessageStore 等逻辑不变，如果开启 RocksDB 选项则走 RocksDB 的 Store 方式。并且支持原 MessageStore 模式下开启 RocksDB 双写选项，同时写入一份数据到 RocksDB 中，方便后续迁移。</p><h1 id="详细实现"><a href="#详细实现" class="headerlink" title="详细实现"></a>详细实现</h1><p>总共有 4800 行左右代码，这里介绍核心变更。</p><h2 id="RocksDB-存储类实现"><a href="#RocksDB-存储类实现" class="headerlink" title="RocksDB 存储类实现"></a>RocksDB 存储类实现</h2><p><code>ConsumeQueueRocksDBStorage</code> 类继承 AbstractRocksDBStorage ，主要实现数据启动时加载、数据的获取以及增删改、Flush 操作以及手动触发 Compaction 的方法。</p><h2 id="Load-方法"><a href="#Load-方法" class="headerlink" title="Load 方法"></a>Load 方法</h2><p>当 RocketMQ 启动时会从磁盘中加载数据文件，以 ConsumeQueue 文件为例：</p><ol><li>原版采用的方式是进行全量遍历：</li></ol><p>在 BrokerController 中的初始化方法里调用 DefaultMessageStore 的 load 方法，其中加载 ConsumeQueue 的方法实现为：</p><ul><li><p>遍历 CQ 目录下的所有 Topic 文件夹以及各分区下的 CQ 文件。</p></li><li><p>为每个 CQ 文件创建逻辑的 CQ 对象，包括 CQ 的类型以及对应的 topic 和 queueId。</p></li><li><p>根据逻辑的 CQ 对象，去 Consume 类中调用 mappedFileQueue 的 load 方法，从磁盘中加载数据。</p></li></ul><pre class="line-numbers language-Java" data-language="Java"><code class="language-Java">private boolean loadConsumeQueues(String storePath, CQType cqType) &#123;        File dirLogic &#x3D; new File(storePath);        File[] fileTopicList &#x3D; dirLogic.listFiles();        if (fileTopicList !&#x3D; null) &#123;            for (File fileTopic : fileTopicList) &#123;                String topic &#x3D; fileTopic.getName();                File[] fileQueueIdList &#x3D; fileTopic.listFiles();                if (fileQueueIdList !&#x3D; null) &#123;                    for (File fileQueueId : fileQueueIdList) &#123;                        int queueId;                        try &#123;                            queueId &#x3D; Integer.parseInt(fileQueueId.getName());                        &#125; catch (NumberFormatException e) &#123;                            continue;                        &#125;                        queueTypeShouldBe(topic, cqType);                        ConsumeQueueInterface logic &#x3D; createConsumeQueueByType(cqType, topic, queueId, storePath);                        this.putConsumeQueue(topic, queueId, logic);                        if (!this.load(logic)) &#123;                            return false;                        &#125;                    &#125;                &#125;            &#125;        &#125;        log.info(&quot;load &#123;&#125; all over, OK&quot;, cqType);        return true;    &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="2"><li>RocksDB 模式</li></ol><p>RocksDB 模式下，并没有将 load 操作放到 RocksDBMessageStore 中，而是统一放到 RocksDBConsumeQueueStore 中，实现如下：</p><pre class="line-numbers language-Java" data-language="Java"><code class="language-Java">public boolean load() &#123;    boolean result &#x3D; this.rocksDBStorage.start();    this.rocksDBConsumeQueueTable.load();    this.rocksDBConsumeQueueOffsetTable.load();    log.info(&quot;load rocksdb consume queue &#123;&#125;.&quot;, result ? &quot;OK&quot; : &quot;Failed&quot;);    return result;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><p>启动 RocksDB 存储类对象，其中调用了预加载方法 postLoad，主要是加载 CQ 单元表以及 Offset 表对应的列族和配置信息。（CQ 表为 default 列族， offset 表为 offset 列族）</p></li><li><p>分别通过相应的对象加载 RocksDB 中存储的 CQ 单元表以及 Offset 表。</p></li><li><p>对于 CQ 单元表，并没有将 CQ 数据直接全部加载到内存以及创建 CQ 逻辑对象，因为 CQ 数据量较大，将大量占用内存，降低启动时间，所以<strong>在需要查询时直接调用相关的接口查询</strong>即可。</p></li><li><p>对于 offset 表，由于 max&#x2F;min 偏移量访问频率极高且计算过程需要进行全量扫描，因此在加载时放到内存中。</p></li></ul><pre class="line-numbers language-Java" data-language="Java"><code class="language-Java">public void load() &#123;    this.offsetCFH &#x3D; this.rocksDBStorage.getOffsetCFHandle();    loadMaxConsumeQueueOffsets();&#125;private void loadMaxConsumeQueueOffsets() &#123;&#x2F;&#x2F; 过滤函数，仅保留代表最大偏移量的条目。    Function&lt;OffsetEntry, Boolean&gt; predicate &#x3D; entry -&gt; entry.type &#x3D;&#x3D; OffsetEntryType.MAXIMUM;    &#x2F;&#x2F;  定义了一个处理函数（fn），用于将筛选后的最大偏移量条目处理并存储到内存结构中。    Consumer&lt;OffsetEntry&gt; fn &#x3D; entry -&gt; &#123;        topicQueueMaxCqOffset.putIfAbsent(entry.topic + &quot;-&quot; + entry.queueId, entry.offset);        log.info(&quot;Max &#123;&#125;:&#123;&#125; --&gt; &#123;&#125;|&#123;&#125;&quot;, entry.topic, entry.queueId, entry.offset, entry.commitLogOffset);    &#125;;    try &#123;    &#x2F;&#x2F; 遍历 RocksDB offset 表对应的列族存储空间，并解析出 MaxOffsets。        forEach(predicate, fn);    &#125; catch (RocksDBException e) &#123;        log.error(&quot;Failed to maximum consume queue offset&quot;, e);    &#125;&#125;&#x2F;&#x2F; 部分 forEach 方法内容：public void forEach(Function&lt;OffsetEntry, Boolean&gt; predicate, Consumer&lt;OffsetEntry&gt; fn) throws RocksDBException &#123; &#x2F;&#x2F; 获取该列族的迭代器，进行数据遍历。try (RocksIterator iterator &#x3D; this.rocksDBStorage.seekOffsetCF()) &#123;    if (null &#x3D;&#x3D; iterator) &#123;        return;    &#125;    &#x2F;&#x2F; 忽略后续的字节操作以及调用处理函数。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Recover-与-cleanDirty"><a href="#Recover-与-cleanDirty" class="headerlink" title="Recover 与 cleanDirty"></a>Recover 与 cleanDirty</h2><p>RocksDB 模式下支持并发 recover，并实现脏数据的清理：</p><ul><li>RocksDBConsumeQueueStore 中调用 recover 或 recoverConcurrently 方法，这将直接调用当前类的 start 方法</li></ul><pre class="line-numbers language-Java" data-language="Java"><code class="language-Java"> @Override public void recover() &#123;     start(); &#125; @Override public boolean recoverConcurrently() &#123;     start();     return true; &#125; @Override public void start() &#123;     if (serviceState.compareAndSet(ServiceState.CREATE_JUST, ServiceState.RUNNING)) &#123;         log.info(&quot;RocksDB ConsumeQueueStore start!&quot;);         this.groupCommitService.start();                  &#x2F;&#x2F; 不管上一个任务是否执行完成都按照固定时间点执行         this.scheduledExecutorService.scheduleAtFixedRate(() -&gt; &#123;             this.rocksDBStorage.statRocksdb(log);         &#125;, 10, this.messageStoreConfig.getStatRocksDBCQIntervalSec(), TimeUnit.SECONDS);         &#x2F;&#x2F; 按照固定延迟执行任务，即上一个任务执行结束后固定延迟再执行         this.scheduledExecutorService.scheduleWithFixedDelay(() -&gt; &#123;             cleanDirty(messageStore.getTopicConfigs().keySet());         &#125;, 10, this.messageStoreConfig.getCleanRocksDBDirtyCQIntervalMin(), TimeUnit.MINUTES);     &#125; &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>start 方法中定义了两个定时任务，分别是 startRocksdb 以及 cleanDirty 方法。其中 startRocksDB 会重新 RocksDB 数据的重新加载，包括各个层的数据以及元数据列表。cleanDirty 方法则会进行 offset 表的无用 CQ 数据的清理（CQ 单元表会通过 Compaction 清理）：</li></ul><pre class="line-numbers language-Java" data-language="Java"><code class="language-Java">private void cleanDirty(final Set&lt;String&gt; existTopicSet) &#123;        try &#123;        &#x2F;&#x2F; 遍历 offsetTable 寻找脏数据，实际上就是判断偏移量关系。            Map&lt;String, Set&lt;Integer&gt;&gt; topicQueueIdToBeDeletedMap &#x3D;                this.rocksDBConsumeQueueOffsetTable.iterateOffsetTable2FindDirty(existTopicSet);            for (Map.Entry&lt;String, Set&lt;Integer&gt;&gt; entry : topicQueueIdToBeDeletedMap.entrySet()) &#123;                String topic &#x3D; entry.getKey();                for (int queueId : entry.getValue()) &#123;                    destroy(new RocksDBConsumeQueue(topic, queueId));                &#125;            &#125;        &#125; catch (Exception e) &#123;            log.error(&quot;cleanUnusedTopic Failed.&quot;, e);        &#125;    &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="开启-RocksDB-存储配置"><a href="#开启-RocksDB-存储配置" class="headerlink" title="开启 RocksDB 存储配置"></a>开启 RocksDB 存储配置</h2><p>当 RocksDB 配置后，将作为一种新的功能注入到 broker，当 brokerController 调用初始化方法时，如果不配置 RocksDB 的存储，则走默认的 DefaultMessageStore，并且支持开启文件和 RocksDB 双写，方便后续迁移。</p><pre class="line-numbers language-Java" data-language="Java"><code class="language-Java">public boolean initialize() throws CloneNotSupportedException &#123;        boolean result &#x3D; this.topicConfigManager.load();        result &#x3D; result &amp;&amp; this.topicQueueMappingManager.load();        result &#x3D; result &amp;&amp; this.consumerOffsetManager.load();        result &#x3D; result &amp;&amp; this.subscriptionGroupManager.load();        result &#x3D; result &amp;&amp; this.consumerFilterManager.load();        if (result) &#123;            try &#123;                DefaultMessageStore defaultMessageStore;                if (this.messageStoreConfig.isEnableRocksDBStore()) &#123;                    defaultMessageStore &#x3D; new RocksDBMessageStore(this.messageStoreConfig, this.brokerStatsManager,                        this.messageArrivingListener, this.brokerConfig, topicConfigManager.getTopicConfigTable());                &#125; else &#123;                    defaultMessageStore &#x3D; new DefaultMessageStore(this.messageStoreConfig, this.brokerStatsManager,                        this.messageArrivingListener, this.brokerConfig, topicConfigManager.getTopicConfigTable());                    if (messageStoreConfig.isRocksdbCQDoubleWriteEnable()) &#123;                        defaultMessageStore.enableRocksdbCQWrite();                    &#125;                &#125;                if (messageStoreConfig.isEnableDLegerCommitLog()) &#123;                    DLedgerRoleChangeHandler roleChangeHandler &#x3D; new DLedgerRoleChangeHandler(this, defaultMessageStore);                    ((DLedgerCommitLog) defaultMessageStore.getCommitLog()).getdLedgerServer().getdLedgerLeaderElector().addRoleChangeHandler(roleChangeHandler);                &#125;                this.brokerStats &#x3D; new BrokerStats(defaultMessageStore);                &#x2F;&#x2F;load plugin                MessageStorePluginContext context &#x3D; new MessageStorePluginContext(messageStoreConfig, brokerStatsManager, messageArrivingListener, brokerConfig);                this.messageStore &#x3D; MessageStoreFactory.build(context, defaultMessageStore);                this.messageStore.getDispatcherList().addFirst(new CommitLogDispatcherCalcBitMap(this.brokerConfig, this.consumerFilterManager));            &#125; catch (IOException e) &#123;                result &#x3D; false;                log.error(&quot;Failed to initialize&quot;, e);            &#125;        &#125;        result &#x3D; result &amp;&amp; this.messageStore.load();        if (result) &#123;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="数据写入流程"><a href="#数据写入流程" class="headerlink" title="数据写入流程"></a>数据写入流程</h2><p>我们就根据此图来解释调用关系，展示了<mark style="background: #FFFF00;">仅 RocksDB 模式以及默认模式下开启 RocksDB 双写的 CQ 数据写入和更新过程</mark>。<br><img src="/images/8-1.png"></p><h3 id="Dispatch-代码实现"><a href="#Dispatch-代码实现" class="headerlink" title="Dispatch 代码实现"></a>Dispatch 代码实现</h3><p>如上所示，当我们开启双写后，将按照如下流程把 LMQ，CQ 数据同时异步写入一份到 RocksDB 中。</p><ol><li><p>当开启双写后，会调用 enableRocksdbCQWrite 方法，这里会增加一个新的 RocksDBDispatcher，之后在 Dispatch 时会遍历 Dispatcher 列表并调用对应的 dispatch 方法：<br><img src="/images/9-1.png"></p></li><li><p>当调用 dispatch 方法时，会调用 Messagestore 的 putMessageInfo 进行 CQ 文件的更新。并且这里的 RocksDB 是基于 GroupCommit 的，也就是在 RocksGroupCommitService 中先追加数据到 buffer，然后有个定时任务去定时把缓存数据通过 store 对象进行持久化。<br><img src="/images/8-2.png"><br><img src="/images/8-3.png"></p></li><li><p>最终会调用到 RocksDBMessageStore 里的 putMessageInfo 0 方法，根据 dispatchEntry 把表数据构建出来，可以看到这里的三个 dispatch 方法，dispatchLMQ，dispatch 普通 CQ：</p></li></ol><p><img src="/images/8-4.png"></p><p>其中 dispatch 是一般 cq 数据的持久化，dispatchLMQ，则是在 dispatch 前进行一些属性的更新。</p><ol start="4"><li>最终调用 offset 表的方法以及 RocksDB 存储对象的批量方法把数据写入到本地和堆中，这样数据就写了一份到 RocksDB 中了：</li></ol><p><img src="/images/8-5.png"></p><h3 id="AssignOffset-代码实现"><a href="#AssignOffset-代码实现" class="headerlink" title="AssignOffset 代码实现"></a>AssignOffset 代码实现</h3><p>当 CommitLog 的 asyncPutMessage 方法被调用时，在 CommitLog 持久化消息前，会先进行 CQ offset 的分配。<code>defaultMessageStore.assignOffset(messageExtBatch);</code>其内部会根据事务类型，调用 ConsumeQueueStore 接口的 assignQueueOffset 方法进行 ConsumeQueue 的获取（如果没有则创建），并进一步调用 ConsumeQueue 对象的 assignQueueOffset 方法进行 maxOffset 的计算并更新。</p><p>以原版为例：</p><pre class="line-numbers language-Java" data-language="Java"><code class="language-Java">@Overridepublic void assignQueueOffset(QueueOffsetOperator queueOffsetOperator, MessageExtBrokerInner msg) &#123;    String topicQueueKey &#x3D; getTopic() + &quot;-&quot; + getQueueId();    long queueOffset &#x3D; queueOffsetOperator.getQueueOffset(topicQueueKey);    msg.setQueueOffset(queueOffset);&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>以 RocksDB 为例：</p><pre class="line-numbers language-Java" data-language="Java"><code class="language-Java">@Override   public void assignQueueOffset(QueueOffsetOperator queueOffsetOperator, MessageExtBrokerInner msg) throws RocksDBException &#123;       String topicQueueKey &#x3D; getTopic() + &quot;-&quot; + getQueueId();       Long queueOffset &#x3D; queueOffsetOperator.getTopicQueueNextOffset(topicQueueKey);       if (queueOffset &#x3D;&#x3D; null) &#123;           &#x2F;&#x2F; we will recover topic queue table from rocksdb when we use it.           queueOffset &#x3D; this.messageStore.getQueueStore().getMaxOffsetInQueue(topic, queueId);           queueOffsetOperator.updateQueueOffset(topicQueueKey, queueOffset);       &#125;       msg.setQueueOffset(queueOffset);   &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以看到 RocksDB 模式下，会在内存中没有对应 ConsumeQueue 的情况下从数据库中获取并更新内存集合。</p><h2 id="RocksDBConsumerQueueStore"><a href="#RocksDBConsumerQueueStore" class="headerlink" title="RocksDBConsumerQueueStore"></a>RocksDBConsumerQueueStore</h2><p>基于 RocksDB 存储类来操作数据库表，比如 CQ 表和 Offset 表（分别对应 RocksDBConsumeQueueTable 和 RocksDBConsumeQueueOffsetTable），并进一步实现更多方法，与原版 ConsumeQueueStore 区别如下：</p><blockquote><p>由于统一了抽象接口方便统一调用，因此原版在此基础上又新增了一些方法，如并发 recover 会在方法后加提示。</p></blockquote><ol><li><p>方法实现对比<br><img src="/images/9-2.png"></p></li><li><p>RocksDB 特有方法<br><img src="/images/9-3.png"></p></li></ol><h2 id="RocksDBMessageStore"><a href="#RocksDBMessageStore" class="headerlink" title="RocksDBMessageStore"></a>RocksDBMessageStore</h2><p>继承自 DefaultMessageStore 并继承了<code>CleanConsumeQueueService</code>、<code>FlushConsumeQueueService</code> 以及 <code>CorrectLogicOffsetService</code> 服务，利用 RocksDB 来实现。其中需要注意的是：</p><ul><li><p>对于 <code>FlushConsumeQueueService</code> RocksDb 是不需要的，因此重写了空的 run 方法。因为我们不需要持久化 CQ 文件，只需要放到 RocksDBConsumerQueueStore 中维护即可。并且我们也<strong>不需要开启</strong> <strong>WAL</strong>，因为 CommitLog 中包含了完整了 CQ 单元信息，我们可以根据这个进行重建。</p></li><li><p>对于 <code>CleanConsumeQueueService</code>，<strong>如果需要删除过期的数据，只需要手动触发 Compaction 即可</strong>，根据自定义的 RocksDBCompactionFilter 可以在 Compaction 过程中进行自动删除。同时还需要删除相应的 index 文件。</p></li><li><p>对于 <code>CorrectLogicOffsetService</code>，同样实现空的 run 方法，原因我们之前已经说过，<strong>不需要利用定时线程去判断逻辑偏移量是否过期，而是在使用时再进行判断</strong>。</p></li></ul><h2 id="CQ-相应表的删除"><a href="#CQ-相应表的删除" class="headerlink" title="CQ 相应表的删除"></a>CQ 相应表的删除</h2><p>当 Topic 被删除时，需要删除 CQ 存储单元和 Offset。分为 ConsumeQueueTable 以及 OffsetTable。RocksDBConsumeQueueStore 调用 destroy 方法：</p><pre class="line-numbers language-Java" data-language="Java"><code class="language-Java">@Override    public void destroy(ConsumeQueueInterface consumeQueue) throws RocksDBException &#123;        String topic &#x3D; consumeQueue.getTopic();        int queueId &#x3D; consumeQueue.getQueueId();        if (StringUtils.isEmpty(topic) || queueId &lt; 0 || !this.rocksDBStorage.hold()) &#123;            return;        &#125;        try (WriteBatch writeBatch &#x3D; new WriteBatch()) &#123;            this.rocksDBConsumeQueueTable.destroyCQ(topic, queueId, writeBatch);            this.rocksDBConsumeQueueOffsetTable.destroyOffset(topic, queueId, writeBatch);            this.rocksDBStorage.batchPut(writeBatch);        &#125; catch (RocksDBException e) &#123;            log.error(&quot;kv deleteTopic &#123;&#125; Failed.&quot;, topic, e);            throw e;        &#125; finally &#123;            this.rocksDBStorage.release();        &#125;    &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>进一步分析两表的删除：</p><ol><li><strong>ConsumeQueueTable: destroyCQ()</strong></li></ol><p>RocksDB 是支持范围查询的，这个范围是根据字节的字典序排序的，因此之前提到过的 Ctrl_1 标识，实际上在这里发挥的了作用，首先看 RocksDBConsumeQueue 表：</p><p>（所在类：RocksDBConsumeQueueTable）首先是调用的主方法，其中有两个构造 CQ 文件起点和终点的方法 buildDeleteCQKey，<strong>实际上是拼接了 Ctrl0 和 Ctrl2 的标识，利用字典序的大小关系确定删除的范围来删除 cq 文件</strong>，很巧妙。</p><pre class="line-numbers language-Java" data-language="Java"><code class="language-Java">public void destroyCQ(final String topic, final int queueId, WriteBatch writeBatch) throws RocksDBException &#123;    final byte[] topicBytes &#x3D; topic.getBytes(StandardCharsets.UTF_8);    final ByteBuffer cqStartKey &#x3D; buildDeleteCQKey(true, topicBytes, queueId);    final ByteBuffer cqEndKey &#x3D; buildDeleteCQKey(false, topicBytes, queueId);    writeBatch.deleteRange(this.defaultCFH, cqStartKey.array(), cqEndKey.array());    log.info(&quot;Rocksdb consumeQueue table delete topic. &#123;&#125;, &#123;&#125;&quot;, topic, queueId);&#125;&#x2F;&#x2F; 拼接private ByteBuffer buildDeleteCQKey(final boolean start, final byte[] topicBytes, final int queueId) &#123;    final ByteBuffer byteBuffer &#x3D; ByteBuffer.allocate(DELETE_CQ_KEY_LENGTH_WITHOUT_TOPIC_BYTES + topicBytes.length);    byteBuffer.putInt(topicBytes.length).put(CTRL_1).put(topicBytes).put(CTRL_1).putInt(queueId).put(start ? CTRL_0 : CTRL_2);    byteBuffer.flip();    &#x2F;&#x2F; buffer 翻转，刚好让 Ctrl0 和 2 组成了一个区间。    return byteBuffer;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="2"><li><strong>OffsetTable: destroyOffset()</strong></li></ol><pre class="line-numbers language-Java" data-language="Java"><code class="language-Java">public void destroyOffset(String topic, int queueId, WriteBatch writeBatch) throws RocksDBException &#123;        final byte[] topicBytes &#x3D; topic.getBytes(StandardCharsets.UTF_8);        final ByteBuffer minOffsetKey &#x3D; buildOffsetKeyByteBuffer(topicBytes, queueId, false);        byte[] minOffsetBytes &#x3D; this.rocksDBStorage.getOffset(minOffsetKey.array());        Long startCQOffset &#x3D; (minOffsetBytes !&#x3D; null) ? ByteBuffer.wrap(minOffsetBytes).getLong(OFFSET_CQ_OFFSET) : null;        final ByteBuffer maxOffsetKey &#x3D; buildOffsetKeyByteBuffer(topicBytes, queueId, true);        byte[] maxOffsetBytes &#x3D; this.rocksDBStorage.getOffset(maxOffsetKey.array());        Long endCQOffset &#x3D; (maxOffsetBytes !&#x3D; null) ? ByteBuffer.wrap(maxOffsetBytes).getLong(OFFSET_CQ_OFFSET) : null;        writeBatch.delete(this.offsetCFH, minOffsetKey.array());        writeBatch.delete(this.offsetCFH, maxOffsetKey.array());        String topicQueueId &#x3D; buildTopicQueueId(topic, queueId);        removeHeapMinCqOffset(topicQueueId);        removeHeapMaxCqOffset(topicQueueId);        log.info(&quot;RocksDB offset table delete topic: &#123;&#125;, queueId: &#123;&#125;, minOffset: &#123;&#125;, maxOffset: &#123;&#125;&quot;, topic, queueId,            startCQOffset, endCQOffset);    &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="CQ-脏数据截断-（truncateDirty）"><a href="#CQ-脏数据截断-（truncateDirty）" class="headerlink" title="CQ 脏数据截断 （truncateDirty）"></a>CQ 脏数据截断 （truncateDirty）</h2><p>我们不需要截断 RocksDBConsumeQueueTable 中的脏 CQ，因为 RocksDBConsumeQueueTable 中的脏 CQ 将在附加新消息时被新 KV 重写，或者在删除主题时被清理。但是 RocksDBConsumeQueueOffsetTable 中的脏偏移信息必须被截断，因为我们使用 RocksDBConsumeQueueOffsetTable 中的偏移信息来重建 topicQueueTable（RocksDBConsumeQueue#increaseQueueOffset），要保证偏移量正确。</p><h1 id="发展前景"><a href="#发展前景" class="headerlink" title="发展前景"></a>发展前景</h1><h2 id="Meta-数据存储"><a href="#Meta-数据存储" class="headerlink" title="Meta 数据存储"></a>Meta 数据存储</h2><p>RocketMQ 中 <strong>Topic 和 Subscription 的数据较大且频繁更新，</strong>同样可以考虑用 RocksDB 来做，目前社区也有了相关的实现，问题在于：</p><ul><li><p>在十万级甚至百万级的 Topic 和 Subscription 场景下，频繁的创建、更新与删除操作会触发元数据的持久化，而元数据的持久化是完整写入而并不是 append only。</p></li><li><p>这些操作会生成大量的大内存对象（如 <code>topicConfigTable</code> 的 JSON 字符串），在内存紧张时可能直接分配到老年代，导致频繁 Full GC。(read -&gt; update -&gt; write)</p></li></ul><h2 id="性能提升"><a href="#性能提升" class="headerlink" title="性能提升"></a>性能提升</h2><p>ref: <a href="https://xiaomi.f.mioffice.cn/wiki/UTZMw0JjIiwVonkX6emkrOAA4Tc">RocketMQ 引入 RocksDB</a></p><h2 id="稳定性分析"><a href="#稳定性分析" class="headerlink" title="稳定性分析"></a>稳定性分析</h2><p>RocksDB 开启后只对 CQ 文件的读写有影响，不再采用独立 CQ 文件的方式进行管理，并且 RocksDB 提供了更高的扩展性，我们可以自定义压缩策略等配置，这样在高并发场景下，不需要频繁的对 CQ 文件进行操作，并且减少了 CommitLog 写入时对 CQ 的更新占用，能够提供更加稳定可靠的消费环境。</p><h2 id="成本分析"><a href="#成本分析" class="headerlink" title="成本分析"></a>成本分析</h2><p>引入了新的组件，成本会如何变化。因为是嵌入式的数据库，且是单机的，我们是不是可以把它当做一个本机的文件操作和存储系统？并且本质上还是写到磁盘，可能变化是不大的，但是对于内存的要求更高。</p><p>同等规模的 CQ 数据存储，RocksDB 由于自身的数据压缩，以及设计好的表和数据格式，RocksDB 占用相对极少。</p><table><thead><tr><th>Consumer Queue 规模</th><th>RocksDB</th><th>文件系统</th></tr></thead><tbody><tr><td>2w</td><td>40 MB</td><td>113 GB</td></tr><tr><td>10w</td><td>100 MB</td><td>566 GB</td></tr></tbody></table><p>对于文件系统的计算：<code>queue size (5.x MB) * num 数量</code></p><h2 id="MQTT"><a href="#MQTT" class="headerlink" title="MQTT"></a>MQTT</h2><p>对于 MQTT，同样可以利用 RocksDB 作为存储，来解决一些问题，例如：</p><ul><li><p>基于 LMQ 优化 MQTT 的场景，可以将 LMQ 相关数据放到 RocksDB 中存储和管理。</p></li><li><p>MQTT 元数据的存储和管理。</p></li></ul><h1 id="其他-KV-数据库可行性分析"><a href="#其他-KV-数据库可行性分析" class="headerlink" title="其他 KV 数据库可行性分析"></a>其他 KV 数据库可行性分析</h1><p>既然上面的接口是扩展性的，说明也可以扩展其他的 KV 数据库，简单分析一下其他数据库是不是也能实现会不会有更加优秀的表现。</p><h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><ol><li><p><a href="https://www.bilibili.com/video/BV19u4y1r7Qd/">20-2 RocketMQ 百万队列能力支持 – rocksdb kv 存储 赵福建_哔哩哔哩_bilibili</a></p></li><li><p><a href="https://github.com/apache/rocketmq/wiki/RIP-66-Support-KV(Rocksdb)-Storage">https://github.com/apache/rocketmq/wiki/RIP-66-Support-KV(Rocksdb)-Storage</a></p></li><li><p><a href="https://github.com/apache/rocketmq/issues/8589">https://github.com/apache/rocketmq/issues/8589</a> 原地升级方案</p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;全网仅有!!，这个方案全网都没有什么参考资料。最近写了一篇 RocketMQ 引入 RocksDB 的设计方案，在写这个的过程中我对 RocketMQ 的底层存储和架构更加清晰了，原本只熟悉 kafka，而 RocketMQ 只停留在使用以及存储的</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Kafka 副本机制详解</title>
    <link href="http://example.com/2024/10/07/kafka-fu-ben-ji-zhi-xiang-jie/"/>
    <id>http://example.com/2024/10/07/kafka-fu-ben-ji-zhi-xiang-jie/</id>
    <published>2024-10-07T08:01:00.000Z</published>
    <updated>2024-10-07T08:03:02.855Z</updated>
    
    <content type="html"><![CDATA[<h2 id="总览"><a href="#总览" class="headerlink" title="总览"></a>总览</h2><p>主要介绍 kafka 的副本机制是什么，如何处理读写，副本存储在哪，以及 kafka 如何根据副本实现高可用，ISR 机制。为什么不用 mysql 那种主从架构（比如 follower 节点来提高读性能，leader 节点只负责写）有什么好处，mysql 那边为什么不这样设计之类的问题。</p><h2 id="一般的分布式副本机制是什么"><a href="#一般的分布式副本机制是什么" class="headerlink" title="一般的分布式副本机制是什么"></a>一般的分布式副本机制是什么</h2><p>一般的分布式副本机制实际上就是官方给出的说法，主要是说明副本机制需要保证什么特性。</p><p>副本机制（replication），也可以说是备份机制。体现在<strong>一个分布式系统中多个互联的机器上保存相同的数据副本</strong>。</p><p>有什么好处？</p><ol><li><mark style="background: #FFFF00;">提高数据冗余</mark>：<u>当系统的一部分组件失效宕机了，系统仍然能够正常运行，保证可用性和数据持久性</u>。（数据不丢失以及系统不会停止服务）</li><li><mark style="background: #FFFF00;">提高伸缩能力</mark>：通过增加机器，来横向的提升读写性能，进一步提高吞吐量。</li><li><mark style="background: #FFFF00;">数据局部性访问</mark>：其实<u>类似 CDN</u>，也就是<u>客户端在读取数据的时候可以优先读取最近的数据副本来降低延时</u>。</li></ol><p>这个是通用的说法，但是 kafka 实际上只实现了第一点，也就是数据冗余。至于增加机器来提高读性能，实际上 kafka 不能实现，后续会重点介绍为什么这样设计。至于<em>局部性访问</em>，那就更难实现了，你<strong>怎么权衡数据存储到位置和多个客户端之间的距离，会不会涉及到大量的数据转移？数据不一致的情况</strong>？</p><p>我们还是先来看 kafka 是怎么使用这个副本机制的。</p><h2 id="Kafka-的副本机制"><a href="#Kafka-的副本机制" class="headerlink" title="Kafka 的副本机制"></a>Kafka 的副本机制</h2><p>副本，这里指的是 partition 下的副本，也就是我们常说的 replica，<mark style="background: #FFFF00;">实际上就是一个只能追加写消息的提交日志</mark>。同一个分区下的所有副本都是保存的相同的消息序列，这些副本可以分布在一个集群中的多个 broker 上，因此可以保证一些 broker 宕机，消息不会丢失，实现持久性和高可用。</p><p>因此，实际上 kafka 的每个 broker 上可能都存放了成百上千个副本。这些副本来自<strong>不同主题 Topic 下的不同分区 partition 的不同副本</strong>。</p><blockquote><ul><li><strong>副本数量 ≤ 节点数量</strong>：副本数量不能超过集群中的节点数量。如果集群中有 3 个节点，那么副本数量最多也只能是 3。否则，Kafka 无法在不同的节点上分布这些副本。</li></ul></blockquote><p>比如一个三个 broker 节点的集群，可以看到分区以及副本是分布在各个 broker 上的：<br><img src="/images/san.png"></p><h2 id="副本的角色如何划分"><a href="#副本的角色如何划分" class="headerlink" title="副本的角色如何划分"></a>副本的角色如何划分</h2><p>副本并不都是一样的角色，kafka 的副本分为 leader 副本和 follower 副本。其中他们的职责如下：</p><ul><li>Leader 副本<mark style="background: #FFFF00;">只负责处理读写请求</mark>。</li><li>follower 副本只负责从 leader 副本<mark style="background: #FFFF00;">异步地拉取消息同步到它自己的消息日志</mark>来保证和 leader 副本的数据一致。</li></ul><p>客户端（生产者，消费者）与副本的交互关系可参考：</p><p><img src="/images/producer.png"></p><p>当 leader 副本挂掉了，kafka 集群如果使用 zookpeeper 作为分布式协调组件，那就会用它从 follower 副本中选举出新的 leader 副本，继续进行读写请求的处理。当旧的 leader 副本恢复了，会再次被加入到副本集合。</p><blockquote><p>如果在 KRaft 模式下，<u>Kafka 使用 Raft 共识算法来管理集群的元数据和选举新 Leader 副本</u>。需要补充：<a href="kafka%20kraft%20%E6%A8%A1%E5%BC%8F%E4%B8%8B%E7%9A%84%E5%8D%8F%E8%B0%83%E7%AE%97%E6%B3%95.md">kafka kraft 模式下的协调算法</a> </p></blockquote><h2 id="为什么-follower-副本不负责读写处理"><a href="#为什么-follower-副本不负责读写处理" class="headerlink" title="为什么 follower 副本不负责读写处理"></a>为什么 follower 副本不负责读写处理</h2><p>为什么不像其他的主从架构系统那样，比如 mysql，redis 的主从架构，都是主节点负责写，从节点负责读，来提高读的吞吐量。</p><p>这种设计有两种好处：</p><ol><li><mark style="background: #FFFF00;">可以实现“read your writes”</mark></li></ol><p>直译过来就是说你可以读取到你自己写的数据。类比到 kafka 中就是生产者写入消息后，可以立即用消费者读取到这条新写入的消息。</p><p>由于 follower 副本需要<strong>异步拉取 leader 副本的消息进行同步</strong>，因此消费者可能从 follower 中拉取不到最新写入的消息，导致消息的滞后处理。</p><ol start="2"><li><mark style="background: #FFFF00;">可以实现单调读</mark></li></ol><p>当消费者多次重复消费同一条消息时，可能会出现这种情况：发现读取这条消息，有时候能拉到，有什么拉不到。主要就是因为多个 follower 副本作为读系统时<u>没有全部同步到这条消息</u>。</p><h2 id="ISR-机制保证消息同步"><a href="#ISR-机制保证消息同步" class="headerlink" title="ISR 机制保证消息同步"></a>ISR 机制保证消息同步</h2><p>In-sync Replicas 同步副本集合。</p><p>概念：<mark style="background: #FFFF00;">ISR 集合实际上就是一个存放与 leader 副本保持同步的副本集合</mark>。当然也包括 leader 副本本身。</p><p>作用：作用是什么？其实主要是用来<mark style="background: #FFFF00;">在 leader 副本挂掉的情况下，从 ISR 副本集合中选举出一个新的 leader 副本</mark>。</p><blockquote><p>因为你总不能用一个没有同步的副本集合作为 leader 副本吧，这不是<strong>导致消息丢失了</strong>嘛？</p></blockquote><p>关于“同步”，如何确定一个副本是否与 leader 副本同步，follower 副本异步拉取 leader 副本消息进行同步。由于是异步，那肯定不能保证实时的同步（即消息的偏移量都到达了同一个位置）。</p><p>比如下面这个图，是不是两个 follower 都是不同步的副本？<br><img src="/images/kafka-1.png"></p><p>不一定，因为 kafka 定义的同步实际上是<mark style="background: #FFFF00;">由一段时间内 follower 副本是否能够与 leader 副本同步来决定的</mark>。就是说，会给你一定的缓冲时间，毕竟你是异步。ß</p><p>至于这个时间怎么确定，其实是由 Broker 端参数：<code>replica.lag.time.max.ms</code> 决定的，比如设置为 10 秒，那就说明 10 秒内，只要你的 follower 副本能够与 leader 副本同步成功，那就算是同步了。</p><blockquote><p>比如 leader 副本写入消息的速度太快了，follower 副本跟不上速度，就会出现这种规定时间内无法同步，进而被踢出 ISR 集合，所以 <strong>ISR 集合是动态的</strong>。我们怎么减少这种被踢出 ISR 集合的情况？</p><ul><li>增加等待时间的值，给 broker 更多的同步时间，或者增加可以接受的滞后的消息数量参数值：<code>replica.lag.max.messages</code>。</li><li>降低生产者的发送速率，防止 follower 跟不上。</li><li>压缩消息大小，提高同步速度。</li></ul></blockquote><h2 id="如果-ISR-集合空了怎么办"><a href="#如果-ISR-集合空了怎么办" class="headerlink" title="如果 ISR 集合空了怎么办"></a>如果 ISR 集合空了怎么办</h2><p>一个小思考：</p><p>ISR 集合中的副本是可以参与 leader 副本选举的，而 ISR 集合之外的存活副本一般被认为是不能与 leader 副本保持同步的，所以正常情况下不参与 leader 副本的选举。而在特殊情况下，ISR 集合中可能一个副本都不存在（即唯一的同步副本 leader 副本也挂掉了），那只能退而求其次使用 unclear 的副本作为选举的 base 副本。</p><p>空了，说明 leader 副本挂掉了，且没有其他 follower 副本与 leader 副本同步。也就是没法进行 leader 选举了，也就无法正常处理客户端的读写请求。</p><p>这种情况下需要看你怎么考虑：</p><ol><li>开启 Broker 端参数 <code>unclean.leader.election.enable</code> 控制是否允许 Unclean 领导者选举。</li></ol><p>Unclear 也就是没有与 leader 副本保持同步的副本，如果开启这个参数，也就可以从 unclear 副本中选举出一个 leader 副本，继续提供客户端的读写请求，<mark style="background: #FFFF00;">保证了可用性，但是会丢失一定的消息，丧失持久性</mark>。</p><ol start="2"><li>不使用 unclear</li></ol><p>由于没有可用于选举 leader 的副本，因此 broker 无法进行 leader 选举，因此无法继续提供服务。<mark style="background: #FFFF00;">丧失可用性，但是消息不会丢失</mark>。</p><p>扩展：这里实际上也是分布式系统 CAP 理论中的 CP 和 AP 系统的决策，也就是要<strong>保证强一致性系统还是高可用性系统</strong>。CAP 理论参考：<a href="../../%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F--%E6%B0%B8%E4%B9%85%E7%AC%94%E8%AE%B0-%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E4%BB%A5%E5%8F%8ACAP%20%E5%92%8C%20BASE%20%E7%90%86%E8%AE%BA.md">分布式–永久笔记-分布式事务以及CAP 和 BASE 理论</a> </p><hr><h2 id="扩展问题"><a href="#扩展问题" class="headerlink" title="扩展问题"></a>扩展问题</h2><p>类似关于 MySQ 架构，以及副本机制设计的疑问点都可以在这里解答。</p><p>TODO.</p><h3 id="补充-：kafka-是怎么通过-ISR-和多副本保证持久性和消息不丢失的？"><a href="#补充-：kafka-是怎么通过-ISR-和多副本保证持久性和消息不丢失的？" class="headerlink" title="补充 ：kafka 是怎么通过 ISR 和多副本保证持久性和消息不丢失的？"></a>补充 ：kafka 是怎么通过 ISR 和多副本保证持久性和消息不丢失的？</h3><p>客户端怎么保证消息不丢失，相关引用：<a href="kafka%20%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E6%B6%88%E6%81%AF%E4%B8%8D%E4%B8%A2%E5%A4%B1.md">kafka 怎么保证消息不丢失</a> </p><ol><li><mark style="background: #FFFF00;">ACK 机制</mark>，也就是生产者消息确认的级别。<ul><li>如果设置为 0,  那生产者不会等待响应，只要发送了就任务消息发送成功。</li><li>如果设置为 1，只要 leader 副本写入消息成功就算发送成功。</li><li>如果设置为 all，必须使 ISR 集合中的所有副本都写入消息成功之后才算发送成功。持久性保证更强。</li></ul></li><li><mark style="background: #FFFF00;">多副本机制</mark>：即多副本分布在不同的 broker 上实现数据冗余，保证消息不丢失持久化和高可用。</li><li><mark style="background: #FFFF00;">禁用 unclear leader 选举</mark>，因为这种情况下会选择没有完全与 leader 副本同步的副本进行选举，可能会丢失一部分消息。</li></ol><p>而且你也可以自定义设置当 ack 机制为 all 时，需要满足的最小 ISR 集合元素数量，比如设置为 2，那在只有一个 leader 副本的情况下，也不能算作发送成功。必须等 ISR 集合中出现 follower 副本才算发送成功。这样的话，leader 副本突然挂掉也不会导致消息丢失。但是可能会丧失一定可用性，还是之前写的，leader 副本写入太快之类的问题导致 follower 副本无法同步。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;总览&quot;&gt;&lt;a href=&quot;#总览&quot; class=&quot;headerlink&quot; title=&quot;总览&quot;&gt;&lt;/a&gt;总览&lt;/h2&gt;&lt;p&gt;主要介绍 kafka 的副本机制是什么，如何处理读写，副本存储在哪，以及 kafka 如何根据副本实现高可用，ISR 机制。为什么不用 my</summary>
      
    
    
    
    <category term="kafka" scheme="http://example.com/categories/kafka/"/>
    
    
    <category term="kafka" scheme="http://example.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>分布式事务之 CAP 和 BASE 理论基础</title>
    <link href="http://example.com/2024/09/16/fen-bu-shi-shi-wu-zhi-cap-he-base-li-lun-ji-chu/"/>
    <id>http://example.com/2024/09/16/fen-bu-shi-shi-wu-zhi-cap-he-base-li-lun-ji-chu/</id>
    <published>2024-09-16T09:02:00.000Z</published>
    <updated>2024-09-16T09:06:02.058Z</updated>
    
    <content type="html"><![CDATA[<p>#CAP理论</p><h2 id="分布式事务"><a href="#分布式事务" class="headerlink" title="分布式事务"></a>分布式事务</h2><p>如果存在跨数据库的数据一致性保证时，就需要用到分布式事务，比如分库。</p><p>比较常见的场景是，商城业务，如果项目体量比较大，那肯定需要进行分库分表，因为<u>单库和单表的性能有瓶颈，而且数据安全无法保证</u>。</p><p>由于数据库事务只能限制在当前数据库内，所以无法实现跨数据库事务保证。</p><p><mark style="background: #FFFF00;">分布式事务的目标是：多个相关的数据库之间能保证数据一致性</mark>。</p><blockquote><p>举个例子：银行业务，有俩人进行转账，A 扣钱，B 收到钱，这个存钱的账户信息放到一个数据库 <code>D1</code>，然后相应的交易记录放到数据库 <code>D2</code>。如果这个过程中，一部分完成了（扣钱和收钱完成），但是订单存储失败，那在年终对账的时候就要吃牢饭了。</p></blockquote><p>那么，我们继续来探讨一下如何实现分布式事务。</p><h2 id="分布式事务理论基础"><a href="#分布式事务理论基础" class="headerlink" title="分布式事务理论基础"></a>分布式事务理论基础</h2><p>先谈理论，是为了让我们知道我们最终实现的系统能变成什么样的。是不是类似 MySQL 的 ACID 特性的事务？先给出结论，接下来将根据这个主题探讨：</p><p><mark style="background: #FFFF00;">ACID 是数据库事务完整性的理论，CAP 是分布式系统设计理论，BASE 是 CAP 理论中 AP 方案的延伸。</mark></p><h3 id="CAP"><a href="#CAP" class="headerlink" title="CAP"></a>CAP</h3><p>CAP 代表的是：</p><ul><li><code>一致性（Consistence）</code> : 所有节点访问同一份最新的数据副本  </li><li><code>可用性（Availability）</code>: 非故障的节点在合理的时间内返回合理的响应（不是错误或者超时的响应）。  </li><li><code>分区容错性（Partition tolerance）</code> : 分布式系统出现网络分区的时候，仍然能够对外提供服务。</li></ul><p>其中网络分区，指的是<u>因为某些故障，导致一部分节点脱离了整个系统所在的网络，节点之间网络无法通信，形成了分区</u>。</p><hr><ol><li><mark style="background: #FFFF00;">我们实现的系统是否能完美的实现这三个特点呢</mark>？ans：No！</li></ol><p>首要的一点是，<strong>这个分区容错 P，必须要有</strong>。因为在实际情况中，<u>网络延迟，节点故障，或者节点通信故障</u>等问题是经常发生的，如果不保证在这种情况下的容错，那系统的数据大概率是乱掉的。</p><ol start="2"><li><mark style="background: #FFFF00;">既然 P 一定要有，CA 能共存吗</mark>？ans：也不能</li></ol><p><strong>为啥无同时保证 CA 呢？</strong><br>举个例子：若系统出现“分区”，系统中的某个节点在进行写操作。为了保证 C，必须要禁止其他节点的读写操作，这就和 A 发生冲突了（可用性）。如果为了保证 A，其他节点的读写操作正常的话，那就和 C 发生冲突了（多节点同时操作数据导致不一致）。</p><ol start="3"><li><mark style="background: #FFFF00;">保证 P 的特点，那就衍生出了两种系统，CP 和 AP</mark></li></ol><p>CP 就是强一致性的系统，比较适合银行这种，哪怕你请求失败也不能把数据的一致性抛弃了，必须强一致，避免数据异常。</p><p>AP 就是高可用优先，也就是说，两个节点的数据可能是不一致的，适合对数据的要求不这么敏感的场景，可以接受”暂时的误差”。</p><blockquote><ul><li>比如社区服务：你这次没获取到最新的帖子有问题吗，没问题，我可以给你返还旧数据，等一段时间你刷新了最新帖子就有了，完全没问题。</li><li>比如社交媒体平台：用户发布一条状态，系统立即返回成功响应，其他用户可能暂时看不到这条状态，但经过一段时间的后台同步，所有用户最终都能看到该状态。</li></ul></blockquote><p>假设我们有一个分布式系统，使用了 AP 优先的策略，但通过后台数据同步机制实现最终一致性，那么它的数据流向可以是这样的：<br><img src="/images/AP.png"></p><ol><li><strong>写操作</strong>：<ul><li>客户端 A 向节点 1 写入数据。</li><li>节点 1 立即返回成功响应，<strong>保证可用性</strong>。</li><li>节点 1 将数据变更记录到日志，并异步地将变更同步到其他节点。</li></ul></li><li><strong>读操作</strong>：<ul><li>客户端 B 向节点 2 读取数据。</li><li>如果节点 2 尚未收到同步的最新数据，可能会返回旧数据。</li><li>在后台数据同步完成后，节点 2 的数据将与节点 1 一致，最终达到一致性。</li></ul></li></ol><h3 id="BASE"><a href="#BASE" class="headerlink" title="BASE"></a>BASE</h3><p>BASE 理论代表的是：</p><ul><li><code>基本可用（Basically Available）</code>：系统在分布式环境中，允许在部分节点故障的情况下，仍然能提供基本的可用性。</li><li><code>软状态（Soft State）</code>：系统允许存在中间状态，即数据在不同节点之间的副本不必时刻保持强一致性。</li><li><code>最终一致性（Eventual Consistency）</code>：系统保证在没有新的更新操作后，所有的数据副本最终会达到一致性。</li></ul><p>其中，基本可用指的是<u>系统在部分功能失效或性能下降的情况下，仍然能对外提供服务</u>。</p><hr><ol><li><mark style="background: #FFFF00;">我们实现的系统是否能完美的实现 BASE 理论的三个特点呢</mark>？ans：No！</li></ol><p>首要的一点是，<strong>基本可用性 BA 和最终一致性 EC 需要在软状态 SS 的支持下才能实现</strong>。因为在实际情况中，<u>网络延迟、节点故障、数据同步延迟</u>等问题是经常发生的，如果不允许存在软状态，那系统的数据一致性和可用性就无法同时实现。</p><ol start="2"><li><mark style="background: #FFFF00;">既然 SS 是必要的，BA 和 EC 能共存吗</mark>？ans：可以</li></ol><p><strong>为啥能同时保证 BA 和 EC 呢？</strong><br>举个例子：若系统出现“分区”，系统中的某个节点在进行写操作。为了保证基本可用性（BA），节点可以立即返回成功响应而不等待其他节点的确认，这样用户体验不受影响。而为了保证最终一致性（EC），系统可以在后台异步进行数据同步，确保所有节点最终达到一致状态。</p><ol start="3"><li><mark style="background: #FFFF00;">保证 SS 的特点，那就衍生出了 BASE 系统的应用场景</mark></li></ol><p>BASE 更适合那些对数据一致性要求不高，但对系统可用性和响应速度要求较高的场景。通过牺牲强一致性，系统可以在高并发、分布式环境中提供更好的可用性和性能。</p><p>可以看到和 AP 极其相似，也符合我们的主题思想，<u>BASE 是 AP 的理论延伸</u>。</p><p>关于分布式事务的实现方式还有很多，敬请期待下部分内容！</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;#CAP理论&lt;/p&gt;
&lt;h2 id=&quot;分布式事务&quot;&gt;&lt;a href=&quot;#分布式事务&quot; class=&quot;headerlink&quot; title=&quot;分布式事务&quot;&gt;&lt;/a&gt;分布式事务&lt;/h2&gt;&lt;p&gt;如果存在跨数据库的数据一致性保证时，就需要用到分布式事务，比如分库。&lt;/p&gt;
&lt;p&gt;比较</summary>
      
    
    
    
    <category term="分布式" scheme="http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
    
    <category term="分布式事务" scheme="http://example.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"/>
    
  </entry>
  
  <entry>
    <title>扫码登录流程</title>
    <link href="http://example.com/2024/08/19/sao-ma-deng-lu-liu-cheng/"/>
    <id>http://example.com/2024/08/19/sao-ma-deng-lu-liu-cheng/</id>
    <published>2024-08-19T08:00:00.000Z</published>
    <updated>2024-08-19T08:00:51.209Z</updated>
    
    <content type="html"><![CDATA[<p>二维码信息里主要包括唯一的二维码 ID，过期的时间，还有扫描状态：未扫描、已扫描、已失效。</p><h2 id="扫码登录流程"><a href="#扫码登录流程" class="headerlink" title="扫码登录流程"></a>扫码登录流程</h2><p>我们从客户端（用户浏览器）到二维码服务端，以及手机到手机端服务器，手机端服务器响应数据到二维码服务器等。（这里的服务器可以当做一个服务）</p><h3 id="客户端到二维码服务"><a href="#客户端到二维码服务" class="headerlink" title="客户端到二维码服务"></a>客户端到二维码服务</h3><p>用户打开网站的登录页面的时候，浏览器会向二维码服务器发送一个获取登录二维码的请求。二维码服务器收到请求之后，会随机的生成一个 uuid，通常是唯一的。将这个 uuid 作为 key 存储到 redis 服务器中，同时会设置一个过期时间，过期之后用户就要重新网页刷新来获取。</p><p>之后会将这个 uuid 和本公司的验证字符串和在一起通过二维码生成接口生成图片，将二维码图片信息和 uuid 返回给浏览器，浏览器拿到 uuid 和图片之后，每隔一定时间就向服务器发送一个判断登陆是否成功的请求，请求中会携带 uuid 作为当前页面的标识符。</p><h3 id="手机到手机端服务器"><a href="#手机到手机端服务器" class="headerlink" title="手机到手机端服务器"></a>手机到手机端服务器</h3><p>用户拿起手机扫描二维码之后，就会得到二维码中包含的验证信息和 uuid，由于手机端已经进行过登陆验证，在访问手机端服务器的时候参数中都会携带一个用户信息 token，这个 token 是在第一次手机登陆过程中产生并且长期有效的。手机端服务器通过这个 token 就可以解析出用户的类似于 userId 等信息。 </p><p>然后手机端服务器会将解析出来的数据作为参数向二维码服务器发送登陆请求，二维码服务器收到请求之后会对参数进行校验，确定是否为用户登录请求接口。如果是就返回给手机一个确认信息。手机端收到信息之后，登陆确认框会显示给用户，用户进行登陆确认之后手机再次发送请求，redis 服务器拿到信息之后，会将刚才 uuid 的 key 的 value 设置为 userId。 </p><p>这样浏览器再次发送请求的时候就可以在 redis 服务器中拿到用户的 id，并调用登陆方法生成一个浏览器端 token。浏览器再发送请求，就会将用户信息的 token 返回给浏览器。</p><blockquote><ul><li>这里存储用户 id 而不是直接存储用户信息是因为手机端的用户信息，不一定是和浏览器端的用户信息完全一致。</li><li>传 token 是为了安全，token 是被加密的，直接传 userId 可能有被窃取的风险。</li></ul></blockquote><h3 id="浏览器通过该-Token-的后续登录流程"><a href="#浏览器通过该-Token-的后续登录流程" class="headerlink" title="浏览器通过该 Token 的后续登录流程"></a>浏览器通过该 Token 的后续登录流程</h3><ol><li>认证成功后，会对当前用户数据进行加密，生成一个加密字符串 token，返还给客户端（服务器端并不进行保存）。</li><li>浏览器会将接收到的 token 值存储在 Local Storage 中（通过 js 代码写入 Local Storage，通过 js 获取，并不会像 cookie 一样自动携带）。</li><li>再次访问时服务器端对 token 值的处理：<ul><li>服务器对浏览器传来的 token 值进行解密，解密完成后进行用户数据的查询。</li><li>如果查询成功，则通过认证，实现状态保持。</li><li>即使有了多台服务器，服务器也只是做了 token 的解密和用户数据的查询，不需要在服务端去保留用户的认证信息或者会话信息。</li><li>这就意味着基于 token 认证机制的应用不需要去考虑用户在哪一台服务器登录了，为应用的扩展提供了便利，解决了 session 扩展性的弊端（session 需要保存 sessionid 到服务端，在分布式场景下，多个服务器之间的 session 共享很麻烦，如果需要同步还需要进行通信以及延迟问题）。</li></ul></li></ol><h3 id="时序图"><a href="#时序图" class="headerlink" title="时序图"></a>时序图</h3><p>画个时序图，应该更好理解一些。</p><p><img src="/images/%E6%B5%81%E7%A8%8B.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;二维码信息里主要包括唯一的二维码 ID，过期的时间，还有扫描状态：未扫描、已扫描、已失效。&lt;/p&gt;
&lt;h2 id=&quot;扫码登录流程&quot;&gt;&lt;a href=&quot;#扫码登录流程&quot; class=&quot;headerlink&quot; title=&quot;扫码登录流程&quot;&gt;&lt;/a&gt;扫码登录流程&lt;/h2&gt;&lt;p&gt;我</summary>
      
    
    
    
    
    <category term="web" scheme="http://example.com/tags/web/"/>
    
    <category term="系统" scheme="http://example.com/tags/%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>提升 IO 密集型服务性能的异步处理模型</title>
    <link href="http://example.com/2024/08/18/ti-sheng-io-mi-ji-xing-fu-wu-xing-neng-de-yi-bu-chu-li-mo-xing/"/>
    <id>http://example.com/2024/08/18/ti-sheng-io-mi-ji-xing-fu-wu-xing-neng-de-yi-bu-chu-li-mo-xing/</id>
    <published>2024-08-18T13:04:00.000Z</published>
    <updated>2024-08-18T13:04:50.909Z</updated>
    
    <content type="html"><![CDATA[<p>电子商务平台的订单处理服务是典型的 I&#x2F;O 密集型（I&#x2F;O Bound）服务。</p><p><strong>IO 密集型任务</strong>：一般就是说需要大量请求数据库和网络请求这些，如果请求比较多，系统大部分时间都花在等待 IO 操作完成。也就导致系统的性能瓶颈是在 IO 的延迟上。因此需要采用<u>并发加载</u>的方式，减少系统串行等待 IO 操作导致的系统性能下降。</p><h2 id="并发加载的方式"><a href="#并发加载的方式" class="headerlink" title="并发加载的方式"></a>并发加载的方式</h2><p>主要讲三种方式，同步模型，线程池异步，NIO 异步模型，CompletableFuture 的异步模型。</p><h3 id="同步模型"><a href="#同步模型" class="headerlink" title="同步模型"></a>同步模型</h3><p>很经典，也就是一个服务调用完返回结果，再去调另一个服务，之后逐个返回结果拼成 VO（响应的数据结果）。</p><p><img src="/images/%E5%90%8C%E6%AD%A5%E6%A8%A1%E5%9E%8B.png"></p><p>这种其实不用多说，肯定是最不可取的，我们至少需要引入异步操作。</p><h3 id="线程池异步"><a href="#线程池异步" class="headerlink" title="线程池异步"></a>线程池异步</h3><p><mark style="background: #FFFF00;">线程池的异步实际上是线程的并发操作，不会阻塞主线程</mark>。</p><p>我们可以通过引入线程池，也就是利用线程池能够管理线程的特点：</p><ul><li>线程可复用，避免线程重复创建</li><li>异步操作，直接分配线程去处理请求，不会阻塞主线程</li><li>任务执行完成后线程被释放，并且能够被分配给其他任务</li></ul><p>这样，我们可以在一个请求来到时，把向各个服务的请求交给线程池处理，我们再拿到响应的数据进行封装。这样能避免串行场景下的等待时间。但是，这样还有问题：</p><h4 id="CPU-仍然资源利用率低"><a href="#CPU-仍然资源利用率低" class="headerlink" title="CPU 仍然资源利用率低"></a>CPU 仍然资源利用率低</h4><p>虽然等待时间减少了，但是这些子线程仍然需要等待 IO 操作，CPU 仍然处于等待 IO 操作的空闲状态。这样也会导致 CPU 资源利用率低。</p><p>所以，一般<u>通过回调去防止阻塞</u>，也就是在执行 IO 操作时做异步处理，当 IO 操作完成之后，通知线程 IO 操作完成，需要继续执行数据的处理和响应。进而减少 CPU 阻塞时间。然而这种场景只适合少量请求的回调，如果回调函数过多，会造成”回调地狱”问题，使得代码可读性和可维护性降低。</p><h4 id="为增加并发度，需要增大线程池容量或数量"><a href="#为增加并发度，需要增大线程池容量或数量" class="headerlink" title="为增加并发度，需要增大线程池容量或数量"></a>为增加并发度，需要增大线程池容量或数量</h4><p>CPU 可以调度的线程数量更多了，但是竞争更激烈了，可能会发生频繁的上下文切换，导致资源白白消耗。并且线程池的参数也不好设置，大量空闲线程也会占用系统资源。</p><p>所以线程池异步实际上也不是完全合适的。</p><blockquote><p>补充：线程池是可以通过 Future 类去接受子线程执行的结果的，可以通过它的 get 方法等待任务执行完成之后获取结果，如果任务还没有结束，则会阻塞在获取操作那里，等待任务完成。</p></blockquote><h3 id="NIO-异步"><a href="#NIO-异步" class="headerlink" title="NIO 异步"></a>NIO 异步</h3><p>和线程池的异步不同，NIO 异步基于事件驱动模型和 selector 机制完成异步加载。</p><ul><li><strong>Selector</strong>：负责监控所有注册的 Channel（Channel 通道实际上就是 I&#x2F;O 连接，比如文件传输通道，Socket 套接字）。</li><li><strong>Channel 的非阻塞模式</strong>：当需要进行 I&#x2F;O 操作时，Channel 不会进行等待，而是立即返回继续处理其他请求。Selector 会监控这些 Channel，当 I&#x2F;O 操作可以进行时（例如数据准备好读取或可以写入），Selector 会通知应用程序处理这些 I&#x2F;O 操作。</li><li><strong>事件驱动模型</strong>：当有 I&#x2F;O 事件（如 read、write、accept 等操作）发生时，Selector 会检测到并返回一个包含所有准备好进行 I&#x2F;O 操作的 Channel 的 SelectionKey 集合。应用程序迭代这个集合，并处理每个 Channel 上的 I&#x2F;O 事件。通过这种方式，I&#x2F;O 操作是由事件驱动的，而不是由线程阻塞等待。</li></ul><p><img src="/images/NIO.png"></p><p><mark style="background: #FFFF00;">那它是怎么和处理请求的线程联系起来的呢</mark>？</p><p>这点，我的理解是：<u>NIO 的选择器能够监听所有的 IO 操作，然后通过 Channel 的方式将这些请求提交给线程池处理，也就不会阻塞主线程了</u>。</p><p>所以和线程池的根本区别是：线程池实际上没有把 IO 操作完全的抛离出去，而是用子线程去处理请求，如果需要使用回调函数则可能会引发回调地狱。而 NIO 呢，是把所有会阻塞线程的 IO 操作异步的通过 selector 去将 IO 操作传递给线程池的线程去做。</p><h4 id="NIO-的不足"><a href="#NIO-的不足" class="headerlink" title="NIO 的不足"></a>NIO 的不足</h4><p><mark style="background: #FFFF00;">NIO 主要适用于高并发的网络通信和 I&#x2F;O 操作，不适用于所有异步任务</mark>。也就是那种需要大量用到 CPU 的请求，NIO 的性能还是有限的。而且 NIO 模型的代码复杂度很高，涉及到缓存，字节，缓冲区等概念模型，编码难度大，不好维护。</p><p>而 CompletableFuture ，更适合 CPU 密集型以及需要大量处理业务逻辑的异步处理任务，同时也能够兼容这种 IO 密集型的任务（因为回调地狱）。能够结合线程池实现任务的异步执行，并且它是基于 Future 的进化版，能够获取异步任务执行结果，总之很全面，下面我们开始继续介绍。</p><blockquote><p>当然，NIO 和 CompletableFuture 也是可以联用的。</p></blockquote><h3 id="CompletableFuture-异步模型"><a href="#CompletableFuture-异步模型" class="headerlink" title="CompletableFuture 异步模型"></a>CompletableFuture 异步模型</h3><p>CompletableFuture 是基于 Future 的进化版，能够解决传统情况下 ListenableFuture 引发的回调地狱问题。因为传统场景下，Future 实现异步和回调是这样的：</p><ul><li>每个异步操作都需要嵌套多个回调函数（比如成功的结果回调，失败的结果回调，或者特殊情况下的回调结果），这种嵌套很容易导致所谓的“回调地狱”，使得代码难以阅读和维护。</li></ul><h4 id="ListenableFuture-的实现方式"><a href="#ListenableFuture-的实现方式" class="headerlink" title="ListenableFuture 的实现方式"></a>ListenableFuture 的实现方式</h4><p>比如下面这段代码是 ListenableFuture 的异步实现：</p><ul><li>定义了一个线程池，两个异步任务</li><li>两个异步任务 AB 执行结束后，通过 AllAsList 方法组合两个异步任务执行的结果，之后再给这个聚合的结果加上一个回调函数。</li><li>这两个异步任务结束后，会调用回调函数，其中首先打印之前的两个异步任务的结果，然后再新增一个异步任务 C 提交给线程池，再给 C 加上回调函数，依次类推，最后打印出所有异步任务的结果</li><li>如果异步任务没有执行完成，则获取异步结果的方法会被阻塞（而 CompletableFuture 虽然通过 get 方法也会阻塞，但是可以选择通过 thenAccept 异步的获取执行结果，所以不会被阻塞）。<pre class="line-numbers language-java" data-language="java"><code class="language-java">ExecutorService executor &#x3D; Executors.newFixedThreadPool(5);ListeningExecutorService guavaExecutor &#x3D; MoreExecutors.listeningDecorator(executor);ListenableFuture&lt;String&gt; future1 &#x3D; guavaExecutor.submit(() -&gt; &#123;    &#x2F;&#x2F; step 1    System.out.println(&quot;执行step 1&quot;);    return &quot;step1 result&quot;;&#125;);ListenableFuture&lt;String&gt; future2 &#x3D; guavaExecutor.submit(() -&gt; &#123;    &#x2F;&#x2F; step 2    System.out.println(&quot;执行step 2&quot;);    return &quot;step2 result&quot;;&#125;);ListenableFuture&lt;List&lt;String&gt;&gt; future1And2 &#x3D; Futures.allAsList(future1, future2);Futures.addCallback(future1And2, new FutureCallback&lt;List&lt;String&gt;&gt;() &#123;    @Override    public void onSuccess(List&lt;String&gt; result) &#123;        System.out.println(result);        ListenableFuture&lt;String&gt; future3 &#x3D; guavaExecutor.submit(() -&gt; &#123;            System.out.println(&quot;执行step 3&quot;);            return &quot;step3 result&quot;;        &#125;);        Futures.addCallback(future3, new FutureCallback&lt;String&gt;() &#123;            @Override            public void onSuccess(String result) &#123;                System.out.println(result);            &#125;                    @Override            public void onFailure(Throwable t) &#123;            &#125;        &#125;, guavaExecutor);    &#125;    @Override    public void onFailure(Throwable t) &#123;    &#125;&#125;, guavaExecutor);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><p>流程图如下：<br><img src="/images/listenableFuture.png"></p><h4 id="CompletableFuture-的实现方式"><a href="#CompletableFuture-的实现方式" class="headerlink" title="CompletableFuture 的实现方式"></a>CompletableFuture 的实现方式</h4><p>CompletableFuture 提供了很多组合操作，比如 <code>thenCombine</code>、<code>thenAccept</code>、<code>thenApply</code>，下面这个就是用到了 thenCombine ，能够将异步任务直接传递到参数，等待两个异步任务执行完成后，可以直接进行调用，其内部就是回调函数的内容。还可以继续拼接其他方法，比如 thenAccept ，可以处理上一个异步操作（thenCombine ）得到的结果。</p><p>可以看到，这里是可以采用<strong>链式调用</strong>的方式完成组合，以及进一步处理。而不是像 ListenableFuture 去嵌套的完成。</p><pre class="line-numbers language-java" data-language="java"><code class="language-java">ExecutorService executor &#x3D; Executors.newFixedThreadPool(5);CompletableFuture&lt;String&gt; cf1 &#x3D; CompletableFuture.supplyAsync(() -&gt; &#123;    System.out.println(&quot;执行step 1&quot;);    return &quot;step1 result&quot;;&#125;, executor);CompletableFuture&lt;String&gt; cf2 &#x3D; CompletableFuture.supplyAsync(() -&gt; &#123;    System.out.println(&quot;执行step 2&quot;);    return &quot;step2 result&quot;;&#125;, executor);cf1.thenCombine(cf2, (result1, result2) -&gt; &#123;    System.out.println(result1 + &quot; , &quot; + result2);    System.out.println(&quot;执行step 3&quot;);    return &quot;step3 result&quot;;&#125;).thenAccept(result3 -&gt; System.out.println(result3));<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>流程图如下：<br><img src="/images/ss.png"></p><blockquote><p>ThenApply，是对上一个操作返回的结果进行封装，再次生成一个异步任务。</p></blockquote><p>因此呢，我们选择用 CompletableFuture 来处理这种异步任务，能够简化异步调用流程以及回调函数的处理。不会遇到回调地狱的问题，代码看起来也更简洁。</p><h3 id="补充-：线程池与-CompletableFuture-结合使用的注意事项"><a href="#补充-：线程池与-CompletableFuture-结合使用的注意事项" class="headerlink" title="补充 ：线程池与 CompletableFuture 结合使用的注意事项"></a>补充 ：线程池与 CompletableFuture 结合使用的注意事项</h3><ol><li>如果在调用方法时，不指定线程池对象，会默认使用公共线程池 ForkJoinPool，导致频繁竞争，并且有性能瓶颈。因此建议都要指定线程池对象。</li><li>线程池对象最好进行业务之间的隔离，否则容易导致业务间干扰。</li><li>循环依赖导致死锁：</li></ol><p>比如下面这个场景：</p><pre class="line-numbers language-java" data-language="java"><code class="language-java">public Object doGet() &#123;  ExecutorService threadPool1 &#x3D; new ThreadPoolExecutor(10, 10, 0L, TimeUnit.MILLISECONDS, new ArrayBlockingQueue&lt;&gt;(100));  CompletableFuture cf1 &#x3D; CompletableFuture.supplyAsync(() -&gt; &#123;  &#x2F;&#x2F;do sth    return CompletableFuture.supplyAsync(() -&gt; &#123;        System.out.println(&quot;child&quot;);        return &quot;child&quot;;      &#125;, threadPool1).join();&#x2F;&#x2F;子任务    &#125;, threadPool1);  return cf1.join();&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>父任务和子任务共用一个线程池，如果请求量过大，父任务将线程池线程用完了，会导致子任务获取不到线程执行，而父任务需要依赖子任务的执行结果，进而导致死锁。</p><p>所以：<strong>需要将父任务与子任务做线程池隔离，两个任务请求不同的线程池，避免循环依赖导致的阻塞</strong>。</p><h2 id="写到最后"><a href="#写到最后" class="headerlink" title="写到最后"></a>写到最后</h2><p>本文讨论了外卖商家端 API 服务的并发加载方式，主要包括同步模型、线程池异步、NIO 异步模型和 CompletableFuture 异步模型。</p><ol><li><strong>同步模型</strong>：逐个调用服务，性能低下，不推荐。</li><li><strong>线程池异步</strong>：通过线程池处理并发请求，减少主线程阻塞，但可能导致 CPU 利用率低和回调地狱问题。</li><li><strong>NIO 异步</strong>：基于事件驱动模型和 selector 机制，适用于高并发的网络通信，但代码复杂度高。</li><li><strong>CompletableFuture 异步模型</strong>：解决回调地狱问题，适合 CPU 密集型任务，代码简洁易读。</li></ol><p>总结：对于电子商务平台的订单处理服务，推荐使用 CompletableFuture 结合线程池实现异步处理，以提高系统性能和可维护性。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;电子商务平台的订单处理服务是典型的 I&amp;#x2F;O 密集型（I&amp;#x2F;O Bound）服务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;IO 密集型任务&lt;/strong&gt;：一般就是说需要大量请求数据库和网络请求这些，如果请求比较多，系统大部分时间都花在等待 IO 操作完成。也就导</summary>
      
    
    
    
    <category term="并发" scheme="http://example.com/categories/%E5%B9%B6%E5%8F%91/"/>
    
    
    <category term="CompletableFuture" scheme="http://example.com/tags/CompletableFuture/"/>
    
  </entry>
  
  <entry>
    <title>AutoMQ 开源可观测性方案：夜莺 Flashcat</title>
    <link href="http://example.com/2024/07/19/automq-kai-yuan-ke-guan-ce-xing-fang-an-ye-ying-flashcat/"/>
    <id>http://example.com/2024/07/19/automq-kai-yuan-ke-guan-ce-xing-fang-an-ye-ying-flashcat/</id>
    <published>2024-07-19T12:07:00.000Z</published>
    <updated>2024-07-19T12:44:57.711Z</updated>
    
    <content type="html"><![CDATA[<h2 id="碎言"><a href="#碎言" class="headerlink" title="碎言"></a>碎言</h2><p>今天集成了夜莺观测平台，对于 Prometheus 更加熟悉了，同时发现了其实这些可观测的平台实际上都差不多的，都是采集 Metrics 数据，然后在平台上做聚合操作。比如观测云的采集工具 datakit，实际上也是采集观测数据，然后通过 token 获取数据并进行聚合查询，以及告警等。</p><blockquote><p>所以,,, 这些平台大同小异，在之前没了解过可能会觉得好厉害，这么多乱七八糟的数据怎么进行监控呢。现在看来就那样，但是不同的产品可能又侧重点不同，大家也都在积极的解决用户的需求，这点还是非常值得鼓励的。</p></blockquote><p>然后呢，夜莺平台有问题反馈的渠道，有一个是免费的知识星球，这个就很好，里面有几千位球友，如果你用这个产品出了什么问题，靠自己搜索解决不了，实际上可能早就出现在知识星球里了。所以这种方案对于快速解决用户问题以及节省人员精力的消耗是非常不错的。同时夜莺和 Open-Falcon 的创始人<strong>秦晓辉</strong>，我看他们的仓库里这位大佬已经提交了上百万行代码，何况是开源项目里的，可见其经验深厚，同时也是极客时间中一个课程《运维监控系统实战笔记》的作者。</p><p>然后呢，知识星球里的文章也文笔很好并且逻辑严谨，值得学习！</p><p>还是开始介绍怎么集成 AutoMQ 与夜莺监控吧，感觉现在写文档比刚开始熟练多了，也能发现逻辑问题，尽量写清楚无争议一些。可能对之后写技术文章也有帮助。之后再考虑多写技术文章，也可以发到这里来。</p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>在现代企业中，随着数据处理需求的不断增长，<a href="https://www.automq.com/zh">AutoMQ</a> [1] 作为一种高效、低成本的流处理系统，逐渐成为企业实时数据处理的关键组件。然而，随着集群规模的扩大和业务复杂性的增加，确保 AutoMQ 集群的稳定性、高可用性和性能优化变得尤为重要。因此，集成一个强大而全面的监控系统对于维护 AutoMQ 集群的健康运行至关重要。<a href="https://flashcat.cloud/docs/content/flashcat-monitor/nightingale-v7/introduction/">夜莺监控系统</a>（Nightingale）[2] 以其高效的数据采集、灵活的告警管理和丰富的可视化能力，成为企业监控AutoMQ 集群的理想选择。通过使用夜莺监控系统，企业可以实时掌握 AutoMQ 集群的运行状态，及时发现和解决潜在问题，优化系统性能，确保业务的连续性和稳定性。</p><h3 id="AutoMQ-概述"><a href="#AutoMQ-概述" class="headerlink" title="AutoMQ 概述"></a><strong>AutoMQ 概述</strong></h3><p>AutoMQ 是一种基于云重新设计的流处理系统，它在保持与 Apache Kafka 100% 兼容的前提下，通过将存储分离至对象存储，显著提升了系统的成本效益和弹性能力。具体来说，AutoMQ 通过构建在 S3 上的流存储库 S3Stream，将存储卸载至云厂商提供的共享云存储 EBS 和 S3，提供低成本、低延时、高可用、高可靠和无限容量的流存储能力。与传统的 Shared Nothing 架构相比，AutoMQ 采用了 Shared Storage 架构，显著降低了存储和运维的复杂性，同时提升了系统的弹性和可靠性。</p><p>AutoMQ 的设计理念和技术优势使其成为替换企业现有 Kafka 集群的理想选择。通过采用 AutoMQ，企业可以显著降低存储成本，简化运维，并实现集群的自动扩缩容和流量自平衡，从而更高效地应对业务需求的变化。此外，AutoMQ 的架构支持高效的冷读操作和服务零中断，确保系统在高负载和突发流量情况下的稳定运行。它的存储结构如下：</p><p><img src="/images/flash.png"></p><h3 id="夜莺概述"><a href="#夜莺概述" class="headerlink" title="夜莺概述"></a>夜莺概述</h3><p>夜莺监控系统（Nightingale）是一款开源的云原生观测分析工具，采用 All-in-One 设计理念，集数据采集、可视化、监控告警和数据分析于一体。其主要优势包括高效的数据采集能力、灵活的告警策略和丰富的可视化功能。夜莺与多种云原生生态紧密集成，支持多种数据源和存储后端，提供低延迟、高可靠性的监控服务。通过使用夜莺，企业可以实现对复杂分布式系统的全面监控和管理，快速定位和解决问题，从而优化系统性能和提高业务连续性。</p><p><img src="/images/yey.png"></p><h2 id="前置条件"><a href="#前置条件" class="headerlink" title="前置条件"></a>前置条件</h2><p>为了实现集群状态的监控，你需要如下环境：</p><ul><li><p>部署一个可用的 AutoMQ 节点&#x2F;集群，并开放 Metrics 采集端口</p></li><li><p>部署夜莺监控及其依赖环境</p></li><li><p>部署 <a href="https://prometheus.io/docs/prometheus/latest/getting_started/">Prometheus</a> [4] 以获取 Metrics 数据</p></li></ul><h2 id="部署-AutoMQ、Prometheus-以及夜莺监控"><a href="#部署-AutoMQ、Prometheus-以及夜莺监控" class="headerlink" title="部署 AutoMQ、Prometheus 以及夜莺监控"></a>部署 AutoMQ、Prometheus 以及夜莺监控</h2><h3 id="部署-AutoMQ"><a href="#部署-AutoMQ" class="headerlink" title="部署 AutoMQ"></a>部署 AutoMQ</h3><p>参考 AutoMQ 文档：<a href="https://docs.automq.com/zh/docs/automq-opensource/IyXrw3lHriVPdQkQLDvcPGQdnNh">集群方式部署 | AutoMQ</a> [5] 。在部署启动前，添加如下配置参数以开启 Prometheu 的拉取接口。通过以下参数启动 AutoMQ 集群以后，每个节点将会额外开放一个 HTTP 接口供我们拉取 AutoMQ 的监控指标。这些指标的格式均遵循 Prometheus Metrics 的格式。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">bin&#x2F;kafka-server-start.sh ...\--override  s3.telemetry.metrics.exporter.type&#x3D;prometheus \--override  s3.metrics.exporter.prom.host&#x3D;0.0.0.0 \--override  s3.metrics.exporter.prom.port&#x3D;8890 \....<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>当启用 AutoMQ 监控指标后，可以在任意一台节点上通过 HTTP 协议拉取到 Prometheus 格式的监控指标，地址为：<code>http://&#123;node_ip&#125;:8890</code>，响应结果示例如下：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">....kafka_request_time_mean_milliseconds&#123;otel_scope_name&#x3D;&quot;io.opentelemetry.jmx&quot;,type&#x3D;&quot;DescribeDelegationToken&quot;&#125; 0.0 1720520709290kafka_request_time_mean_milliseconds&#123;otel_scope_name&#x3D;&quot;io.opentelemetry.jmx&quot;,type&#x3D;&quot;CreatePartitions&quot;&#125; 0.0 1720520709290...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>关于指标介绍，可以参考 AutoMQ 官网文档：<a href="https://docs.automq.com/zh/docs/automq-opensource/ArHpwR9zsiLbqwkecNzcqOzXn4b">Metrics | AutoMQ</a> [6] 。</p><h3 id="部署-Prometheus"><a href="#部署-Prometheus" class="headerlink" title="部署 Prometheus"></a>部署 Prometheus</h3><p>Prometheus 可以通过下载二进制包部署，也可以通过 Docker 方式部署。以下是这两种部署方式的介绍。</p><h4 id="二进制部署"><a href="#二进制部署" class="headerlink" title="二进制部署"></a>二进制部署</h4><p>为了方便使用，你可以新建一个脚本，并根据需要修改 Prometheus 的下载版本，最后执行脚本即可完成部署。首先，新建脚本：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">cd &#x2F;homevim install_prometheus.sh# !!! 粘贴下面的脚本内容 并保存退出# 授予权限chmod +x install_prometheus.sh# 执行脚本.&#x2F;install_prometheus.sh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>脚本内容如下：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">version&#x3D;2.45.3filename&#x3D;prometheus-$&#123;version&#125;.linux-amd64mkdir -p &#x2F;opt&#x2F;prometheuswget https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;prometheus&#x2F;releases&#x2F;download&#x2F;v$&#123;version&#125;&#x2F;$&#123;filename&#125;.tar.gztar xf $&#123;filename&#125;.tar.gzcp -far $&#123;filename&#125;&#x2F;*  &#x2F;opt&#x2F;prometheus&#x2F;# config as a service cat &lt;&lt;EOF &gt;&#x2F;etc&#x2F;systemd&#x2F;system&#x2F;prometheus.service[Unit]Description&#x3D;&quot;prometheus&quot;Documentation&#x3D;https:&#x2F;&#x2F;prometheus.io&#x2F;After&#x3D;network.target[Service]Type&#x3D;simpleExecStart&#x3D;&#x2F;opt&#x2F;prometheus&#x2F;prometheus --config.file&#x3D;&#x2F;opt&#x2F;prometheus&#x2F;prometheus.yml --storage.tsdb.path&#x3D;&#x2F;opt&#x2F;prometheus&#x2F;data --web.enable-lifecycle --web.enable-remote-write-receiverRestart&#x3D;on-failureSuccessExitStatus&#x3D;0LimitNOFILE&#x3D;65536StandardOutput&#x3D;syslogStandardError&#x3D;syslogSyslogIdentifier&#x3D;prometheus[Install]WantedBy&#x3D;multi-user.targetEOFsystemctl enable prometheussystemctl restart prometheussystemctl status prometheus<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>随后修改 Prometheus 的配置文件，<strong>增加采集 AutoMQ 可观测数据的任务</strong>，并<strong>重启</strong> Prometheus，执行命令：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"># config 配置文件内容填下面的vim &#x2F;opt&#x2F;prometheus&#x2F;prometheus.yml# 重启 Prometheussystemctl restart prometheus<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p><strong>配置文件内容参考如下</strong>，请将下列中的<code>client_ip</code>修改为 AutoMQ 开放的可观测数据暴露地址：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"># my global configglobal:  scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.scrape_configs:  # The job name is added as a label &#96;job&#x3D;&lt;job_name&gt;&#96; to any timeseries scraped from this config.  - job_name: &quot;prometheus&quot;    static_configs:      - targets: [&quot;localhost:9090&quot;]        - job_name: &quot;automq&quot;    static_configs:      - targets: [&quot;&#123;client_ip&#125;:8890&quot;]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>部署完成后，我们可以通过浏览器访问 Prometheus，查看是否真正采集到了 AutoMQ 的 Metrics数据，访问<code>http://&#123;client_ip&#125;:9090/targets</code>：</p><p><img src="/images/Prometheus.png"></p><h4 id="Docker-部署"><a href="#Docker-部署" class="headerlink" title="Docker 部署"></a>Docker 部署</h4><p>如果你已经有一个在运行的 Prometheus Docker 容器，请先执行命令删除该容器：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker stop prometheusdocker rm prometheus<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>新建配置文件，并在 Docker 启动时进行挂载：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">mkdir -p &#x2F;opt&#x2F;prometheusvim &#x2F;opt&#x2F;prometheus&#x2F;prometheus.yml# 配置内容参考上述 “二进制部署” 中的配置<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>启动 Docker 容器：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">docker run -d \  --name&#x3D;prometheus \  -p 9090:9090 \  -v &#x2F;opt&#x2F;prometheus&#x2F;prometheus.yml:&#x2F;etc&#x2F;prometheus&#x2F;prometheus.yml \  -m 500m \  prom&#x2F;prometheus \  --config.file&#x3D;&#x2F;etc&#x2F;prometheus&#x2F;prometheus.yml \  --enable-feature&#x3D;otlp-write-receiver \  --web.enable-remote-write-receiver<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这样你便得到了一个采集 AutoMQ Metrics 的 Prometheus 服务，关于更多 AutoMQ Metrics 与 Prometheus 集成的介绍，可以参考：<a href="https://docs.automq.com/zh/automq/observability/integrating-metrics-with-prometheus">将 Metrics 集成到 Prometheus | AutoMQ</a> [7]。</p><h3 id="部署夜莺监控"><a href="#部署夜莺监控" class="headerlink" title="部署夜莺监控"></a>部署夜莺监控</h3><p>夜莺监控可以通过下面三种方式进行部署，关于更详细的部署说明可以参考<a href="https://flashcat.cloud/docs/content/flashcat-monitor/nightingale-v7/install/intro/">官方文档</a> [8]：</p><ul><li><p>Docker compose 方式部署</p></li><li><p>二进制方式部署</p></li><li><p>Helm 方式部署</p></li></ul><p>接下来我将采用二进制的方式进行部署。</p><h4 id="下载夜莺"><a href="#下载夜莺" class="headerlink" title="下载夜莺"></a>下载夜莺</h4><p>请在夜莺 <a href="https://github.com/ccfos/nightingale">Github releases</a> [9] 页中选择合适的版本进行下载，这里我们采用的版本是<code>v7.0.0-beta.14</code>。如果你是 amd 架构的机器，可直接执行如下命令：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">cd &#x2F;home# 下载wget https:&#x2F;&#x2F;github.com&#x2F;ccfos&#x2F;nightingale&#x2F;releases&#x2F;download&#x2F;v7.0.0-beta.14&#x2F;n9e-v7.0.0-beta.14-linux-amd64.tar.gzmkdir -p &#x2F;home&#x2F;flashcat# 解压文件到 &#x2F;home&#x2F;flashcat 文件夹tar -xzf &#x2F;home&#x2F;n9e-v7.0.0-beta.14-linux-amd64.tar.gz -C &#x2F;home&#x2F;flashcat# 进入主目录cd &#x2F;home&#x2F;flashcat<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="配置依赖环境"><a href="#配置依赖环境" class="headerlink" title="配置依赖环境"></a>配置依赖环境</h4><p>夜莺依赖 MySQL 和 Redis，因此需要提前安装这两个环境。你可以通过 Docker 方式部署，也可以通过执行命令进行安装，参考命令如下：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"># install mysqlyum -y install mariadb*systemctl enable mariadbsystemctl restart mariadbmysql -e &quot;SET PASSWORD FOR &#39;root&#39;@&#39;localhost&#39; &#x3D; PASSWORD(&#39;1234&#39;);&quot;# install redisyum install -y redissystemctl enable redissystemctl restart redis<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这里 Redis 设置的是无密码的。并且这里指定 MySQL 数据库的密码为<code>1234</code>，<strong>如果你需要更改为其他的密码</strong>，要在夜莺的配置文件中进行配置，以保证夜莺能连接到你的数据库。修改夜莺配置文件：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">vim &#x2F;home&#x2F;flashcat&#x2F;etc&#x2F;config.toml修改 [DB] 下的用户名和密码：[DB]# postgres: host&#x3D;%s port&#x3D;%s user&#x3D;%s dbname&#x3D;%s password&#x3D;%s sslmode&#x3D;%s# postgres: DSN&#x3D;&quot;host&#x3D;127.0.0.1 port&#x3D;5432 user&#x3D;root dbname&#x3D;n9e_v6 password&#x3D;1234 sslmode&#x3D;disable&quot;# sqlite: DSN&#x3D;&quot;&#x2F;path&#x2F;to&#x2F;filename.db&quot;DSN &#x3D; &quot;&#123;username&#125;:&#123;password&#125;@tcp(127.0.0.1:3306)&#x2F;n9e_v6?charset&#x3D;utf8mb4&amp;parseTime&#x3D;True&amp;loc&#x3D;Local&amp;allowNativePasswords&#x3D;true&quot;# enable debug mode or not<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="导入数据库表"><a href="#导入数据库表" class="headerlink" title="导入数据库表"></a>导入数据库表</h4><p>执行如下命令：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">mysql -uroot -p1234 &lt; n9e.sql<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>请通过数据库工具检测是否成功导入数据库表：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">&gt; show databases;+--------------------+| Database           |+--------------------+| n9e_v6             |+--------------------+&gt; show tables;+-----------------------+| Tables_in_n9e_v6      |+-----------------------+| alert_aggr_view       || alert_cur_event       || alert_his_event       || alert_mute            || alert_rule            || alert_subscribe       || alerting_engines      || board                 || board_busigroup       || board_payload         || builtin_cate          || builtin_components    || builtin_metrics       |······<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="修改夜莺配置文件"><a href="#修改夜莺配置文件" class="headerlink" title="修改夜莺配置文件"></a>修改夜莺配置文件</h4><p>你需要修改夜莺的配置文件，进行 Prometheus 数据源的设置：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">vim &#x2F;home&#x2F;flashcat&#x2F;etc&#x2F;config.toml# 修改 [[Pushgw.Writers]] 部分的内容为[[Pushgw.Writers]]# Url &#x3D; &quot;http:&#x2F;&#x2F;127.0.0.1:8480&#x2F;insert&#x2F;0&#x2F;prometheus&#x2F;api&#x2F;v1&#x2F;write&quot;Url &#x3D; &quot;http:&#x2F;&#x2F;&#123;client_ip&#125;:9090&#x2F;api&#x2F;v1&#x2F;write&quot;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="启动夜莺"><a href="#启动夜莺" class="headerlink" title="启动夜莺"></a>启动夜莺</h4><p>在夜莺的根目录 <code>/home/flashcat</code>下执行：<code>./n9e</code>。成功启动后，可在浏览器中访问 <code>http://&#123;client_ip&#125;:17000</code>，默认的登录账号和密码为：</p><ul><li><p>账号：<code>root</code></p></li><li><p>密码：<code>root.2020</code></p></li></ul><p><img src="/images/start'.png"></p><h4 id="接入-Prometheus-数据源"><a href="#接入-Prometheus-数据源" class="headerlink" title="接入 Prometheus 数据源"></a>接入 Prometheus 数据源</h4><p>左侧边栏集成 -&gt; 数据源 -&gt; Prometheus。</p><p><img src="/images/datasource-1.png"></p><p>至此，我们的夜莺监控就部署结束了。</p><h2 id="夜莺监控-AutoMQ-集群状态"><a href="#夜莺监控-AutoMQ-集群状态" class="headerlink" title="夜莺监控 AutoMQ 集群状态"></a>夜莺监控 AutoMQ 集群状态</h2><p>接下来，我将介绍夜莺监控提供的一部分功能，帮助你更好地了解夜莺与 AutoMQ 集成的可用功能。</p><h3 id="即时查询"><a href="#即时查询" class="headerlink" title="即时查询"></a>即时查询</h3><p>选择内置的 AutoMQ 指标：</p><p><img src="/images/query1.png"></p><p>可以尝试查询一些数据，比如Fetch请求处理时间的平均值 <code>kafka_request_time_50p_milliseconds</code>:</p><p><img src="/images/avg.png"></p><p><img src="/images/avg2.png"></p><p>同时你也可以自定义一些指标，并利用表达式对指标进行聚合：</p><p><img src="/images/%E8%81%9A%E5%90%88.png"></p><h3 id="警报功能"><a href="#警报功能" class="headerlink" title="警报功能"></a>警报功能</h3><p>选择左侧边栏警报 -&gt; 警报规则 -&gt; 新建规则。比如我们可以给 <code>kafka_network_io_bytes_total</code>设置报警，这个指标的意义是 Kafka Broker 节点通过网络发送或接收的字节总数，通过对这个指标设置表达式，就能够计算 Kafka Broker 节点的入站网络 I&#x2F;O 速率。表达式为：</p><pre class="line-numbers language-none"><code class="language-none">sum by(job, instance) (rate(kafka_network_io_bytes_total&#123;direction&#x3D;&quot;in&quot;&#125;[1m]))<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>设置警报规则：</p><p><img src="/images/alertrule.png"></p><p>Data preview：</p><p><img src="/images/preview.png"></p><p>同时你也可以设置发生警报时会通知到的群组：</p><p><img src="/images/group.png"></p><p>创建完告警后，让我们来模拟高并发的消息处理场景：短时间内总共<code>2500000</code>条消息被发送到 AutoMQ 节点，我采用的方式是通过 Kafka SDK 的方式进行消息发送，一次共 50 个 Topic ，给每个 Topic 发送 500 条消息，共 100 次。示例如下：</p><pre class="line-numbers language-java" data-language="java"><code class="language-java">import org.apache.kafka.clients.admin.AdminClient;import org.apache.kafka.clients.admin.AdminClientConfig;import org.apache.kafka.clients.admin.NewTopic;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.ProducerConfig;import org.apache.kafka.clients.producer.ProducerRecord;import org.apache.kafka.clients.producer.RecordMetadata;import org.apache.kafka.common.serialization.StringSerializer;import java.util.ArrayList;import java.util.List;import java.util.Properties;import java.util.concurrent.ExecutionException;public class KafkaTest &#123;    private static final String BOOTSTRAP_SERVERS &#x3D; &quot;http:&#x2F;&#x2F;&#123;&#125;:9092&quot;; &#x2F;&#x2F; your automq broker ip    private static final int NUM_TOPICS &#x3D; 50;    private static final int NUM_MESSAGES &#x3D; 500;    public static void main(String[] args) throws Exception &#123;        KafkaTest test &#x3D; new KafkaTest();        &#x2F;&#x2F; test.createTopics();    &#x2F;&#x2F; create 50 topics        for(int i &#x3D; 0; i &lt; 100; i++)&#123;            test.sendMessages();   &#x2F;&#x2F; 25,000 messages will be sent each time, and 500 messages will be sent to each of 50 topics.        &#125;    &#125;        public void createTopics() &#123;        Properties props &#x3D; new Properties();        props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVERS);        try (AdminClient adminClient &#x3D; AdminClient.create(props)) &#123;            List&lt;NewTopic&gt; topics &#x3D; new ArrayList&lt;&gt;();            for (int i &#x3D; 1; i &lt;&#x3D; NUM_TOPICS; i++) &#123;                topics.add(new NewTopic(&quot;Topic-&quot; + i, 1, (short) 1));            &#125;            adminClient.createTopics(topics).all().get();            System.out.println(&quot;Topics created successfully&quot;);        &#125; catch (InterruptedException | ExecutionException e) &#123;            e.printStackTrace();        &#125;    &#125;    public void sendMessages() &#123;        Properties props &#x3D; new Properties();        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVERS);        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());        try (KafkaProducer&lt;String, String&gt; producer &#x3D; new KafkaProducer&lt;&gt;(props)) &#123;            for (int i &#x3D; 1; i &lt;&#x3D; NUM_TOPICS; i++) &#123;                String topic &#x3D; &quot;Topic-&quot; + i;                for (int j &#x3D; 1; j &lt;&#x3D; NUM_MESSAGES; j++) &#123;                    String key &#x3D; &quot;key-&quot; + j;                    String value &#x3D; &quot;&#123;\&quot;userId\&quot;: &quot; + j + &quot;, \&quot;action\&quot;: \&quot;visit\&quot;, \&quot;timestamp\&quot;: &quot; + System.currentTimeMillis() + &quot;&#125;&quot;;                    ProducerRecord&lt;String, String&gt; record &#x3D; new ProducerRecord&lt;&gt;(topic, key, value);                    producer.send(record, (RecordMetadata metadata, Exception exception) -&gt; &#123;                        if (exception &#x3D;&#x3D; null) &#123;                            System.out.printf(&quot;Sent message to topic %s partition %d with offset %d%n&quot;, metadata.topic(), metadata.partition(), metadata.offset());                        &#125; else &#123;                            exception.printStackTrace();                        &#125;                    &#125;);                &#125;            &#125;            System.out.println(&quot;Messages sent successfully&quot;);        &#125;    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>随后我们可以在夜莺控制台看到报警信息：</p><p><img src="/images/alert2.png"></p><p>告警详细信息：</p><p><img src="/images/alertinfo.png"></p><h3 id="仪表盘"><a href="#仪表盘" class="headerlink" title="仪表盘"></a>仪表盘</h3><p>首先我们可以利用已知的指标建立自己的仪表盘，如下所示是对 AutoMQ 消息请求处理时间，消息总数，网络 IO 比特数进行的统计仪表盘：</p><p><img src="/images/board.png"></p><p>同时，我们也可以利用官方内置的仪表盘进行监测。左侧边栏 -&gt; 聚合 -&gt; 模板中心：</p><p><img src="/images/template.png"></p><p>选择 AutoMQ，可以看到有几个 DashBoard 可以选用:</p><p><img src="/images/dashboard.png"></p><p>我们选择 Topic Metrics 仪表盘，展示内容如下：</p><p><img src="/images/metrics.png"></p><p>这里展示了 AutoMQ 集群在最近一段时间内的消息输入和输出的占用情况、消息输入和请求的速率、消息大小等。这些指标用于监控和优化 AutoMQ 集群的性能和稳定性：通过消息输入和输出的占用情况，可以评估生产者和消费者的负载，确保集群能正常处理消息流量；消息输入速率用于实时监控生产者发送消息的速率，从而识别潜在的瓶颈或突发流量；请求速率帮助了解客户端请求的频率，以便优化资源分配和处理能力；消息大小指标则用于分析消息的平均大小，从而调整配置以优化存储和网络传输效率。通过监控这些指标，能够及时发现并解决性能问题，确保 AutoMQ 集群的高效和稳定运行。</p><p>至此，我们的集成过程已完成。关于更多的使用方式，你可以参考夜莺的<a href="https://flashcat.cloud/docs/content/flashcat-monitor/nightingale-v7/overview/">官方文档</a> [10] 进行体验。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过本文的介绍，我们详细阐述了如何使用夜莺监控系统（Nightingale）对 AutoMQ 集群进行全面监控。我们从AutoMQ 和夜莺的基本概念入手，逐步讲解了如何部署 AutoMQ、Prometheus 和夜莺，并配置监控和告警规则。通过这种集成，企业可以实时掌握 AutoMQ 集群的运行状态，及时发现和解决潜在问题，优化系统性能，确保业务的连续性和稳定性。夜莺监控系统以其强大的数据采集能力、灵活的告警机制和丰富的可视化功能，成为企业监控复杂分布式系统的理想选择。希望本文能为您在实际应用中提供有价值的参考，助力您的系统运维更加高效和稳定。</p><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p>[1] AutoMQ：<a href="https://www.automq.com/zh">https://www.automq.com/zh</a></p><p>[2] 夜莺监控：<a href="https://flashcat.cloud/docs/content/flashcat-monitor/nightingale-v7/introduction/">https://flashcat.cloud/docs/content/flashcat-monitor/nightingale-v7/introduction/</a></p><p>[3] 夜莺架构：<a href="https://flashcat.cloud/docs/content/flashcat-monitor/nightingale-v7/introduction/">https://flashcat.cloud/docs/content/flashcat-monitor/nightingale-v7/introduction/</a></p><p>[4] Prometheus：<a href="https://prometheus.io/docs/prometheus/latest/getting_started/">https://prometheus.io/docs/prometheus/latest/getting_started/</a></p><p>[5] 集群方式部署 | AutoMQ：<a href="https://docs.automq.com/zh/docs/automq-opensource/IyXrw3lHriVPdQkQLDvcPGQdnNh">https://docs.automq.com/zh/docs/automq-opensource/IyXrw3lHriVPdQkQLDvcPGQdnNh</a></p><p>[6] Metrics | AutoMQ：<a href="https://docs.automq.com/zh/docs/automq-opensource/ArHpwR9zsiLbqwkecNzcqOzXn4b">https://docs.automq.com/zh/docs/automq-opensource/ArHpwR9zsiLbqwkecNzcqOzXn4b</a></p><p>[7] 将 Metrics 集成到 Prometheus：<a href="https://docs.automq.com/zh/automq/observability/integrating-metrics-with-prometheus">https://docs.automq.com/zh/automq/observability/integrating-metrics-with-prometheus</a></p><p>[8] 部署说明：<a href="https://flashcat.cloud/docs/content/flashcat-monitor/nightingale-v7/install/intro/">https://flashcat.cloud/docs/content/flashcat-monitor/nightingale-v7/install/intro/</a></p><p>[9] 夜莺 Github releases：<a href="https://github.com/ccfos/nightingale">https://github.com/ccfos/nightingale</a></p><p>[10] 夜莺官方文档：<a href="https://flashcat.cloud/docs/content/flashcat-monitor/nightingale-v7/overview/">https://flashcat.cloud/docs/content/flashcat-monitor/nightingale-v7/overview/</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;碎言&quot;&gt;&lt;a href=&quot;#碎言&quot; class=&quot;headerlink&quot; title=&quot;碎言&quot;&gt;&lt;/a&gt;碎言&lt;/h2&gt;&lt;p&gt;今天集成了夜莺观测平台，对于 Prometheus 更加熟悉了，同时发现了其实这些可观测的平台实际上都差不多的，都是采集 Metrics 数</summary>
      
    
    
    
    <category term="AutoMQ" scheme="http://example.com/categories/AutoMQ/"/>
    
    
    <category term="AutoMQ" scheme="http://example.com/tags/AutoMQ/"/>
    
  </entry>
  
  <entry>
    <title>日常 PR</title>
    <link href="http://example.com/2024/07/18/ri-chang-pr/"/>
    <id>http://example.com/2024/07/18/ri-chang-pr/</id>
    <published>2024-07-18T00:51:00.000Z</published>
    <updated>2024-07-18T01:01:00.514Z</updated>
    
    <content type="html"><![CDATA[<p>这周做了一个 FP，主要是实现了腾讯云 COS 和华为云对象存储 OBS 的 CredentialsProvider。</p><p>（仓库是私有的，好像看不到具体的我的提交）<br><img src="/images/pr1.png"></p><p><img src="/images/pr2.png"></p><p>我简单解释一下就是：</p><ul><li>一种获取 AK 和 SK 的方式是静态的，也就是我们直接指定凭证去操作对象存储。</li><li>第二种是以 IAM 角色绑定服务器实例，通过服务器内请求远端 Imdsendpoint 得到临时凭证，之后就可以利用这个临时凭证去操作对象存储。</li></ul><p>至于<strong>为什么要获取临时凭证</strong>，这个主要是考虑到之后 AutoMQ 商业版会作为 Saas 服务提供给客户，客户只需要说：我需要换掉 Kafka 接入 AutoMQ，你们告诉我该怎么做。</p><p>我们直接提供商业版安装包，里面的逻辑都是封装好的，且在开源版的基础上增加了更多的实用功能，满足不同的需求。因此嘞，你只需要修改一下我们给的 terraform 配置文件，以及 ansible 等工具的配置，来完成自动化的部署，由于底层兼容了各大云厂商的 Api 以及 IMDS，所以你只需要照着说明来理解。</p><p>至于其他的一些逻辑，我之后做到了或者理解了会发一部分内容，尽情期待。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;这周做了一个 FP，主要是实现了腾讯云 COS 和华为云对象存储 OBS 的 CredentialsProvider。&lt;/p&gt;
&lt;p&gt;（仓库是私有的，好像看不到具体的我的提交）&lt;br&gt;&lt;img src=&quot;/images/pr1.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;</summary>
      
    
    
    
    
    <category term="push" scheme="http://example.com/tags/push/"/>
    
  </entry>
  
  <entry>
    <title>Prometheus + Grafana监控搞定了</title>
    <link href="http://example.com/2024/07/14/prometheus-grafana-xiang-mu-jian-kong-gao-ding-liao/"/>
    <id>http://example.com/2024/07/14/prometheus-grafana-xiang-mu-jian-kong-gao-ding-liao/</id>
    <published>2024-07-14T08:22:00.000Z</published>
    <updated>2024-07-14T13:43:36.395Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>这篇文章只是碎碎念，格式不太好请见谅，今天好好的做了些自己的事情，之后可能就很少有时间自己做了。</p></blockquote><p>今天上午把本地的 Prometheus + Grafana 监控搞定了，然后下午就尝试在服务器上部署，然后把前端页面改了一下，搞了个弹窗用来提示 Grafana 监控功能上线，然后导航栏也加了个按钮。</p><h2 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h2><p>目前的效果是能看到下面这种数据监控情况：<br><img src="/images/Prometheus%20+%20Grafana%E7%9B%91%E6%8E%A7%E6%90%9E%E5%AE%9A%E4%BA%86.png"></p><p>其实感觉还挺有用的，我暂时还没设置报警，这个功能可以在后续监控到响应时间不正常的接口进行报警和通知，通知的形式有很多种，包括邮箱，飞书 bot 等。</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>然后嘞，我提供了两个版本：</p><ul><li>快照版本：也就是某个时间段的数据，这个主要是为了避免访客在使用时看不到数据，还得自己去调接口从而拿到数据的情况。</li><li>实时更新版本：这个就是正常的选择最近的时间范围了，比如几个小时之内，几十分钟之内啊，或者具体的某一天的统计情况，这个就看你自己的选择了，你也可以适当的压测一些接口（虽然我做了限流就是说😏，你可以试一下短链接跳转多来那么个几百上千次访问没问题的）。</li></ul><p>这两个版本都需要进行登录，我专门创建了一个访客用户，你可以访问这些数据：</p><ul><li>用户名：<code>visitor</code> </li><li>密码：<code>wzc123</code></li></ul><h2 id="关于部署教程"><a href="#关于部署教程" class="headerlink" title="关于部署教程"></a>关于部署教程</h2><p>很抱歉，目前只有一篇我自己在做重构的时候输出的步骤文章，但是暂时没办法细细优化，总不能拿一篇水文来给各位。所以呢这个计划就暂时搁置了，之后会找时间整理一下，争取输出一篇高质量的文章，帮你避免很多坑的那种（主要是我也踩坑很多）。</p><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>还是决定把项目地址公布出来了，也希望大家能帮我找出更多的问题，也能让我有所准备并解决。</p><ul><li>项目部署地址： <a href="http://www.ktpro.ink/login">http://www.ktpro.ink/login</a></li></ul><p>其中也包括了 Grafana 监控的提示，你可以自行查看！</p><p>还有一件事也蛮开心的，换成 consul 之后，整个项目 + 中间件的内存占用才不到 3g，之前用 nacos 不加监控就已经达到 3.6g。因此还要强调一次：注重项目结构的搭建和中间件选用，最大程度的在满足功能的前提下节省成本。</p><p>实际上，这个和 AutoMQ 的理念不谋而合，都是以实用为主，最大程度的提高资源利用率和效率。</p><p>祝大家生活愉快！</p><p>好想去旅行….<br><img src="/images/Snipaste_2024-05-07_19-52-55.png"></p><hr><p>害，好多计划搁置了：<br><img src="/images/1-2.png"></p><p><img src="/images/2-3.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;这篇文章只是碎碎念，格式不太好请见谅，今天好好的做了些自己的事情，之后可能就很少有时间自己做了。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;今天上午把本地的 Prometheus + Grafana 监控搞定了，然后下午就尝试在服务器上部署，然后把</summary>
      
    
    
    
    <category term="项目" scheme="http://example.com/categories/%E9%A1%B9%E7%9B%AE/"/>
    
    
    <category term="项目优化" scheme="http://example.com/tags/%E9%A1%B9%E7%9B%AE%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>很开心，换掉 Nacos 后服务器内存省下很多！</title>
    <link href="http://example.com/2024/07/13/hen-kai-xin-huan-diao-nacos-hou-fu-wu-qi-nei-cun-sheng-xia-hen-duo/"/>
    <id>http://example.com/2024/07/13/hen-kai-xin-huan-diao-nacos-hou-fu-wu-qi-nei-cun-sheng-xia-hen-duo/</id>
    <published>2024-07-13T13:59:00.000Z</published>
    <updated>2024-07-13T16:14:02.079Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>如题，最近打算在项目中引入 Grafana + Prometheus 作为数据监控，用来监控服务接口调用次数以及重点接口耗时等情况，这个后续做完会出一篇教程。</p><p>我们还是先看看替换之后的结果（旧的截图忘记截了），可以看到目前所有中间件加两个项目模块启动后内存占用是 <strong>2.5g</strong>，这个是什么概念呢，在之前用 Nacos 时总的内存占用达到了 <strong>3.6 g</strong>，也就是说只换掉了一个 Nacos 就节省了 1g 的内存，这 1g 内存你去部署个博客都绰绰有余了：<br><img src="/images/linux.png"></p><p>而且还有一个可怕的一点是，之前我的中间件如 Redis，Mysql，Nacos 都是 Docker 方式部署，比较方便，并且呢对容器的内存大小作了限制，容器如果运行内存超过限制就被扼杀掉，保证系统不会崩掉。说重点，之前通过 <code>docker stats</code> 查看容器占用情况，Nacos 是一直稳定在 1g 左右的（限制最大为 1.5g），所以现在来看，Consul 几乎是不占内存的 ??，神奇，相见恨晚呐。</p><p>我现在只有一台服务器，配置是 2c 4g 的，和公司分的 8c 16g 根本没法比，但是我只需要部署的模块只有两个，一个聚合服务，一个网关服务。所以需求其实不高。但是我发现了一个问题，就是 Nacos 的占用实在是太高了，然后我们继续来看下面 Nacos 的特点和功能。</p><h2 id="Nacos"><a href="#Nacos" class="headerlink" title="Nacos"></a>Nacos</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ol><li><strong>多功能集成</strong>：Nacos 集成了服务发现、配置管理和动态 DNS 服务，功能非常全面。</li><li><strong>简单易用</strong>：Nacos 提供简单的 API 和用户界面，易于使用和管理。</li><li><strong>健康检查</strong>：支持多种健康检查方式，包括 HTTP、TCP 和自定义脚本。</li><li><strong>动态配置管理</strong>：支持分布式配置管理，支持配置的实时推送和灰度发布。</li><li><strong>多种数据存储支持</strong>：支持 MySQL、嵌入式数据库等多种存储方式，便于灵活选择。</li></ol><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ol><li><strong>成熟度</strong>：相对于 Consul，Nacos 的成熟度和社区支持可能稍逊一筹。</li><li><strong>性能和扩展性</strong>：在高并发、大规模场景下，Nacos 的性能和扩展性可能需要进一步优化。</li><li><strong>依赖性</strong>：Nacos 依赖于 MySQL 等外部存储，增加了系统复杂性。</li></ol><p>我们可以看到 Nacos 的一个很大的缺点就是<strong>性能比较低</strong>，或者说比较重，当然我不是说 Nacos 不好，对于新手来说，一套 Spring Cloud Alibaba 系列下的中间件都是非常兼容的，对于初学微服务来说非常的好使。</p><p>然而，Consul 由于是更加面向国际的开源项目，它的功能也非常全面，且弊端很小，所以我就考虑了使用 Consul 来替换 Nacos，毕竟它也兼容 Spring Cloud。下面还是先介绍一下 Consul。</p><h2 id="Consul"><a href="#Consul" class="headerlink" title="Consul"></a>Consul</h2><p>Consul 是 HashiCorp 提供的一款开源工具，主要用于服务发现和配置管理，具有强大的分布式特性和高可用性。</p><h4 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h4><ol><li><strong>成熟稳定</strong>：Consul 经过多年的发展，已经非常成熟和稳定，广泛应用于生产环境。</li><li><strong>高可用和分布式</strong>：Consul 采用 Raft 协议，保证了分布式系统的一致性和高可用性。</li><li><strong>多数据中心支持</strong>：支持多数据中心的服务发现和配置管理，适用于复杂的分布式系统。</li><li><strong>丰富的健康检查</strong>：支持多种健康检查方式，包括 HTTP、TCP 和自定义脚本。</li><li><strong>强大的生态系统</strong>：Consul 具有丰富的插件和工具生态，易于与其他系统集成。</li></ol><h4 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h4><ol><li><strong>学习曲线</strong>：Consul 的配置和管理相对复杂，学习曲线较陡。</li><li><strong>功能单一</strong>：Consul 主要专注于服务发现和配置管理，功能相对单一。</li><li><strong>存储依赖</strong>：Consul 依赖于其内置的存储系统（基于 Raft 协议），在某些情况下可能不如外部存储灵活。</li></ol><p>一看到缺点，我们会想到学习曲线抖？你是说一个换掉 Nacos 的 pom 依赖，再改一下配置文件的 Consul 开放端口算难度高？好吧，其实要复杂点，你再配置个 ACL，再配置用户认证，那确实会难度陡增，然而，Nacos 不也一样，如果你要用复杂的功能。而且对于我们这种小项目，最重要的就是功能和业务展示，也不会说遇到各种乱七八糟的错误，所以换掉 Nacos 对于我们来说非常的划算。如果之后做项目，我同样选择 Consul。</p><h2 id="关于如何部署-Consul"><a href="#关于如何部署-Consul" class="headerlink" title="关于如何部署 Consul"></a>关于如何部署 Consul</h2><ol><li>参考官方文档，选择一种方式部署即可。参考：<a href="https://developer.hashicorp.com/consul/docs/install">Install Consul | Consul | HashiCorp Developer</a> </li><li>Pom 依赖<pre class="line-numbers language-java" data-language="java"><code class="language-java">&lt;dependency&gt;    &lt;groupId&gt;org.springframework.cloud&lt;&#x2F;groupId&gt;&lt;artifactId&gt;spring-cloud-starter-consul-discovery&lt;&#x2F;artifactId&gt;&lt;&#x2F;dependency&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li>项目中的配置文件，你可以参考下面这种：<pre class="line-numbers language-none"><code class="language-none">spring  cloud:    consul:      host: localhost      port: 8500      discovery: # 指定注册对外暴露的服务名称        service-name: $&#123;spring.application.name&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><p>这篇文档并不是专门的配置教程，而且也不难，主要是希望你在选择中间件时多考虑自己的需要，合适最好！</p><p>接下来，我将着手引入 Prometheus 和 Grafana，尽情期待！</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;如题，最近打算在项目中引入 Grafana + Prometheus 作为数据监控，用来监控服务接口调用次数以及重点接口耗时等情况，这个后续</summary>
      
    
    
    
    <category term="项目" scheme="http://example.com/categories/%E9%A1%B9%E7%9B%AE/"/>
    
    
    <category term="项目优化" scheme="http://example.com/tags/%E9%A1%B9%E7%9B%AE%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>如何通过 CloudCanal 实现从 Kafka 到 AutoMQ 的数据迁移</title>
    <link href="http://example.com/2024/07/12/ru-he-tong-guo-cloudcanal-shi-xian-cong-kafka-dao-automq-de-shu-ju-qian-yi/"/>
    <id>http://example.com/2024/07/12/ru-he-tong-guo-cloudcanal-shi-xian-cong-kafka-dao-automq-de-shu-ju-qian-yi/</id>
    <published>2024-07-12T12:34:00.000Z</published>
    <updated>2024-07-12T12:51:15.699Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>实习过程中，接触到了 Kafka 数据迁移，了解到了好多的数据迁移工具，比如 CloudCanal，DataX等。<br>其实我觉得这些对于开发人员来说也是要会的一件事，因为我们自己开发过程中可能会用到将数据从 Mysql 直接同步到消息队列的Topic中，也就是直接模拟日志入队来验证某些功能是否能正常工作，提前发现问题，并且呢这类迁移数据的工具非常的多，且支持很多的源数据源和目标数据源，能解决我们大多数的数据迁移问题。</p><p>所以顺便写了篇文档，希望也能对你有所帮助！</p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a><strong>引言</strong></h2><p>随着大数据技术的飞速发展，Apache Kafka 作为一种高吞吐量、低延迟的分布式消息系统，已经成为企业实时数据处理的核心组件。然而，随着业务的扩展和技术的发展，企业面临着不断增加的存储成本和运维复杂性问题。为了更好地优化系统性能和降低运营成本，企业开始寻找更具优势的消息系统解决方案。其中，<a href="https://docs.automq.com/zh/docs/automq-opensource/HSiEwHVfdiO7rWk34vKcVvcvn2Z">AutoMQ</a> [1] 作为一种基于云重新设计的消息系统，凭借其显著的成本优势和弹性能力，成为了企业的理想选择。</p><h3 id="AutoMQ-概述"><a href="#AutoMQ-概述" class="headerlink" title="AutoMQ 概述"></a><strong>AutoMQ 概述</strong></h3><p>AutoMQ 基于云重新设计了 Kafka，将存储分离至对象存储，在保持与 Apache Kafka 100% 兼容的前提下，为用户提供高达10倍的成本优势和百倍的弹性优势。AutoMQ 通过构建在S3上的流存储库 S3Stream，将存储卸载至云厂商提供的共享云存储 EBS 和 S3，提供低成本、低延时、高可用、高可靠和无限容量的流存储能力。与传统的Shared Nothing 架构相比，AutoMQ 采用了 Shared Storage 架构，显著降低了存储和运维的复杂性，同时提升了系统的弹性和可靠性。</p><p>AutoMQ 的设计理念和技术优势使其成为替换企业现有 Kafka 集群的理想选择。通过采用 AutoMQ，企业可以显著降低存储成本，简化运维，并实现集群的自动扩缩容和流量自平衡，从而更高效地应对业务需求的变化。</p><p><img src="/images/a3128234-2cc4-4e0a-9561-2f8c5654e2ec.gif"></p><h3 id="CloudCanal-概述"><a href="#CloudCanal-概述" class="headerlink" title="CloudCanal 概述"></a><strong>CloudCanal 概述</strong></h3><p><a href="https://www.clougence.com/?src=cc-doc">CloudCanal</a> [2] 是一款数据同步、迁移工具，帮助企业构建高质量数据管道，具备实时高效、精确互联、稳定可拓展、一站式、混合部署、复杂数据转换等优点。CloudCanal 支持数据迁移、数据同步、结构迁移和同步、数据校验和订正等功能，能够满足企业在数据管理过程中对于数据质量和稳定性的高要求。通过消费源端数据源的增量操作日志，CloudCanal 可以准实时地在对端数据源重放操作，以达到数据同步的目的。</p><h3 id="数据迁移的必要性"><a href="#数据迁移的必要性" class="headerlink" title="数据迁移的必要性"></a><strong>数据迁移的必要性</strong></h3><p>在企业的日常运营中，数据系统的升级和迁移是不可避免的。例如，当企业的电商平台面临流量激增和数据量爆炸式增长时，现有的 Kafka 集群可能无法满足需求，导致性能瓶颈和存储成本的显著增加。为了应对这些挑战，企业可能决定迁移到更具成本效益和弹性的 AutoMQ 系统。</p><p>在这种迁移过程中，全量同步和增量同步都是关键步骤。全量同步可以将 Kafka 中的所有现有数据迁移到 AutoMQ，确保基础数据的完整性。增量同步则在全量同步完成后，实时捕捉和同步 Kafka 中的新增和变更数据，确保在迁移过程中，两个系统之间的数据保持一致。接下来，我将以增量同步为例，详细介绍如何使用 CloudCanal 实现从 Kafka 到 AutoMQ 的数据迁移，确保数据在迁移过程中保持一致和完整。</p><h2 id="前置条件"><a href="#前置条件" class="headerlink" title="前置条件"></a>前置条件</h2><p>在进行数据迁移之前，需要确保以下前提条件已经满足。本文将以一个 Kafka 节点和一个 AutoMQ 节点为例，演示增量同步的过程。</p><ol><li><p><strong>Kafka</strong> <strong>节点</strong>：一个已部署并运行的 Kafka 节点，确保 Kafka 节点能够正常接收和处理消息，Kafka节点的网络配置允许与 CloudCanal 服务通信。</p></li><li><p><strong>AutoMQ 节点</strong>：一个已部署并运行的 AutoMQ 节点，确保 AutoMQ 节点能够正常接收和处理消息，AutoMQ 节点的网络配置允许与 CloudCanal 服务通信。</p></li><li><p><strong>CloudCanal 服务</strong>: 已部署和配置好的 CloudCanal 服务。</p></li></ol><h2 id="部署-AutoMQ、kafka-以及-CloudCanal"><a href="#部署-AutoMQ、kafka-以及-CloudCanal" class="headerlink" title="部署 AutoMQ、kafka 以及 CloudCanal"></a>部署 AutoMQ、kafka 以及 CloudCanal</h2><h3 id="部署-AutoMQ"><a href="#部署-AutoMQ" class="headerlink" title="部署 AutoMQ"></a>部署 AutoMQ</h3><p>参考 AutoMQ 官网文档: <a href="https://docs.automq.com/zh/docs/automq-opensource/EvqhwAkpriAomHklOUzcUtybn7g">QuickStart | AutoMQ</a> [3]</p><h3 id="部署-Kafka"><a href="#部署-Kafka" class="headerlink" title="部署 Kafka"></a>部署 Kafka</h3><p>参考 Apache Kafka 官方文档：<a href="https://kafka.apache.org/quickstart">QuickStart | Kafka</a> [4]</p><h3 id="部署-CloudCanal"><a href="#部署-CloudCanal" class="headerlink" title="部署 CloudCanal"></a>部署 CloudCanal</h3><h4 id="安装与启动"><a href="#安装与启动" class="headerlink" title="安装与启动"></a>安装与启动</h4><ol><li>安装基础工具</li></ol><pre class="line-numbers language-YAML" data-language="YAML"><code class="language-YAML">## ubuntusudo apt updatesudo apt install apt-transport-https ca-certificates curl gnupg-agent software-properties-commonsudo apt-get install -y lsofsudo apt-get install -y bcsudo apt-get install -y p7zip-full<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="2"><li>下载安装包</li></ol><p>登录 <a href="https://www.clougence.com/?src=cc-doc-install-linux">CloudCanal 官方网站</a> [5]，点击下载私有部署版按钮，获取软件包下载链接。下载并解压到文件夹<code>/opt/</code></p><pre class="line-numbers language-Bash" data-language="Bash"><code class="language-Bash">cd &#x2F;opt# 下载wget -cO cloudcanal.7z &quot;$&#123;软件包下载链接&#125;&quot;# 解压7z x cloudcanal.7z -o.&#x2F;cloudcanal_homecd cloudcanal_home&#x2F;install_on_docker<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>install_on_docker</code>目录内容包括</p><ul><li><strong>镜像</strong>: images 目录下四个 tar 结尾的压缩文件</li><li><strong>docker 容器编排文件</strong>: docker-compose.yml 文件</li><li><strong>脚本</strong>：一些管理 CloudCanal 容器以及维护的脚本</li></ul><ol start="3"><li>准备 Docker 环境</li></ol><p>请确保以下端口未被占用</p><table><thead><tr><th></th><th></th><th></th></tr></thead><tbody><tr><td>组件</td><td>端口</td><td>用途</td></tr><tr><td>cloudcanal-mysql</td><td>25000</td><td>元数据库 mysql 对外映射端口</td></tr><tr><td>cloudcanal-prometheus</td><td>19090</td><td>prometheuse 监控指标查询端口</td></tr><tr><td>cloudcanal-console</td><td>7007</td><td>console 和 sidecar 通信端口</td></tr><tr><td>cloudcanal-console</td><td>8111</td><td>console web控制台端口</td></tr><tr><td>cloudcanal-sidecar</td><td>18787</td><td>任务 debug 端口（e.g.,自定义代码 debug）</td></tr></tbody></table><p>如果你没有 docker 和 docker compose 环境，可参考 <a href="https://docs.docker.com/engine/install/">Docker 官方文档</a> [6] (版本 <strong>17.x.x</strong> 及以上)。也可直接使用目录中提供的脚本进行安装：</p><pre class="line-numbers language-YAML" data-language="YAML"><code class="language-YAML">## ubuntu,进入 install_on_docker 目录bash .&#x2F;support&#x2F;install_ubuntu_docker.sh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ol start="4"><li>启动 CloudCanal，执行安装脚本以启动：</li></ol><pre class="line-numbers language-YAML" data-language="YAML"><code class="language-YAML">## ubuntubash install.sh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>出现如下标识即安装成功<br><img src="/images/sucess.png"></p><h4 id="激活-CloudCanal"><a href="#激活-CloudCanal" class="headerlink" title="激活 CloudCanal"></a>激活 CloudCanal</h4><p>安装成功后，你可以通过 <code>http://&#123;ip&#125;:8111</code> 在浏览器中访问 CloudCanal 的控制台。注意，如果无法正常访问页面，可以尝试通过脚本更新当前 CloudCanal 的版本，可使用如下命令：</p><pre class="line-numbers language-YAML" data-language="YAML"><code class="language-YAML"># 进入安装目录cd &#x2F;opt&#x2F;cloudcanal_home&#x2F;install_on_docker# 停止当前 CloudCanalsudo bash stop.sh# 更新并启动新的 CloudCanalsudo bash upgrade.sh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol><li>进入登录界面后，通过试用账号登录<ul><li>账号: <code>test@clougence.com</code></li><li>密码: <code>clougence2021</code></li><li>默认验证码: 777777</li></ul></li><li>登录成功，需要激活 CloudCanal 账号即可正常使用。申请免费许可证并激活: <a href="https://www.clougence.com/cc-doc/license/license_use">许可证获取 | CloudCanal</a> [7]，激活成功后，主界面状态为：<br><img src="/images/home.png"></li></ol><h2 id="数据迁移过程"><a href="#数据迁移过程" class="headerlink" title="数据迁移过程"></a>数据迁移过程</h2><h3 id="准备源端-Kafka-数据"><a href="#准备源端-Kafka-数据" class="headerlink" title="准备源端 Kafka 数据"></a>准备源端 Kafka 数据</h3><p>可以选择如下方式：</p><ul><li><p>CloudCanal 提供的 Mysql-&gt;Kafka 数据同步过程，参考：<a href="https://www.clougence.com/cc-doc/bestPractice/mysql_kafka_sync">MySQL 到 Kafka 同步 | CloudCanal</a> [8]</p></li><li><p>通过 Kafka SDK 准备数据</p></li><li><p>通过 Kafka 提供的脚本手动生产消息</p></li></ul><p>这里我将通过 Kafka SDK 的方式进行数据准备，下面是参考代码：</p><pre class="line-numbers language-Java" data-language="Java"><code class="language-Java">import org.apache.kafka.clients.admin.AdminClient;import org.apache.kafka.clients.admin.AdminClientConfig;import org.apache.kafka.clients.admin.NewTopic;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.ProducerConfig;import org.apache.kafka.clients.producer.ProducerRecord;import org.apache.kafka.clients.producer.RecordMetadata;import org.apache.kafka.common.serialization.StringSerializer;import java.util.ArrayList;import java.util.List;import java.util.Properties;import java.util.concurrent.ExecutionException;public class KafkaTest &#123;    private static final String BOOTSTRAP_SERVERS &#x3D; &quot;$&#123;kafka_broker_ip:port&#125;&quot;; &#x2F;&#x2F;修改为你自己的 Kafka 节点地址    private static final int NUM_TOPICS &#x3D; 50;    private static final int NUM_MESSAGES &#x3D; 500;    public static void main(String[] args) throws Exception &#123;        KafkaTest test &#x3D; new KafkaTest();        test.createTopics();        test.sendMessages();    &#125;        &#x2F;&#x2F; 创建50个 Topic，格式为 Topic-n    public void createTopics() &#123;        Properties props &#x3D; new Properties();        props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVERS);        try (AdminClient adminClient &#x3D; AdminClient.create(props)) &#123;            List&lt;NewTopic&gt; topics &#x3D; new ArrayList&lt;&gt;();            for (int i &#x3D; 1; i &lt;&#x3D; NUM_TOPICS; i++) &#123;                topics.add(new NewTopic(&quot;Topic-&quot; + i, 1, (short) 1));            &#125;            adminClient.createTopics(topics).all().get();            System.out.println(&quot;Topics created successfully&quot;);        &#125; catch (InterruptedException | ExecutionException e) &#123;            e.printStackTrace();        &#125;    &#125;    &#x2F;&#x2F; 为50个 Topic-n 分别发送序号从1到1000共一千条消息，消息格式为 Json格式    public void sendMessages() &#123;        Properties props &#x3D; new Properties();        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVERS);        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());        try (KafkaProducer&lt;String, String&gt; producer &#x3D; new KafkaProducer&lt;&gt;(props)) &#123;            for (int i &#x3D; 1; i &lt;&#x3D; NUM_TOPICS; i++) &#123;                String topic &#x3D; &quot;Topic-&quot; + i;                for (int j &#x3D; 1; j &lt;&#x3D; NUM_MESSAGES; j++) &#123;                    String key &#x3D; &quot;key-&quot; + j;                    String value &#x3D; &quot;&#123;\&quot;userId\&quot;: &quot; + j + &quot;, \&quot;action\&quot;: \&quot;visit\&quot;, \&quot;timestamp\&quot;: &quot; + System.currentTimeMillis() + &quot;&#125;&quot;;                    ProducerRecord&lt;String, String&gt; record &#x3D; new ProducerRecord&lt;&gt;(topic, key, value);                    producer.send(record, (RecordMetadata metadata, Exception exception) -&gt; &#123;                        if (exception &#x3D;&#x3D; null) &#123;                            System.out.printf(&quot;Sent message to topic %s partition %d with offset %d%n&quot;, metadata.topic(), metadata.partition(), metadata.offset());                        &#125; else &#123;                            exception.printStackTrace();                        &#125;                    &#125;);                &#125;            &#125;            System.out.println(&quot;Messages sent successfully&quot;);        &#125;    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>创建完成后，可以通过各种可视化工具查看 Kafka 节点状态，比如 <a href="https://redpanda.com/redpanda-console-kafka-ui">Redpanda Console</a> [9]、<a href="https://github.com/obsidiandynamics/kafdrop">Kafdrop</a> [10] 等。这里我选择 Redpanda Console，可以看到当前已经有了 50 个Topic，并且每个 Topic 下有500条初始消息。<br><img src="/images/50.png"></p><p>其中消息的格式为 Json：</p><pre class="line-numbers language-JSON" data-language="JSON"><code class="language-JSON">&#123;    &quot;action&quot;: &quot;INSERT&#x2F;UPDATE&#x2F;DELETE&quot;,    &quot;bid&quot;: 1,    &quot;before&quot;: [],    &quot;data&quot;: [&#123;        &quot;id&quot;:&quot;string data&quot;,        &quot;username&quot;:&quot;string data&quot;,        &quot;user_id&quot;:&quot;string data&quot;,        &quot;ip&quot;:&quot;string data&quot;,        &quot;request_time&quot;:&quot;1608782968300&quot;,&quot;request_type&quot;:&quot;string data&quot;&#125;],    &quot;db&quot;: &quot;access_log_db&quot;,    &quot;schema&quot;: &quot;&quot;,    &quot;table&quot;:&quot;access_log&quot;,    &quot;dbValType&quot;: &#123;        &quot;id&quot;:&quot;INT&quot;,        &quot;username&quot;:&quot;VARCHAR&quot;,        &quot;user_id&quot;:&quot;INT&quot;,        &quot;ip&quot;:&quot;VARCHAR&quot;,        &quot;request_time&quot;:&quot;TIMESTAMP&quot;,        &quot;request_type&quot;:&quot;VARCHAR&quot;,&#125;,    &quot;jdbcType&quot;: &#123;        &quot;id&quot;:&quot;0&quot;,        &quot;username&quot;:&quot;0&quot;,        &quot;user_id&quot;:&quot;0&quot;,        &quot;ip&quot;:&quot;0&quot;,        &quot;request_time&quot;:&quot;0&quot;,        &quot;request_type&quot;:&quot;0&quot;,&#125;,    &quot;entryType&quot;: &quot;ROWDATA&quot;,    &quot;isDdl&quot;: false,    &quot;pks&quot;: [&quot;id&quot;],    &quot;execTs&quot;: 0,    &quot;sendTs&quot;: 0,    &quot;sql&quot;: &quot;&quot;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>并且，AutoMQ 节点当前并无任何数据：<br><img src="/images/automq.png"></p><h3 id="添加-CloudCanal-数据源"><a href="#添加-CloudCanal-数据源" class="headerlink" title="添加 CloudCanal 数据源"></a>添加 CloudCanal 数据源</h3><p>CloudCanal 界面上方 数据源管理 -&gt; 新增数据源<br><img src="/images/datasource.png"></p><p>同理增加 Kafka 数据源，并对两个节点都进行连接测试，可以得到如下结果：<br><img src="/images/23.png"></p><h3 id="创建数据迁移任务"><a href="#创建数据迁移任务" class="headerlink" title="创建数据迁移任务"></a>创建数据迁移任务</h3><ol><li><p>CloudCanal 界面上方 同步任务-&gt;创建任务<br><img src="/images/task.png"></p></li><li><p>选择任务规格，这取决于你需要迁移的数据量大小：<br><img src="/images/2-2.png"></p></li><li><p>选择需要进行数据迁移的 Topics：<br><img src="/images/topic.png"></p></li><li><p>任务确定<br><img src="/images/task2.png"></p></li><li><p>任务创建完成后默认自动启动，会跳转到任务列表，你还需要更改源数据源配置以开启心跳配置，能及时更新任务状态，步骤为 任务详情-&gt;源数据源配置-&gt;修改配置-&gt;生效配置：<br><img src="/images/5-2.png"></p></li><li><p>随后等待任务重启完成，即可看到如下情况：<br><img src="/images/6-1.png"></p></li><li><p>验证 AutoMQ 中是否已经正确创建了 Topic 结构<br><img src="/images/7-1.png"></p></li></ol><h3 id="准备增量数据"><a href="#准备增量数据" class="headerlink" title="准备增量数据"></a>准备增量数据</h3><p>任务已经正常运行，接下来我们需要准备增量数据，使得迁移任务能够将增量数据同步到 AutoMQ。这里我们仍然通过 Kafka SDK 新增数据。新增数据之后，我们可以通过 <strong>任务详情-&gt;增量同步-&gt;查看日志-&gt;任务运行日志</strong> 中查看任务执行情况：</p><pre class="line-numbers language-Java" data-language="Java"><code class="language-Java">2024-07-11 17:16:45.995 [incre-fetch-from-buffer-14-thd-0] INFO  c.c.c.mq.worker.reader.kafka.KafkaIncreEventBroker - getWithoutAck successfully, batch:64, real:642024-07-11 17:16:45.995 [incre-fetch-from-buffer-14-thd-0] INFO  c.c.c.mq.worker.reader.kafka.KafkaIncreEventBroker - getWithoutAck successfully, batch:64, real:642024-07-11 17:16:45.996 [incre-fetch-from-buffer-14-thd-0] INFO  c.c.c.mq.worker.reader.kafka.KafkaIncreEventBroker - getWithoutAck successfully, batch:64, real:642024-07-11 17:16:45.996 [incre-fetch-from-buffer-14-thd-0] INFO  c.c.c.mq.worker.reader.kafka.KafkaIncreEventBroker - getWithoutAck successfully, batch:64, real:642024-07-11 17:16:45.996 [incre-fetch-from-buffer-14-thd-0] INFO  c.c.c.mq.worker.reader.kafka.KafkaIncreEventBroker - getWithoutAck successfully, batch:64, real:642024-07-11 17:16:45.997 [incre-fetch-from-buffer-14-thd-0] INFO  c.c.c.mq.worker.reader.kafka.KafkaIncreEventBroker - getWithoutAck successfully, batch:64, real:642024-07-11 17:16:45.997 [incre-fetch-from-buffer-14-thd-0] INFO  c.c.c.mq.worker.reader.kafka.KafkaIncreEventBroker - getWithoutAck successfully, batch:64, real:642024-07-11 17:16:45.997 [incre-fetch-from-buffer-14-thd-0] INFO  c.c.c.mq.worker.reader.kafka.KafkaIncreEventBroker - getWithoutAck successfully, batch:64, real:642024-07-11 17:16:45.998 [incre-fetch-from-buffer-14-thd-0] INFO  c.c.c.mq.worker.reader.kafka.KafkaIncreEventBroker - getWithoutAck successfully, batch:64, real:642024-07-11 17:16:45.998 [incre-fetch-from-buffer-14-thd-0] INFO  c.c.c.mq.worker.reader.kafka.KafkaIncreEventBroker - getWithoutAck successfully, batch:64, real:642024-07-11 17:16:45.998 [incre-fetch-from-buffer-14-thd-0] INFO  c.c.c.mq.worker.reader.kafka.KafkaIncreEventBroker - getWithoutAck successfully, batch:64, real:642024-07-11 17:16:45.999 [incre-fetch-from-buffer-14-thd-0] INFO  c.c.c.mq.worker.reader.kafka.KafkaIncreEventBroker - getWithoutAck successfully, batch:64, real:64<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="验证迁移结果"><a href="#验证迁移结果" class="headerlink" title="验证迁移结果"></a>验证迁移结果</h2><p>验证AutoMQ是否正确同步到消息：<br><img src="/images/11-1.png"></p><p><img src="/images/12-1.png"></p><p>多次新增数据后依旧正常完成迁移：<br><img src="/images/13.png"></p><p>可以看到在增量同步任务执行期间对 Kafka 新增的数据都已经同步到了 AutoMQ 中。至此，我们的迁移过程已经全部完成。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>随着企业数据规模的不断扩大和业务需求的多样化，数据迁移和同步变得尤为重要。通过本文的介绍，我们详细探讨了如何利用 CloudCanal 实现从 Kafka 到 AutoMQ 的增量同步数据迁移，以应对存储成本和运维复杂性的问题。在迁移过程中，增量同步技术确保了数据的一致性和业务的连续性，为企业提供了一个高效、可靠的解决方案。</p><p>希望本文能够为你在数据迁移和同步方面提供有价值的参考和指导，帮助实现系统的平滑过渡和性能优化!</p><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p>[1] AutoMQ: <a href="https://docs.automq.com/zh/docs/automq-opensource/HSiEwHVfdiO7rWk34vKcVvcvn2Z">https://docs.automq.com/zh/docs/automq-opensource/HSiEwHVfdiO7rWk34vKcVvcvn2Z</a><br>[2] CloudCanal: <a href="https://www.clougence.com/?src=cc-doc">https://www.clougence.com/?src=cc-doc</a><br>[3] QuickStart | AutoMQ: <a href="https://docs.automq.com/zh/docs/automq-opensource/EvqhwAkpriAomHklOUzcUtybn7g">https://docs.automq.com/zh/docs/automq-opensource/EvqhwAkpriAomHklOUzcUtybn7g</a><br>[4] QuickStart | Kafka: <a href="https://kafka.apache.org/quickstart">https://kafka.apache.org/quickstart</a><br>[5] CloudCanal 官方网站: <a href="https://www.clougence.com/?src=cc-doc-install-linux">https://www.clougence.com/?src=cc-doc-install-linux</a><br>[6] Docker 官方文档: <a href="https://docs.docker.com/engine/install/">https://docs.docker.com/engine/install/</a><br>[7] 许可证获取 | CloudCanal: <a href="https://www.clougence.com/cc-doc/license/license_use">https://www.clougence.com/cc-doc/license/license_use</a><br>[8] MySQL 到 Kafka 同步 | CloudCanal: <a href="https://www.clougence.com/cc-doc/bestPractice/mysql_kafka_sync">https://www.clougence.com/cc-doc/bestPractice/mysql_kafka_sync</a><br>[9] Redpanda Console: <a href="https://redpanda.com/redpanda-console-kafka-ui">https://redpanda.com/redpanda-console-kafka-ui</a><br>[10] Kafdrop: <a href="https://github.com/obsidiandynamics/kafdrop">https://github.com/obsidiandynamics/kafdrop</a><br>[11] FAQ 索引 | CloudCanal: <a href="https://www.clougence.com/cc-doc/faq/cloudcanal_faq_list">https://www.clougence.com/cc-doc/faq/cloudcanal_faq_list</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;实习过程中，接触到了 Kafka 数据迁移，了解到了好多的数据迁移工具，比如 CloudCanal，DataX等。&lt;br&gt;其实我觉得这些对于</summary>
      
    
    
    
    <category term="AutoMQ" scheme="http://example.com/categories/AutoMQ/"/>
    
    
    <category term="AutoMQ" scheme="http://example.com/tags/AutoMQ/"/>
    
  </entry>
  
  <entry>
    <title>如何通过三种方式部署 Redpanda？</title>
    <link href="http://example.com/2024/07/06/ru-he-tong-guo-san-chong-fang-shi-bu-shu-redpanda/"/>
    <id>http://example.com/2024/07/06/ru-he-tong-guo-san-chong-fang-shi-bu-shu-redpanda/</id>
    <published>2024-07-06T15:20:00.000Z</published>
    <updated>2024-07-06T15:28:54.693Z</updated>
    
    <content type="html"><![CDATA[<p>刚好工作中用到 Redpanda，这里就写了一篇在部署过程中的操作和小 Tips。</p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>虽然 redpanda 是基于 Kafka 的流处理系统，但是它是用 C++写的，因此启动方式并不一样，不像kafka那样用脚本就可以实现简单的topic管理和收发消息。</p><blockquote><p>Redpanda有自己的工具集，主要是rpk命令行工具，而不是使用Kafka的启动脚本和管理工具。rpk工具旨在提供一种简单、一致的方式来管理Redpanda集群，包括配置、启动、监控和维护等任务。</p></blockquote><p>由于是 C++ 编写，而且不需要 Zookeeper 和 JVM ,因此速度相比 Kafka 提升很多，大概有下三点：</p><ul><li><p>操作简单。只有一个二进制可执行文件，部署简单，无需jvm、zk。</p></li><li><p>尽可能的零数据丢失。使用raft协议保障数据安全。</p></li><li><p>实测比kafka快10倍。Thread-Per-Core架构和io_uring的加持，使得其比kafka更快。</p></li></ul><blockquote><p>参考：<a href="https://blog.csdn.net/yaxuan88521/article/details/128884371">kafka替代者:Redpanda的架构及部署-CSDN博客</a></p></blockquote><p>官方说明：</p><blockquote><ul><li><p>Redpanda 的设计目标是在任何数据流工作负载上实现最佳性能。</p></li><li><p>它可以扩展以使用单台机器上的所有可用资源，也可以扩展以将性能分布到多个节点。Redpanda 基于 C++ 构建，与其他平台相比，它提供更高的吞吐量和低 10 倍的 p99 延迟。这实现了以前难以想象的使用案例，这些用例需要高吞吐量、低延迟和最小的硬件占用空间。</p></li><li><p>Redpanda 被打包为单个二进制文件：它不依赖于任何外部系统。</p></li><li><p>它与 Kafka API 兼容，因此可以与基于 Kafka 构建的完整工具和集成生态系统配合使用。Redpanda 可以部署在数据中心或云中的裸机、容器或虚拟机上。Redpanda 控制台可让您轻松设置、管理和监控集群。此外，分层存储可让您近乎实时地将日志段卸载到对象存储，从而提供长期数据保留和主题恢复。</p></li><li><p>Redpanda在整个平台上使用<a href="https://raft.github.io/">Raft 共识算法来协调将数据写入日志文件并在多个服务器之间复制该数据。</a></p></li><li><p>Raft 促进了 Redpanda 集群中节点之间的通信，以确保它们同意变更并保持同步，即使其中少数节点处于故障状态。这使得 Redpanda 能够容忍部分环境故障，并在高负载下提供可预测的性能。</p></li><li><p>Redpanda 提供数据主权。</p></li></ul></blockquote><ol><li><h2 id="使用-Docker-Compose-快速体验"><a href="#使用-Docker-Compose-快速体验" class="headerlink" title="使用 Docker Compose 快速体验"></a>使用 Docker Compose 快速体验</h2></li></ol><blockquote><p><a href="https://docs.redpanda.com/current/get-started/quick-start/">https://docs.redpanda.com/current/get-started/quick-start/</a></p></blockquote><p>注意需要设置环境变量，或者手动把docker compose 文件修改，用指定的 console 版本或者 redpanda 版本。</p><ul><li>手动设置环境变量：</li></ul><pre class="line-numbers language-Bash" data-language="Bash"><code class="language-Bash">export REDPANDA_VERSION&#x3D;&lt;desired_version&gt;   # 如23.3.8 export REDPANDA_CONSOLE_VERSION&#x3D;&lt;desired_version&gt;  # 如 2.6.0<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li>或者直接指定版本 （并声明外部地址）</li></ul><pre class="line-numbers language-Bash" data-language="Bash"><code class="language-Bash">version: &quot;3.7&quot;name: redpanda-quickstart-one-brokernetworks:  redpanda_network:    driver: bridgevolumes:  redpanda-0: nullservices:  redpanda-0:    command:      - redpanda      - start      - --kafka-addr internal:&#x2F;&#x2F;0.0.0.0:9092,external:&#x2F;&#x2F;0.0.0.0:19092      # Address the broker advertises to clients that connect to the Kafka API.      # Use the internal addresses to connect to the Redpanda brokers&#39;      # from inside the same Docker network.      # Use the external addresses to connect to the Redpanda brokers&#39;      # from outside the Docker network.      - --advertise-kafka-addr internal:&#x2F;&#x2F;redpanda-0:9092,external:&#x2F;&#x2F;10.1.0.132:19092      - --pandaproxy-addr internal:&#x2F;&#x2F;0.0.0.0:8082,external:&#x2F;&#x2F;0.0.0.0:18082      # Address the broker advertises to clients that connect to the HTTP Proxy.      - --advertise-pandaproxy-addr internal:&#x2F;&#x2F;redpanda-0:8082,external:&#x2F;&#x2F;10.1.0.132:18082      - --schema-registry-addr internal:&#x2F;&#x2F;0.0.0.0:8081,external:&#x2F;&#x2F;0.0.0.0:18081      # Redpanda brokers use the RPC API to communicate with each other internally.      - --rpc-addr redpanda-0:33145      - --advertise-rpc-addr redpanda-0:33145      # Mode dev-container uses well-known configuration properties for development in containers.      - --mode dev-container      # Tells Seastar (the framework Redpanda uses under the hood) to use 1 core on the system.      - --smp 1      - --default-log-level&#x3D;info    image: docker.redpanda.com&#x2F;redpandadata&#x2F;redpanda:v23.3.18-amd64    container_name: redpanda-0    volumes:      - redpanda-0:&#x2F;var&#x2F;lib&#x2F;redpanda&#x2F;data    networks:      - redpanda_network    ports:      - 18081:18081      - 18082:18082      - 19092:19092      - 19644:9644  console:    container_name: redpanda-console    image: docker.redpanda.com&#x2F;redpandadata&#x2F;console:v2.6.0    networks:      - redpanda_network    entrypoint: &#x2F;bin&#x2F;sh    command: -c &#39;echo &quot;$$CONSOLE_CONFIG_FILE&quot; &gt; &#x2F;tmp&#x2F;config.yml; &#x2F;app&#x2F;console&#39;    environment:      CONFIG_FILEPATH: &#x2F;tmp&#x2F;config.yml      CONSOLE_CONFIG_FILE: |        kafka:          brokers: [&quot;redpanda-0:9092&quot;]          schemaRegistry:            enabled: true            urls: [&quot;http:&#x2F;&#x2F;redpanda-0:8081&quot;]        redpanda:          adminApi:            enabled: true            urls: [&quot;http:&#x2F;&#x2F;redpanda-0:9644&quot;]    ports:      - 8080:8080    depends_on:      - redpanda-0<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/images/1-1.png"></p><h3 id="Kafka-原生脚本测试"><a href="#Kafka-原生脚本测试" class="headerlink" title="Kafka 原生脚本测试"></a>Kafka 原生脚本测试</h3><p>使用kafka下的脚本进行测试：</p><pre class="line-numbers language-Bash" data-language="Bash"><code class="language-Bash"># create topicbin&#x2F;kafka-topics.sh --create --topic wangzhichuang --bootstrap-server localhost:19092# 发送消息bin&#x2F;kafka-console-producer.sh --topic wangzhichuang --bootstrap-server localhost:19092# 消费消息bin&#x2F;kafka-console-consumer.sh --topic wangzhichuang --from-beginning --bootstrap-server localhost:19092<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>注意端口一般是19092。</p><p>结果如下：</p><p><img src="/images/2-1.png"></p><h3 id="Redpanda-提供的-rpk-工具"><a href="#Redpanda-提供的-rpk-工具" class="headerlink" title="Redpanda 提供的 rpk 工具"></a>Redpanda 提供的 rpk 工具</h3><ol><li>获取集群信息：</li></ol><pre class="line-numbers language-Bash" data-language="Bash"><code class="language-Bash">docker exec -it redpanda-0 rpk cluster info<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/images/3-2.png"></p><ol start="2"><li>创建一个名为聊天室的主题：</li></ol><pre class="line-numbers language-Bash" data-language="Bash"><code class="language-Bash">docker exec -it redpanda-0 rpk topic create chat-room<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/images/4-1.png"></p><ol start="3"><li>向主题生成一条消息：</li></ol><pre class="line-numbers language-Bash" data-language="Bash"><code class="language-Bash">docker exec -it redpanda-0 rpk topic produce chat-room<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/images/5-1.png"></p><ol start="4"><li>消费一条消息</li></ol><pre class="line-numbers language-Bash" data-language="Bash"><code class="language-Bash">docker exec -it redpanda-0 rpk topic consume chat-room --num 1<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/images/6.png"></p><ol start="5"><li>测试完销毁资源</li></ol><pre class="line-numbers language-Bash" data-language="Bash"><code class="language-Bash">docker compose down<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ol start="2"><li>手动部署单节点</li></ol><p>官方给出的链接里有在 debian 系统上直接安装的，但是好像有些镜像问题，就不随便搞了，换了一个通用的指令，自己解压。</p><blockquote><ul><li><p>自动部署：<a href="https://docs.redpanda.com/current/deploy/deployment-option/self-hosted/manual/production/production-deployment-automation/">https://docs.redpanda.com/current/deploy/deployment-option/self-hosted/manual/production/production-deployment-automation/</a> 使用 Terraform 和 Ansible 等自动化工具部署 Redpanda。</p></li><li><p>Linux上手动部署：<a href="https://docs.redpanda.com/current/deploy/deployment-option/self-hosted/manual/production/dev-deployment/">https://docs.redpanda.com/current/deploy/deployment-option/self-hosted/manual/production/dev-deployment/</a></p></li></ul></blockquote><ul><li>安装</li></ul><pre class="line-numbers language-Bash" data-language="Bash"><code class="language-Bash">curl -1sLf &#39;https:&#x2F;&#x2F;dl.redpanda.com&#x2F;nzc4ZYQK3WRGd9sy&#x2F;redpanda&#x2F;cfg&#x2F;setup&#x2F;bash.deb.sh&#39; | \sudo -E bash &amp;&amp; sudo apt install redpanda -y<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><blockquote><p>可能会遇到这种问题：</p><p><img src="/images/7.png"></p><p>这是因为镜像源的问题，我的解决方法是注释掉该kitware的源： 文件: <code>vim /etc/apt/sources.list.d/kitware.list</code></p><p>注释掉唯一的源。然后重新 apt-get update。最后重新执行上述命令。</p></blockquote><p>正常情况下会得到：</p><p><img src="/images/8.png"></p><p>同理有需要的话安装 redpanda console</p><pre class="line-numbers language-Bash" data-language="Bash"><code class="language-Bash">curl -1sLf &#39;https:&#x2F;&#x2F;dl.redpanda.com&#x2F;nzc4ZYQK3WRGd9sy&#x2F;redpanda&#x2F;cfg&#x2F;setup&#x2F;bash.deb.sh&#39; | \sudo -E bash &amp;&amp; sudo apt-get install redpanda-console -y<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h2><p>接下来就是启动 Redpanda。</p><ol><li>首先确定配置值</li></ol><pre class="line-numbers language-Bash" data-language="Bash"><code class="language-Bash">sudo rpk redpanda config bootstrap --self &lt;listener-address&gt; --advertised-kafka &lt;advertised-kafka-address&gt; --ips &lt;seed-server1-ip&gt;,&lt;seed-server2-ip&gt;,&lt;seed-server3-ip&gt; &amp;&amp; \sudo rpk redpanda config set redpanda.empty_seed_starts_cluster false<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li><p><code>listener-address</code> : 你可以将其设置为0.0.0.0:9092，这表示Redpanda将监听所有可用的网络接口上的9092端口，用于接收Kafka客户端的连接。</p></li><li><p><code>advertised-kafka-address</code> : 这个地址是客户端用来连接到Redpanda的地址。在单节点配置中，这通常是该节点的外部或公网IP地址与端口的组合。假设你的服务器IP是192.168.1.100，那么你可以将其设置为192.168.1.100:9092。</p></li><li><p><code>seed-server-ips</code> : 在单节点集群中，这将是该节点自己的IP地址。使用相同的IP地址，例如192.168.1.100。</p></li></ul><blockquote><p>比如我自己的：</p><p>由于redpanda版本在23.3.8以后才能用 rpk 在后面拼接监听器地址，所以我这里之后要手动修改配置文件：<code>/etc/redpanda/redpanda.yaml</code></p><p>如下部分改为</p><p>暂时无法在飞书文档外展示此内容</p><p><img src="/images/9.png"></p></blockquote><p>随后启动：</p><pre class="line-numbers language-Bash" data-language="Bash"><code class="language-Bash">sudo rpk redpanda config bootstrap --self 10.1.0.132 --ips 10.1.0.132 &amp;&amp; sudo rpk redpanda config set redpanda.empty_seed_starts_cluster falsesudo rpk redpanda config set redpanda.empty_seed_starts_cluster false<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="启动-Redpanda"><a href="#启动-Redpanda" class="headerlink" title="启动 Redpanda"></a>启动 Redpanda</h2><ol><li>系统服务启动</li></ol><pre class="line-numbers language-Bash" data-language="Bash"><code class="language-Bash">sudo systemctl start redpanda-tuner redpanda<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ol start="2"><li>启动成功后可以通过 status 命令查看服务状态如下：</li></ol><pre class="line-numbers language-Bash" data-language="Bash"><code class="language-Bash">sudo systemctl status redpanda<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><blockquote><p>Redpanda-tuner 是Redpanda提供的一个工具，它的主要目的是自动调整Linux系统的配置，以确保Redpanda能够以最佳性能运行。这个工具专门针对Redpanda的运行环境进行优化，包括内核参数、磁盘设置、网络设置等方面的调整。</p><p>因此不会被启动为持久的可用服务，不需要查看状态。</p></blockquote><p><img src="/images/10.png"></p><blockquote><p>如果出现启动过程中卡住的情况，并且查看状态时提示错误，可以着重检查配置文件，即<code>/etc/redpanda/redpanda.yaml</code>文件，注意地址要填成你的服务器ip地址。</p></blockquote><ol start="3"><li>使用 rpk 工具查看集群状态</li></ol><pre class="line-numbers language-Bash" data-language="Bash"><code class="language-Bash">rpk cluster info<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/images/11.png"></p><ul><li>尝试创建 topic</li></ul><pre class="line-numbers language-Bash" data-language="Bash"><code class="language-Bash">rpk topic create &lt;topic-name&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/images/12.png"></p><p>其他命令和 docker-compose 部署后使用相同，请参考之前的内容。</p><ol start="3"><li>部署集群</li></ol><p>其实和上述类似，只是配置文件需要指定多台机器。</p><p>主要是修改配置文件<code>/etc/redpanda/redpanda.yaml</code>：</p><pre class="line-numbers language-YAML" data-language="YAML"><code class="language-YAML">redpanda:    data_directory: &#x2F;var&#x2F;lib&#x2F;redpanda&#x2F;data    empty_seed_starts_cluster: false    seed_servers:        - host:            address: &lt;seed-server1-ip&gt;            port: 33145        - host:            address: &lt;seed-server2-ip&gt;            port: 33145        - host:            address: &lt;seed-server3-ip&gt;            port: 33145    rpc_server:        address: &lt;listener-address&gt;        port: 33145    kafka_api:        - address: &lt;listener-address&gt;          port: 9092    admin:        - address: &lt;listener-address&gt;          port: 9644    advertised_rpc_api:        address: &lt;listener-address&gt;        port: 33145    advertised_kafka_api:        - address: &lt;advertised-kafka-address&gt;          port: 9092<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>至此 redpanda 的部署已经完成！</p><p>下面开始集成到 KPC 项目。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;刚好工作中用到 Redpanda，这里就写了一篇在部署过程中的操作和小 Tips。&lt;/p&gt;
&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;虽然 redpanda 是基于 Kafka</summary>
      
    
    
    
    
    <category term="Redpanda" scheme="http://example.com/tags/Redpanda/"/>
    
  </entry>
  
  <entry>
    <title>项目优化--冷热数据分离</title>
    <link href="http://example.com/2024/04/16/xiang-mu-you-hua-leng-re-shu-ju-fen-chi/"/>
    <id>http://example.com/2024/04/16/xiang-mu-you-hua-leng-re-shu-ju-fen-chi/</id>
    <published>2024-04-16T09:33:00.000Z</published>
    <updated>2024-04-16T10:07:25.477Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>不一定做，毕竟比较复杂，暂时想点方案。</p></blockquote><h2 id="为什么要做"><a href="#为什么要做" class="headerlink" title="为什么要做"></a>为什么要做</h2><blockquote><p>其实做冷热分离是为了<u>提高性能，降低存储成本</u>的一种存储数据的策略。</p></blockquote><p>单日访问统计表，也就是 <code>t_stats_today</code>  表，没必要做分表，因为这个场景是用户想查询某一天该数据的访问记录或者该分组下所有数据的访问记录，这个实际上用到的可能性很小（因为我们默认的查询区间是最近一个星期的访问记录，对于之前的某一天的访问记录，一般用不到）。</p><p>那我们可以做个优化，也就是<mark style="background: #FFFF00;">冷热数据分离</mark>，把最近一段时间的单日访问记录存到数据表中，但是更早的数据可以通过备份表（用一个 back 标识为单日访问记录的备份表）存储到适合大容量存储的数据库但是查询效率相比于热库较低，比如 Postgresql 等。如果用户真要访问这种冷门的数据再从冷库中查询。</p><p>同时也是为了适应取消 gid 之后，减少查询当日访问统计记录的关联查询次数。那这样我们那个绑定表导致的笛卡尔积问题就不用考虑了。<a href="../%E9%A1%B9%E7%9B%AE%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/4%20%E9%A1%B9%E7%9B%AE%E9%97%AE%E9%A2%98--%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9B%B8%E5%90%8C%E5%88%86%E7%89%87%E9%94%AE%E4%BD%BF%E7%94%A8left%20join%E4%BC%9A%E5%87%BA%E7%8E%B0%E7%AC%9B%E5%8D%A1%E5%B0%94%E7%A7%AF.md">4 项目问题–分库分表相同分片键使用left join会出现笛卡尔积</a> </p><h2 id="其他问题"><a href="#其他问题" class="headerlink" title="其他问题"></a>其他问题</h2><h3 id="怎么实施冷热数据分离"><a href="#怎么实施冷热数据分离" class="headerlink" title="怎么实施冷热数据分离"></a>怎么实施冷热数据分离</h3><blockquote><p>可以看字节的方案：字节跳动技术团队-冷热数据分分离方案: <a href="https://mp.weixin.qq.com/s/ZKRkZP6rLHuTE1wvnqmAPQ">https://mp.weixin.qq.com/s/ZKRkZP6rLHuTE1wvnqmAPQ</a> </p></blockquote><p>实习的时候就有这种场景，把大量的推荐以及推送日志冷热分离，减少对线上数据库的存储压力。（具体还在研究如何实现）</p><ul><li><input disabled="" type="checkbox"> #task 如何实施冷热分离？ 🔽</li></ul><p>应该要结合项目考虑。</p><h3 id="冷热分离有什么问题"><a href="#冷热分离有什么问题" class="headerlink" title="冷热分离有什么问题"></a>冷热分离有什么问题</h3><ol><li>首先你的代码复杂度会提高很多，你划分冷热数据，在比较复杂的场景下很难实行。如果<strong>划分不够好，导致频繁的访问冷库数据</strong>，性能会降低很多。</li><li>还有问题就是<strong>冷热数据的同步</strong>，以及<strong>数据一致性问题</strong>，你打算什么时候进行热库同步数据到冷库呢。</li><li><strong>高可用性</strong>，对于不同的数据库，你维护的方式也不同，不管是冷库还是热库挂掉或者性能下降都会导致系统问题。</li></ol><p>… 大概想到这么点，可能会再补充吧。</p><h3 id="怎么定义冷数据和热数据的"><a href="#怎么定义冷数据和热数据的" class="headerlink" title="怎么定义冷数据和热数据的"></a>怎么定义冷数据和热数据的</h3><p>冷数据一般就是线上环境中<strong>对于一些实时性要求较低的功能</strong>，比如支付，你不可能交给冷库来做，因为性能不够高，而且它的数据是由热库按照一定规则同步过来的，可能会有一些延迟。</p><blockquote><p>冷数据是不经常访问的数据。它可以<u>存储在更便宜、更慢速的存储介质</u>上，如高容量硬盘驱动器或者云存储服务的冷数据层。</p></blockquote><p>热数据相反，<strong>就是对于实时性以及计算要求高的需求</strong>，需要用热库快速查询和返回数据，并且保证数据的准确性。</p><blockquote><p>gpt 参考：- <strong>热数据</strong>：指的是<mark style="background: #FFFF00;">经常被访问和修改的数据</mark>。这类数据应该<u>存储在快速、低延迟的存储系统</u>上，以便快速访问。例如，活跃的数据库记录或者需要实时处理的数据。</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;不一定做，毕竟比较复杂，暂时想点方案。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;为什么要做&quot;&gt;&lt;a href=&quot;#为什么要做&quot; class=&quot;headerlink&quot; title=&quot;为什么要做&quot;&gt;&lt;/a&gt;为什么要做&lt;/h2&gt;&lt;blockq</summary>
      
    
    
    
    <category term="冷热数据分离" scheme="http://example.com/categories/%E5%86%B7%E7%83%AD%E6%95%B0%E6%8D%AE%E5%88%86%E7%A6%BB/"/>
    
    
    <category term="冷热分离" scheme="http://example.com/tags/%E5%86%B7%E7%83%AD%E5%88%86%E7%A6%BB/"/>
    
  </entry>
  
  <entry>
    <title>关于ThreadLocal管理资源出现内存泄漏问题的补充</title>
    <link href="http://example.com/2024/04/10/guan-yu-threadlocal-guan-li-zi-yuan-chu-xian-nei-cun-xie-lou-wen-ti-de-bu-chong/"/>
    <id>http://example.com/2024/04/10/guan-yu-threadlocal-guan-li-zi-yuan-chu-xian-nei-cun-xie-lou-wen-ti-de-bu-chong/</id>
    <published>2024-04-10T15:09:00.000Z</published>
    <updated>2024-04-10T15:10:13.769Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>之前没想过这个问题，这里带着一些小疑问整理一下。</p></blockquote><h3 id="ThreadLocal-内存泄漏问题"><a href="#ThreadLocal-内存泄漏问题" class="headerlink" title="ThreadLocal 内存泄漏问题"></a>ThreadLocal 内存泄漏问题</h3><blockquote><p>Tip：2024-04-10</p></blockquote><p><mark style="background: #FFFF00;">为什么会发生内存泄漏问题</mark>？</p><p>首先我们都知道 ThreadLocal 是通过把自身对象作为 key，要管理的资源作为 value 在线程内部 map 对象中进行存储的。</p><p>而这个 <strong>key 是弱引用，vlaue 是强引用</strong>。既然是弱引用，如果这个 ThreadLocal 对象没有被其他程序引用，那可能就会被 JVM 垃圾回收掉，那会出现什么情况？<strong>空 key</strong>！那 <strong>value 永远无法被访问到，而且也不会被清理掉，这就是内存泄漏问题</strong>。</p><p>避免方法：<u>尽量在使用完后，调用 remove 方法</u>，<u>手动删掉资源对象，避免出现内存泄漏问题</u>。</p><p>而且 threadlocal 在设计的时候也会在调用 get，set，remove 方法时判断 key 是否为 null 了，如果为 null 直接就清理掉这个键值对。</p><blockquote><p>小问题：Threadlocal 对象为什么是弱引用？咋判断的？</p><ul><li><mark style="background: #FFFF00;">设计为弱引用是为了避免内存泄漏</mark>!!?？</li></ul><p>原因在于，如果是强引用的 key，说明这个 threadlocalmap 只要用到了这个 ThreadLocal 对象作为 key，那就是强引用。这有啥问题？<u>只要线程一直活跃，那 Threadlocalmap 中就一直保留着 Threadlocal 的对象，哪怕没有被用到，也不会被回收（强引用）</u>，那不就是内存泄漏问题吗？</p><p>所以设计为弱引用，只要我们注意 remove 掉不用的资源，加上 Threadlocal 自己的优化，能尽可能的避免内存泄漏问题。</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;之前没想过这个问题，这里带着一些小疑问整理一下。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;ThreadLocal-内存泄漏问题&quot;&gt;&lt;a href=&quot;#ThreadLocal-内存泄漏问题&quot; class=&quot;headerlink&quot; titl</summary>
      
    
    
    
    
    <category term="ThreadLocal" scheme="http://example.com/tags/ThreadLocal/"/>
    
  </entry>
  
  <entry>
    <title>跳表skipList解析</title>
    <link href="http://example.com/2024/02/28/tiao-biao-skiplist-jie-xi/"/>
    <id>http://example.com/2024/02/28/tiao-biao-skiplist-jie-xi/</id>
    <published>2024-02-28T13:53:00.000Z</published>
    <updated>2024-04-05T03:03:04.890Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h2><p>面试的时候问了这些。所以要弄清楚具体的查询过程，以及跳表的结构，还有实现利用 ZSET 实现<strong>排行榜是怎么从跳表上获取元素的</strong>。怎么获取某个数据的名次。</p><p>老规矩，先来一个对整体的大概了解。</p><blockquote><ol><li>跳表的结构，多层链表，链表节点中有多个指针（从而形成多层链表）</li><li>跳表进行范围查询，不是以两端点为准查到底层后，返回两端点之间的值。而是通过从顶层查询范围查询中的 start 开始，大于等于 start 的第一个元素，找到之后直接往下层走，继续找第一个大于等于 start 的元素，最后到达底层，就在底层链表上，往右遍历，直到遇到第一个大于 end 的元素为止结束查询。</li><li>跳表进行单点查询（也就是查询某个元素的具体权重），是通过哈希表实现的，跳表的多层结构只负责查询节点值，而不负责权重的存储。这样一想确实是，哈希表存储这种键值对不是更好嘛，查询效率是 O（1）。比如 LRU 算法，不也是用哈希表来存储缓存值，并通过双向链表表示元素的使用情况嘛，最新插入的元素在头部或者尾部，具体实现决定。</li></ol></blockquote><p>总之呢，就是范围查询是由跳表实现的，单点查询由哈希表实现。而这两种操作对应了 ZSET 的命令：<code>ZRANGEBYSCORE</code> 返回权重范围内的元素。<code>ZSCORE</code>获取某个元素的权重值。</p><h2 id="2-跳表结构"><a href="#2-跳表结构" class="headerlink" title="2. 跳表结构"></a>2. 跳表结构</h2><p>那现在就开始具体源码分析吧，基本查询过程就上面分析的那样。这里就重在理解其本质。</p><h3 id="2-1-ZSET-的结构体"><a href="#2-1-ZSET-的结构体" class="headerlink" title="2.1. ZSET 的结构体"></a>2.1. ZSET 的结构体</h3><pre class="line-numbers language-c" data-language="c"><code class="language-c">typedef struct zset &#123;    dict *dict;    zskiplist *zsl;&#125; zset;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>这里要知道，所有的数据结构其底层都是通过 RedisObject 把所有这些数据结构封装了（比如 zset, sds, listpack 等），然后类似哈希表那种，在桶数组上某些节点用指针指向具体的数据结构实体。从而保证 Redis 能存储各式各样的数据。</p></blockquote><p>可以看出嘛，是由字典 dict 以及跳表实现。具体字典实现参考：<a href="Redis--%E6%B0%B8%E4%B9%85%E7%AC%94%E8%AE%B0--dict%E5%AD%97%E5%85%B8%E7%BB%93%E6%9E%84.md">Redis–永久笔记–dict字典结构</a> ，而跳表就仔细讲讲。</p><h3 id="2-2-跳表的节点"><a href="#2-2-跳表的节点" class="headerlink" title="2.2. 跳表的节点"></a>2.2. 跳表的节点</h3><p>比如下面的跳表结构：<br><img src="/images/%E8%B7%B3%E8%A1%A8skipList.png"></p><p>可以看到，在 level 0 上一共有 7 个结点，分别是 3、11、23、33、42、51、62，这些结点会通过指针连接起来，同时头结点中的 level0 指针会指向结点 3。然后，在这 7 个结点中，结点 11、33 和 51 又都包含了一个指针，同样也依次连接起来，且头结点的 level 1 指针会指向结点 11。这样一来，这 3 个结点就组成了 level 1 上的所有结点。</p><p>最后，结点 33 中还包含了一个指针，这个指针会指向尾结点，同时，头结点的 level 2 指针会指向结点 33，这就形成了 level 2，只不过 level 2 上只有 1 个结点。</p><blockquote><p>这时候就想到了，这具体是咋划分的呢，也就是非底层节点到底选哪些下一层的节点作为这一层的节点，选多少个节点呢？</p><p>这个在后面再说。[[Redis–永久笔记–跳表skipList数据结构#3. 跳表的其他参数]] </p></blockquote><hr><p>看一下跳表节点的结构体源码：</p><pre class="line-numbers language-c" data-language="c"><code class="language-c">typedef struct zskiplistNode &#123;    sds ele;   &#x2F;&#x2F;Sorted Set中的元素    double score;  &#x2F;&#x2F;元素权重值    struct zskiplistNode *backward;    &#x2F;&#x2F;后向指针        &#x2F;&#x2F;节点的level数组，保存每层上的前向指针和跨度    struct zskiplistLevel &#123;        struct zskiplistNode *forward;        unsigned long span;    &#125; level[];        &#x2F;&#x2F; 就是多层链表中每一层的各节点的前指针以及跨越的节点个数&#125; zskiplistNode;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><code>ele</code> 是 <strong>SDS 类型</strong>的，也就是存储加入到 zset 的元素值，而 <code>score</code> 是该值的权重，用来作为排序的依据。</li><li><code>backward </code> 好奇怪，你<strong>指向前一个节点</strong>，为什么要命名成这样 ,back? 其实好像也可以理解，因为这个指针是为了方便从链表的尾部往前进行遍历，所以从后开始就是 back 喽。</li><li>其实是可以看到里面那个多层链表结构体 <code>zskiplistLevel</code> 是一个数组，也就是说明每个节点都有“向上延申的可能”。并且还有跨度表明当前层每隔几个节点就连接一次，以及 forward 指针指向每个跨度下的节点。连接成一个新的链表。</li></ul><h3 id="2-3-跳表是怎么获取元素对应的在整个跳表中的顺序的"><a href="#2-3-跳表是怎么获取元素对应的在整个跳表中的顺序的" class="headerlink" title="2.3. 跳表是怎么获取元素对应的在整个跳表中的顺序的"></a>2.3. 跳表是怎么获取元素对应的在整个跳表中的顺序的</h3><ul><li>如果没懂的话，可以这样理解，根据跳表假如<strong>想获取某个元素的权重在整个 ZSET 中的元素顺序</strong>（正序或逆序），你遍历之后，不是要到达底部嘛拿元素嘛，但是你遍历到这个元素之后，发现这个顺序你并不知道是在哪（根据权重排序后，元素的顺序），所以你<u>需要往前遍历找到当前层（level 0）的头节点，来判断自己在哪个位置</u>。这样的话，你这个效率就是 <code>O(n)</code> 了，很慢啊。</li></ul><p>跳表咋解决呢？</p><ul><li>Redis 这个跳表的实现就很巧妙，它是在查询过程中，每个 node 节点里面不是有跨度嘛，就是那个 span，只要进行<mark style="background: #FFFF00;">累加</mark>，这样到达底部的时候，就能得到该节点元素在跳表元素中的全局位置。而那个 forward 指针正好起到一个规范查询过程的一个作用。</li></ul><p>还是不懂，我把图标一下。<br><img src="/images/%E8%B7%B3%E8%A1%A8%E6%9F%A5%E8%AF%A2%E9%A1%BA%E5%BA%8F.png"></p><p>比如这个 23，跨度和就是 2+1&#x3D;3，即 level 1 中 11 节点的跨度+level 0 的23 节点跨度。</p><p>这个是用来进行 Zset 特有的排序 RANK 功能。比如命令：<strong>Zrank 正序</strong> 和 <strong>ZREVrank</strong> 反序。</p><p>获得范围查询元素的顺序也很方便，就是找到初始节点 start 所在的位置，因为也进行了累加，所以它的顺序就是确定的，所以呢其他元素只要也依次累加 span 即可（最底层 span 为 1 嘛）就可以得到元素所在的全局顺序。</p><h3 id="2-4-跳表本身的结构体定义"><a href="#2-4-跳表本身的结构体定义" class="headerlink" title="2.4. 跳表本身的结构体定义"></a>2.4. 跳表本身的结构体定义</h3><pre class="line-numbers language-c" data-language="c"><code class="language-c">typedef struct zskiplist &#123;    struct zskiplistNode *header, *tail;    unsigned long length;    int level;&#125; zskiplist;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以看到有一个头节点，尾节点，说明我们可以从前往后，也可以从后往前遍历这整个跳表，而且刚才也讲了跳表的节点都有指向前者的指针以及后者的指针。</p><blockquote><p>然后呢，这个就是 ZSET 中的一部分，跳表。</p></blockquote><h2 id="3-跳表的其他参数"><a href="#3-跳表的其他参数" class="headerlink" title="3. 跳表的其他参数"></a>3. 跳表的其他参数</h2><p>怎么设置的层数？每层节点个数怎么确定。跳表查询效率怎么样？</p><p><a href="https://zhuanlan.zhihu.com/p/306183653">Redis源码剖析之跳表(skiplist) - 知乎 (zhihu.com)</a>  </p><ol><li>首先看<strong>查询效率</strong></li></ol><p>假如我们要查询元素 13，那么流程大概是这样，也就是发现本层下一个节点值大于目标节点值就从现在的位置下去，继续往后找。<br><img src="/images/%E8%B7%B3%E8%A1%A8%E6%9F%A5%E8%AF%A2.png"></p><p>此时的节点设置的是上一层是下一层的节点的二倍，这样查询的过程相当于二分查找，比如有序数组上的二分，每次找到中间元素进行判断，这个跳表相当于把所有的中间值拿出来到各层链表上，我们每次直接从上层判断中间值，遇到就下去到达底层拿到目标数据。也就提高了单链表 <code> O(n）</code> 的查询效率，变成了 <code>O (logn)</code>。</p><p>而且<u>层数越多，查询的效率越快，但是消耗空间就越多</u>。</p><ol start="2"><li><strong>上层节点的值不固定，由随机算法决定</strong>。</li></ol><p>如上图这种每两个节点之间拿出节点到上层，保证 2：1 的节点比，在进行元素的删除和插入时，你就需要重新调整各层链表的指针，比如上面个节点有多层，这个元素删掉后需要修改大量指针进行重构，以保证有序以及比例关系。所以是很不方便的。</p><p>Redis 的跳表采用的是<strong>zslRandomLevel 函数</strong> 随机决定每个节点的层数。（也就是给每个节点初始化为可能是多级结构，也可能只是底层节点）。</p><blockquote><p>具体是函数内部会生成一个随机数，这个随机数如果小于规定的概率值（大概 1&#x2F;4 概率抽到），这个节点层数就增加一层。所以每个节点增加层数的概率大概为 1&#x2F;4，当插入和删除节点的时候，只要修改待修改的节点的指针即可。</p></blockquote><p><mark style="background: #FFFF00;">可能不太理解，为什么这样就能减少插入和删除时的调整开销了呢</mark>？</p><ul><li>因为上面如果需要保证严格的 2：1 或者其他比例关系，你插入和删除节点就需要保证大部分的节点都需要进行调整以保证比例关系。所以开销大。</li><li>通过随机生成函数，每个节点的层数不定，但是又使全局保证了一种平衡（因为有概率嘛），当插入节点和删除节点的时候，只需要在目标值相邻的节点之间建立指针关联即可。不需要调整其他节点的指针，也就提高了效率。</li></ul><h2 id="4-跳表和哈希表联用"><a href="#4-跳表和哈希表联用" class="headerlink" title="4. 跳表和哈希表联用"></a>4. 跳表和哈希表联用</h2><p>我们上面其实已经知道了，哈希表是用来快速判断整个 ZSET 中是否存在响应的值，并且可以以 <code>O（1）</code> 时间复杂度拿到目标值对应的权重（ZSCORE 命令）。</p><p>然后呢跳表是用来快速定位根据权重排序的元素顺序（累加跨度 span）已经快速查询目标权重范围内的元素（查询到底层进行再遍历）。</p><blockquote><p>这两个算是两个索引，所以需要在插入的时候对两个索引都要进行维护（插入）。</p></blockquote><p><mark style="background: #FFFF00;">插入元素过程</mark>：</p><p>在调用插入元素的函数 ZsetAdd 时，会先通过 dictFind 函数里判断是否存在这个元素是，如果不存在，则分别进行跳表或者压缩列表以及哈希表元素的插入。如果存在，则判断是否需要更新权重，如果需要更新，则会调用 zslUpdateScore 函数进行跳表节点中对应节点的权重值更新，<strong>然后把哈希表的权重指针指向跳表中该节点的权重值</strong>。（这样就能保证哈希表指向的就是最新的权重值）</p><p>zadd 函数具体内容：</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"> &#x2F;&#x2F;如果采用ziplist编码方式时，zsetAdd函数的处理逻辑 if (zobj-&gt;encoding &#x3D;&#x3D; OBJ_ENCODING_ZIPLIST) &#123;   ...&#125;&#x2F;&#x2F;如果采用skiplist编码方式时，zsetAdd函数的处理逻辑else if (zobj-&gt;encoding &#x3D;&#x3D; OBJ_ENCODING_SKIPLIST) &#123;        zset *zs &#x3D; zobj-&gt;ptr;        zskiplistNode *znode;        dictEntry *de;        &#x2F;&#x2F;从哈希表中查询新增元素        de &#x3D; dictFind(zs-&gt;dict,ele);        &#x2F;&#x2F;如果能查询到该元素        if (de !&#x3D; NULL) &#123;            &#x2F;* NX? Return, same element already exists. *&#x2F;            if (nx) &#123;                *flags |&#x3D; ZADD_NOP;                return 1;            &#125;            &#x2F;&#x2F;从哈希表中查询元素的权重            curscore &#x3D; *(double*)dictGetVal(de);            &#x2F;&#x2F;如果要更新元素权重值            if (incr) &#123;                &#x2F;&#x2F;更新权重值               ...            &#125;            &#x2F;&#x2F;如果权重发生变化了            if (score !&#x3D; curscore) &#123;                &#x2F;&#x2F;更新跳表结点                znode &#x3D; zslUpdateScore(zs-&gt;zsl,curscore,ele,score);                &#x2F;&#x2F;让哈希表元素的值指向跳表结点的权重                dictGetVal(de) &#x3D; &amp;znode-&gt;score;                 ...            &#125;            return 1;        &#125;       &#x2F;&#x2F;如果新元素不存在        else if (!xx) &#123;            ele &#x3D; sdsdup(ele);            &#x2F;&#x2F;新插入跳表结点            znode &#x3D; zslInsert(zs-&gt;zsl,score,ele);            &#x2F;&#x2F;新插入哈希表元素            serverAssert(dictAdd(zs-&gt;dict,ele,&amp;znode-&gt;score) &#x3D;&#x3D; DICT_OK);            ...            return 1;        &#125;         ..<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><input checked="" disabled="" type="checkbox"> #task 跳表 skipList 数据结构 ✅ 2024-02-29</li></ul><h2 id="5-为什么用跳表，不用红黑树？"><a href="#5-为什么用跳表，不用红黑树？" class="headerlink" title="5. 为什么用跳表，不用红黑树？"></a>5. 为什么用跳表，不用红黑树？</h2><ul><li><strong>内存占用更少</strong>。平衡树每个节点包含 2 个指针，redis 中平均指针 1.33 个。</li><li><strong>在做范围查找的时候，跳表比平衡树操作要简单</strong>。跳表只需要在找到小值之后，对第 1 层链表进行若干步的遍历就可以实现。</li><li><strong>从算法实现难度上来比较，跳表比平衡树要简单得多</strong>。因为红黑树涉及各种旋转保持平衡的操作。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-前言&quot;&gt;&lt;a href=&quot;#1-前言&quot; class=&quot;headerlink&quot; title=&quot;1. 前言&quot;&gt;&lt;/a&gt;1. 前言&lt;/h2&gt;&lt;p&gt;面试的时候问了这些。所以要弄清楚具体的查询过程，以及跳表的结构，还有实现利用 ZSET 实现&lt;strong&gt;排行榜是怎么</summary>
      
    
    
    
    <category term="Redis" scheme="http://example.com/categories/Redis/"/>
    
    
    <category term="redis/ziplist" scheme="http://example.com/tags/redis-ziplist/"/>
    
  </entry>
  
  <entry>
    <title>如果不用Cookie怎么统计uv呢</title>
    <link href="http://example.com/2024/02/24/ru-guo-bu-yong-cookie-zen-me-tong-ji-uv-ni/"/>
    <id>http://example.com/2024/02/24/ru-guo-bu-yong-cookie-zen-me-tong-ji-uv-ni/</id>
    <published>2024-02-24T11:08:00.000Z</published>
    <updated>2024-04-05T03:03:25.889Z</updated>
    
    <content type="html"><![CDATA[<p>如题，面试的时候问了这个问题，我当时没想好解决方案，事后就思考了一下。</p><blockquote><p><em>怎么不用 Cookie 来统计 uv</em>. 就在这猜测一下吧，思考过程：</p><ul><li>如果不用 cookie，那就说明用 session 也不行，因为 session 也是需要依赖 Cookie 来保存这个 session ID 嘛。</li><li>然后如果是缓存的话，还能想到<strong>浏览器的 Local Storage 存这个 uv 字段</strong>，但是呢，这样不能绝对的保证这个用户一定是新用户，因为 Local Storage 虽然不容易被删，<strong>但是也能够删掉</strong>。</li><li>想了一下，比如 b 站，哪怕我换个浏览器，登录 b 站账号之后，本应该是没有缓存嘛，但是它确实是能推送你喜欢的视频，所以嘛，我觉得它这个统计是在用户登录上面做了手脚，比如给用户信息加一些字段，来表示这个用户是新用户还是老用户。</li><li>但是呢<u>又想到这个短链接跳转它并不是只给登录用户使用，未登录也可以通过它跳转</u>。那怎么来判断是否为新还是老用户呢？</li></ul><p>然后呢我就查资料啊，问 GPT，都没有绝对的保证判断用户的新老。也就不了了之了。还是那句话，我觉得删完 Cookie，确实算为一个新用户没啥问题，<mark style="background: #FFFF00;">毕竟操作者就是用户</mark>，誰会想到闲着没事删删 Cookie 呢，笑了。</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;如题，面试的时候问了这个问题，我当时没想好解决方案，事后就思考了一下。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;怎么不用 Cookie 来统计 uv&lt;/em&gt;. 就在这猜测一下吧，思考过程：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果不用 cookie，那就说明用 sessio</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>一批大量的 redis 请求都过来, 怎么优化（批处理优化问题）</title>
    <link href="http://example.com/2024/02/24/yi-pi-da-liang-de-redis-qing-qiu-du-guo-lai-zen-me-you-hua-pi-chu-li-you-hua-wen-ti/"/>
    <id>http://example.com/2024/02/24/yi-pi-da-liang-de-redis-qing-qiu-du-guo-lai-zen-me-you-hua-pi-chu-li-you-hua-wen-ti/</id>
    <published>2024-02-24T10:43:00.000Z</published>
    <updated>2024-04-05T03:03:41.435Z</updated>
    
    <content type="html"><![CDATA[<p>面试问到一千条 redis 命令来到，怎么进行优化，来减少处理时间。</p><p>之前没了解过，然后想的一些方案并不是这么有效，所以下来自己总结一下。</p><h1 id="单机模式下"><a href="#单机模式下" class="headerlink" title="单机模式下"></a>单机模式下</h1><blockquote><p>暂时先讲单机模式下的批处理优化。</p></blockquote><h2 id="通信携带命令条数不同导致效率不同"><a href="#通信携带命令条数不同导致效率不同" class="headerlink" title="通信携带命令条数不同导致效率不同"></a>通信携带命令条数不同导致效率不同</h2><ol><li>如果一次客户端到 redis 服务端的通信中只携带一条 redis 指令，那大概的总耗时就是：1 个来回时间 + 1 个 redis 处理命令时间。N 次就是 N 个来回时间 + N 条命令处理时间。</li></ol><p>画个图吧：<br><img src="/images/n%E6%AC%A1%E9%80%9A%E4%BF%A1.png"></p><ol start="2"><li>一次通信携带多条指令，减少总的通信次数。</li></ol><p>这样的 N 条命令的总耗时就为：N&#x2F;M 个来回时间，N 个处理时间。（M 代指每次携带的命令条数）。</p><p>图就不画了，应该能想象出来，不是重点。</p><ol start="3"><li>总之：<mark style="background: #FFFF00;">通信时间在 Redis 的使用过程中占大头，所以减少通信次数就是批处理优化的一个主要攻克点</mark>。</li></ol><h2 id="Redis-中怎么操作"><a href="#Redis-中怎么操作" class="headerlink" title="Redis 中怎么操作"></a>Redis 中怎么操作</h2><blockquote><p>我当时想到了列表 List 数据类型，但是列表好像并不算是一个批处理命令，然后就扯其他的了。</p></blockquote><p>现在才醒悟，对啊，<code>mset </code> 和 <code>hmset</code>。就是专门用来做批处理的命令，怎么没想到类比 Mysql 分批插入大量数据呢。也就是一次把多个 redis 命令封装在一起作为一次通信发给 Redis 客户端。</p><p><strong>语法</strong> </p><pre class="line-numbers language-java" data-language="java"><code class="language-java">mset key1 value1 [ key2 value2]hmset key field1 value1 [field2 value2]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><strong>代码示例</strong> </p><p>如上，可以通过数组传递需要设置的批量 redis 命令，设置 String 类型的。并且 key 可以不变。而对于 hash 数据类型，其批量插入只能指定一个键。Set 类型的批量插入命令（SADD）也是有局限的（只能指定一个 set 的键）。</p><p>MSET 命令，数组批量插入示例：<br><img src="/images/hset%E5%91%BD%E4%BB%A4.png" alt="|500"></p><h2 id="PipeLine-优化批处理执行"><a href="#PipeLine-优化批处理执行" class="headerlink" title="PipeLine 优化批处理执行"></a>PipeLine 优化批处理执行</h2><p>PipeLine 不管是 Jedis 还是 Redis 都提供的有，这个<u>主要是用来把各种不同的 Redis 命令都装在一起</u>，然后统一发送到 Redis 服务端。</p><p><u>与批处理命令不同的是，这个可以装”不同的命令”，也就是说可以解决 HMSET 和 SADD 中批量插入时只能指定一个 Key 的问题</u>。你只要把多个 SADD 命令和 HMSET 命令装进来就行了。</p><p>示例：</p><ol><li>模拟 MSET<br><img src="/images/Pipeline%E6%A8%A1%E6%8B%9Fmset.png"></li></ol><p>PipeLine 的 <code>sync()</code> 命令就是发起一次通信请求到 Redis 服务端。</p><p>但是命令到达之后可能就会进入队列中等待，执行的顺序是有先后的，所以呢速度可能会比 mset 慢一些，但是好在对更多的数据类型都支持。</p><h1 id="集群模式下"><a href="#集群模式下" class="headerlink" title="集群模式下"></a>集群模式下</h1><p>在集群模式下，如果批处理命令没有分配到同一个槽位（也就是同一个 Redis 节点），就会导致执行失败。所以在集群部署下做批处理就需要其他工具了。</p><p>TODO</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;面试问到一千条 redis 命令来到，怎么进行优化，来减少处理时间。&lt;/p&gt;
&lt;p&gt;之前没了解过，然后想的一些方案并不是这么有效，所以下来自己总结一下。&lt;/p&gt;
&lt;h1 id=&quot;单机模式下&quot;&gt;&lt;a href=&quot;#单机模式下&quot; class=&quot;headerlink&quot; title</summary>
      
    
    
    
    
    <category term="Redis/批处理优化" scheme="http://example.com/tags/Redis-%E6%89%B9%E5%A4%84%E7%90%86%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>联合索引进行的范围查询为什么会导致索引失效</title>
    <link href="http://example.com/2024/02/24/lian-he-suo-yin-jin-xing-de-fan-wei-cha-xun-wei-shi-me-hui-dao-zhi-suo-yin-shi-xiao/"/>
    <id>http://example.com/2024/02/24/lian-he-suo-yin-jin-xing-de-fan-wei-cha-xun-wei-shi-me-hui-dao-zhi-suo-yin-shi-xiao/</id>
    <published>2024-02-24T09:15:00.000Z</published>
    <updated>2024-02-24T11:11:21.111Z</updated>
    
    <content type="html"><![CDATA[<p>也是面试问的，由于不够深入底层存储结构，导致对索引的理解不够深刻。加入了 TODO，准备好好的解析底层结构。</p><p>自己想的范围查询过程貌似差不多对了。但是没说清楚索引失效原因，补充一下。</p><h2 id="到底为什么失效呢"><a href="#到底为什么失效呢" class="headerlink" title="到底为什么失效呢"></a>到底为什么失效呢</h2><p>比如有三个字段，A&#x3D;, B&gt;, C&#x3D;. 最后只会走两个索引 A 和 B 的，并不能找到唯一的记录。</p><p>看来面试的时候猜到了一点。就是三个字段先从 A 字段开始匹配，找到某个节点对应的记录后，下到第二层对应的 A 字段相等的节点上，然后 B 字段就从 B 字段这个层往后查询，因为是范围查询吗，没有机会往下走去找 C 字段（不是精确值，下不去）。所以最后能查询的数据不是唯一的，而是很多数据，这时候索引就走不全了。（B+树联合索引的存储结构得看看，还有记录在节点中的存储情况）</p><p>然后呢，由于没有查到需要的 C 字段，所以会<u>进行回表查询</u>，而且是大量的回表查询，那这时候 Mysql 不是在执行 SQL 的时候有一个优化器吗，它<mark style="background: #FFFF00;">发现走索引然后进行回表的开销，比全表扫描更大</mark>。所以它就不走索引，而是通过全表扫描来进行查询。也就是说联合索引失效了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;也是面试问的，由于不够深入底层存储结构，导致对索引的理解不够深刻。加入了 TODO，准备好好的解析底层结构。&lt;/p&gt;
&lt;p&gt;自己想的范围查询过程貌似差不多对了。但是没说清楚索引失效原因，补充一下。&lt;/p&gt;
&lt;h2 id=&quot;到底为什么失效呢&quot;&gt;&lt;a href=&quot;#到底为什么失</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>消息队列怎么保证消息不丢失的</title>
    <link href="http://example.com/2024/02/18/xiao-xi-dui-lie-zen-me-bao-zheng-xiao-xi-bu-diu-shi-de/"/>
    <id>http://example.com/2024/02/18/xiao-xi-dui-lie-zen-me-bao-zheng-xiao-xi-bu-diu-shi-de/</id>
    <published>2024-02-18T03:00:00.000Z</published>
    <updated>2024-02-18T03:00:38.182Z</updated>
    
    <content type="html"><![CDATA[<ul><li><input checked="" disabled="" type="checkbox"> #task 消息队列怎么保证消息不丢失的 ✅ 2024-02-18</li></ul><h2 id="为什么会发生消息丢失"><a href="#为什么会发生消息丢失" class="headerlink" title="为什么会发生消息丢失"></a>为什么会发生消息丢失</h2><p>可能因为网络问题导致消息丢失，以及存储时消息队列宕机了等情况。具体发生位置如下。</p><p>还是画个简单的流程图吧，比较清晰：<br><img src="/images/%E6%B6%88%E6%81%AF%E5%8F%91%E9%80%81%E8%BF%87%E7%A8%8B.png"></p><h2 id="如何解决"><a href="#如何解决" class="headerlink" title="如何解决"></a>如何解决</h2><p>可以从生产者、消息队列本身、消费者解决。</p><h3 id="生产者保证消息不丢失"><a href="#生产者保证消息不丢失" class="headerlink" title="生产者保证消息不丢失"></a>生产者保证消息不丢失</h3><ol><li>事务管理：当发送一条消息后，开启事务，等待消息队列传送回消息处理成功之后才继续发送消息。</li><li>生产者确认机制：开启生产者确认机制，只要消息成功发送到交换机之后 RabbitMQ 就会发送一个 ack 给生产者（即使消息没有 Queue 接收，也会发送 ack）。如果消息没有成功发送到交换机，就会发送一条 nack 消息，提示发送失败。</li></ol><p>当服务端确定一条或多条消息后就会调用生产者提供的回调方法来提醒生产者自己是否成功接收到了消息并处理。</p><pre class="line-numbers language-java" data-language="java"><code class="language-java">&#x2F;&#x2F; 消息是否成功发送到Exchangefinal RabbitTemplate.ConfirmCallback confirmCallback &#x3D; (CorrelationDatacorrelationData, boolean ack, String cause) -&gt; &#123;log.info(&quot;correlationData: &quot; + correlationData);log.info(&quot;ack: &quot; + ack);if(!ack) &#123;log.info(&quot;异常处理....&quot;);&#125;&#125;;rabbitTemplate.setConfirmCallback(confirmCallback)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="路由交换机-Exchange-保证消息不丢失"><a href="#路由交换机-Exchange-保证消息不丢失" class="headerlink" title="路由交换机 Exchange 保证消息不丢失"></a>路由交换机 Exchange 保证消息不丢失</h3><p>生产者只会保证消息到达路由不会丢失，而不能保证从交换机到达某个消息队列，然后从消息队列到达消费者不丢失消息。</p><p><strong>解决方法</strong>：</p><p>开启 <code>Return 消息机制</code>。当从交换机到消息队列 queue 时，如果失败则会调用 return 方法，告诉生产者消息发送失败。</p><pre class="line-numbers language-java" data-language="java"><code class="language-java">final RabbitTemplate.ReturnCallback returnCallback &#x3D; (Message message, int replyCode, String replyText, String exchange, String routingKey) -&gt;log.info(&quot;return exchange: &quot; + exchange + &quot;, routingKey: &quot;+ routingKey + &quot;, replyCode: &quot; + replyCode + &quot;, replyText: &quot;+ replyText);rabbitTemplate.setReturnCallback(returnCallback);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="消费者消费失败导致消息丢失的问题"><a href="#消费者消费失败导致消息丢失的问题" class="headerlink" title="消费者消费失败导致消息丢失的问题"></a>消费者消费失败导致消息丢失的问题</h3><p>因为<strong>消费者默认采用自动 ack</strong>，所以当消费者收到消息后，还没有执行完，MQ 宕机了，但是由于是自动 ack，所以消费者收到消息后，直接就返回 ack，消息队列收到后就把这条消息从消息队列中删除了。</p><p>最终消息没有消费成功，但是却返回了 ack，并导致消息丢失。</p><p><strong>解决方法</strong>：</p><p>开启手动发送 ack。</p><p>配置：<code>spring.rabbitmq.listener.simple.acknowledge-mode=manual</code>  </p><p>样例：总之就是执行完自己的处理逻辑再返回 ack。</p><pre class="line-numbers language-java" data-language="java"><code class="language-java">@RabbitListener(queues &#x3D; RabbitMqConfig.MAIL_QUEUE)public void onMessage(Message message, Channel channel) throws IOException &#123;try &#123;Thread.sleep(5000);&#125; catch (InterruptedException e) &#123;e.printStackTrace();&#125;long deliveryTag &#x3D; message.getMessageProperties().getDeliveryTag();&#x2F;&#x2F;总之就是执行完自己的处理逻辑再返回ack。channel.basicAck(deliveryTag, true);System.out.println(&quot;mail listener receive: &quot; + newString(message.getBody()));&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="开启消息队列持久化"><a href="#开启消息队列持久化" class="headerlink" title="开启消息队列持久化"></a>开启消息队列持久化</h3><p>如果不开启持久化，RabbitMQ 异常重启的时候，就会导致消息丢失，因为不能从其他地方恢复数据。</p><p><strong>开启消息队列持久化后，就可以以一定的规则把消息队列的数据同步到磁盘上或者远程服务器上，以保证消息持久化</strong>。</p><blockquote><ul><li><p>具体就是，生产者每发送一条消息到交换机以及消息队列，他们就会把消息存储到各自的持久化日志中。接下来，每当一个消息消息消费成功返回 ack 的时候，就从持久化中删除这个消息。</p></li><li><p>一旦服务宕机，就可以从持久化日志中进行恢复未完成的消息。</p></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;input checked=&quot;&quot; disabled=&quot;&quot; type=&quot;checkbox&quot;&gt; #task 消息队列怎么保证消息不丢失的 ✅ 2024-02-18&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;为什么会发生消息丢失&quot;&gt;&lt;a href=&quot;#为什么会发生消息丢</summary>
      
    
    
    
    <category term="消息队列" scheme="http://example.com/categories/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
    
    <category term="消息队列" scheme="http://example.com/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
  </entry>
  
  <entry>
    <title>为什么要引入消息队列之异步削峰解耦</title>
    <link href="http://example.com/2024/02/17/wei-shi-me-yao-yin-ru-xiao-xi-dui-lie-zhi-yi-bu-xue-feng-jie-ou/"/>
    <id>http://example.com/2024/02/17/wei-shi-me-yao-yin-ru-xiao-xi-dui-lie-zhi-yi-bu-xue-feng-jie-ou/</id>
    <published>2024-02-17T03:11:40.598Z</published>
    <updated>2024-02-17T03:12:31.384Z</updated>
    
    <content type="html"><![CDATA[<p>消息队列其实就是用一个队列存储消息，我们可以从中获取消息并处理。</p><h2 id="1-消息队列的三大用处"><a href="#1-消息队列的三大用处" class="headerlink" title="1. 消息队列的三大用处"></a>1. 消息队列的三大用处</h2><p><em><strong>异步、削峰、解耦</strong></em>。</p><blockquote><ol><li>异步处理提高系统性能（减少响应时间）</li><li>削峰：就是限流</li><li>解耦：降低系统耦合性，更好维护，拓展性更高</li></ol></blockquote><h3 id="1-1-异步"><a href="#1-1-异步" class="headerlink" title="1.1. 异步"></a>1.1. 异步</h3><ol><li><em>为什么要异步</em>？</li></ol><p>传统场景下，客户端发起某个请求，假如说是下单吧，那负责下单的系统，不仅要进程支付功能的调用，更新库存，还要发送下单成功通知等。最后再返回一个状态说我都做完了，你可以做其他事了。用户这边体验就很差，下个单要等这么久。</p><p>如果全交给这一个系统做这一连串的事，那就很慢。</p><ol start="2"><li><em>加入消息队列怎么解决</em>？</li></ol><p>其实就是负责下单的系统接收到请求后，就发送消息到消息队列，那其他订阅了这个消息队列的系统就可以进行<strong>消费</strong>，然后做自己该干的事，而发送消息的系统直接就返回给用户说明下单成功。这样看来，用户的体验就很快。</p><blockquote><p>比如下单后，要过一段时间才会收到短信提醒。</p></blockquote><h2 id="2-削峰"><a href="#2-削峰" class="headerlink" title="2. 削峰"></a>2. 削峰</h2><p>简单来说就是拿消息队列做一层<strong>类似缓冲的作用</strong>，大量请求来到就先存在消息队列，然后消费者再慢慢吸收这些消息，<strong>防止大量请求打到数据库</strong>，Mysql 数据库一般 QPS 两千就差不多了，如果再多可能就崩了。</p><h2 id="3-解耦"><a href="#3-解耦" class="headerlink" title="3. 解耦"></a>3. 解耦</h2><ol><li><em>为啥需要解耦捏</em>？</li></ol><p>传统场景下，一个系统要给多个系统发送”消息”的时候，需要考虑下面几个问题。</p><ul><li>每个系统是不是都需要这个消息，假如又有新系统也要这个数据怎么办？再重发一次嘛？还是说要存一下，那也太消耗空间了。</li><li>如果消费者系统挂了，要不要重发？</li><li>……</li></ul><blockquote><p>所以对于一个系统来说，压力太大，耦合度太高，万一这个系统也崩了，那整个项目也就寄的差不多了。</p></blockquote><ol start="2"><li><em>引入消息队列后怎么解决的</em>。</li></ol><p>简单来说的话，就是生产者系统<strong>只需要关注消息的发送</strong>，而不用考虑后续出现的问题，比如一个系统想向其他系统发送消息，只需要把消息送到消息队列，然后就不管了，接下来与这个业务相关的系统就可以从消息队列中自行读取。</p><p>通过消息队列中的<strong>订阅-发布模式</strong>就可以让订阅了这个队列的消费者接收到请求并进行处理，也就实现了解耦。</p><ol start="3"><li>结合自己的项目</li></ol><ul><li><input checked="" disabled="" type="checkbox"> #task 结合自己的项目考虑为什么要引入消息队列 🔽 ✅ 2024-01-31</li></ul><p>参考：<a href="../../4%20%E9%A1%B9%E7%9B%AE%E7%AC%94%E8%AE%B0%E7%9B%92/%E7%9F%AD%E9%93%BE%E6%8E%A5shortlink/%E9%A1%B9%E7%9B%AE%E4%BA%AE%E7%82%B9%E5%AE%9E%E7%8E%B0%E6%80%BB%E7%BB%93/%E5%BC%95%E5%85%A5%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%EF%BC%8C%E9%87%8D%E6%9E%84%E7%9F%AD%E9%93%BE%E6%8E%A5%E9%87%8D%E5%AE%9A%E5%90%91%E6%97%B6%E8%AE%BF%E9%97%AE%E8%BE%83%E6%85%A2%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%88%E6%B5%81%E9%87%8F%E5%89%8A%E5%B3%B0%EF%BC%89.md">引入消息队列，重构短链接重定向时访问较慢的问题（流量削峰）</a> </p><blockquote><p><strong>面试技巧</strong>：你需要去考虑一下你负责的系统中是否有类似的场景，就是一个系统或者一个模块，调用了多个系统或者模块，互相之间的调用很复杂，维护起来很麻烦。但是其实这个调用是不需要直接同步调用接口的，如果用 MQ 给它异步化解耦，也是可以的，你就需要去考虑在你的项目里，是不是可以运用这个 MQ 去进行系统的解耦。在简历中体现出来这块东西，用 MQ 作解耦。</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;消息队列其实就是用一个队列存储消息，我们可以从中获取消息并处理。&lt;/p&gt;
&lt;h2 id=&quot;1-消息队列的三大用处&quot;&gt;&lt;a href=&quot;#1-消息队列的三大用处&quot; class=&quot;headerlink&quot; title=&quot;1. 消息队列的三大用处&quot;&gt;&lt;/a&gt;1. 消息队列的三大用处</summary>
      
    
    
    
    <category term="框架/中间件" scheme="http://example.com/categories/%E6%A1%86%E6%9E%B6-%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
    <category term="消息队列" scheme="http://example.com/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
  </entry>
  
  <entry>
    <title>对Synchronized锁升级，锁消除的理解</title>
    <link href="http://example.com/2024/02/17/dui-synchronized-suo-sheng-ji-suo-xiao-chu-de-li-jie/"/>
    <id>http://example.com/2024/02/17/dui-synchronized-suo-sheng-ji-suo-xiao-chu-de-li-jie/</id>
    <published>2024-02-17T02:00:00.000Z</published>
    <updated>2024-02-17T03:10:27.066Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Synchronized 锁升级放下这么久了，今天打算好好学一下。主要方式是通过读<u>《Java 并发编程的艺术》</u>来进行学习。</p><p><mark style="background: #FFFF00;">Synchronized 锁状态共分为无锁、偏向锁、轻量级锁以及重量级锁。锁一旦升级就不能进行降级</mark>。</p><h2 id="先从-Synchronized-锁对对象的影响开始"><a href="#先从-Synchronized-锁对对象的影响开始" class="headerlink" title="先从 Synchronized 锁对对象的影响开始"></a>先从 Synchronized 锁对对象的影响开始</h2><p>先看一下用来判断一个对象是什么的对象头（Mark Word）。</p><table><thead><tr><th>锁状态</th><th>25bit</th><th>4 bit</th><th>1 bit 用来判断是否为偏向锁</th><th>2bit 锁标识</th></tr></thead><tbody><tr><td>无锁状态</td><td>hashCode</td><td>对象分代年龄</td><td>0</td><td>01</td></tr></tbody></table><p>如表所示，这是一个”未被锁的对象”，其对象头存储了 hashCode ，也就是调用 &#x3D;&#x3D; 时判断对象是否为同一个对象的时候用到的值，原来对象在新建的时候就已经设置好了 hashcode，不是用到的时候再计算的。</p><p>然后就是 4 bit, 我们都知道 GC 垃圾回收过程中，一般对象刚分配会分到新生代 Eden，在多次存活后（先放到 Suvivor 区），<u>当年龄达到后，就会移交到老年代</u>，这样就能减少对新生代进行垃圾回收时的性能消耗。每次存活对象的年龄都会增加，原来就是存储在这里啊。</p><p>然后就是判断偏向锁的标识，很明显就是判断是否给当前对象加了偏向锁，最后的锁标识则是用来标识当前对象被锁的状态到底是什么锁，是偏向锁还是、轻量级锁或者重量级锁。</p><h2 id="偏向锁"><a href="#偏向锁" class="headerlink" title="偏向锁"></a>偏向锁</h2><p><strong>大多数情况下，锁不仅不存在多线程竞争，而且总是由同一个线程获得</strong>。为了减少线程获取锁以及释放锁带来的性能消耗，引入了偏向锁。</p><blockquote><p>意思很明显，就是让这个对象偏向于被某个线程加锁。</p></blockquote><p>实现原理：</p><table><thead><tr><th>锁状态</th><th>23bit</th><th>2bit</th><th>4bit</th><th>1bit 是否是偏向锁</th><th>2bit 锁标识位</th></tr></thead><tbody><tr><td>偏向锁</td><td>偏向线程 ID</td><td>偏向时间戳</td><td>对象分代年龄</td><td>1</td><td>01</td></tr></tbody></table><p>当一个线程访问对象获取锁的时候，会判断这个对象的对象头中的偏向锁标识是否为 1</p><ul><li>如果为 0，则说明未被加锁，于是设置为 1，并把偏向线程 ID 设置为自己的 ID。执行完后不释放锁，下次再访问到这个代码块的时候，判断这个线程 ID 是否是自己的 ID，如果是，<strong>由于没有释放锁，则直接执行代码块</strong>。</li><li>如果为 1，说明被加锁，判断是偏向锁 ID 是否为自己的 ID，如果是则直接执行，因为自己已经获取了锁。如果不是自己的 ID，则尝试用 CAS 修改线程 ID 为自己的。</li></ul><p>此时重点来了，线程尝试修改 CAS 修改线程 ID 为自己的 ID 时，修改失败。也就是说明其他线程还用着呢，你怎么就要获取了，这里就发生了<mark style="background: #FFFF00;">锁的竞争</mark>。</p><p>因此会<strong>发生偏向锁的撤销（偏向锁标识设置为 0，锁标识不变，因为无锁和偏向锁标识一样），然后进一步升级为轻量级锁</strong>。</p><p>画了个流程图：</p><p><img src="/images/%E5%81%8F%E5%90%91%E9%94%81.png"></p><h2 id="轻量级锁（自旋锁）"><a href="#轻量级锁（自旋锁）" class="headerlink" title="轻量级锁（自旋锁）"></a>轻量级锁（自旋锁）</h2><p>既然已经到了轻量级锁，说明出现了竞争。</p><p>从加锁和解锁两部分说明。</p><ol><li>加锁<br>线程通过 CAS 修改对象头内的锁标识</li></ol><ul><li>如果当前锁标识为无锁状态，则直接获取锁，并修改标识为有锁状态。</li><li>如果锁标识为有锁，则等待其他线程释放锁，并频繁的进行”自旋操作”尝试获取锁。</li></ul><ol start="2"><li>释放锁<br>设置标识为无锁状态。</li></ol><p>这样有什么问题？</p><ul><li>造成其他线程一直空转（不执行任务，但占用 CPU）并进行自旋操作，造成资源浪费，如果一直获取不到就会一直等待。</li></ul><p>因此轻量级锁适合于锁竞争不激烈的情况，如果锁竞争太过激烈（<mark style="background: #FFFF00;">比如自旋次数超过 10 次</mark>），就需要升级为重量级锁。</p><h2 id="重量级锁"><a href="#重量级锁" class="headerlink" title="重量级锁"></a>重量级锁</h2><p>其他线程在获取锁失败时发现是重量级锁，则<strong>不会等待，而是直接挂起，等待获取锁的线程执行完之后进行唤醒</strong>，其他线程才继续开始获取锁，并执行代码。</p><p>由此可以看到重量级锁，避免了线程空转，减少了 CPU 消耗，但是想对的，线程被阻塞，性能变慢。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;Synchronized 锁升级放下这么久了，今天打算好好学一下。主要方式是通过读&lt;u&gt;《Java 并发编程的艺术》&lt;/u&gt;来进行学习。&lt;/</summary>
      
    
    
    
    <category term="锁" scheme="http://example.com/categories/%E9%94%81/"/>
    
    
    <category term="Synchronized" scheme="http://example.com/tags/Synchronized/"/>
    
  </entry>
  
</feed>
